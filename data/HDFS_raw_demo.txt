DEBUG org.apache.hadoop.hdfs.server.namenode.NameNodeResourceChecker: Going to check the following volumes disk space:  <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer: Connecting to datanode  <*> 
WARN org.apache.hadoop.hdfs.DFSInputStream: <*> <*> . Throwing a BlockMissingException 
DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer: Connecting to datanode  <*> 
WARN org.apache.hadoop.hdfs.DFSInputStream: <*> <*> . Throwing a BlockMissingException 
TRACE org.apache.hadoop.hdfs.BlockReaderFactory: this : trying to construct a BlockReaderLocal  for short-circuit reads. 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : successfully loaded  <*> 
DEBUG org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : starting cache cleaner thread which will run  every  <*>  ms 
TRACE org.apache.hadoop.hdfs.BlockReaderFactory: this : returning new block reader local. 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : found waitable for  key 
WARN org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : failed to get  key 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : successfully loaded  <*> 
DEBUG org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : starting cache cleaner thread which will run  every  <*>  ms 
TRACE org.apache.hadoop.hdfs.BlockReaderFactory: this : got InvalidToken exception while trying to  construct BlockReaderLocal via  <*> 
TRACE org.apache.hadoop.hdfs.DFSClient: Address  targetAddr <*> 
TRACE org.apache.hadoop.hdfs.BlockReaderFactory: this : trying to create a remote block reader from the  UNIX domain socket at  <*> 
TRACE org.apache.hadoop.hdfs.BlockReaderFactory: nextDomainPeer: reusing existing peer  <*> 
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.Sender: Sending DataTransferOp  <*> :  proto 
TRACE org.apache.hadoop.hdfs.DFSClient: Address  targetAddr <*> 
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.Sender: Sending DataTransferOp  <*> :  proto 
TRACE org.apache.hadoop.hdfs.BlockReaderFactory: this : trying to create a remote block reader from a  TCP socket 
TRACE org.apache.hadoop.hdfs.BlockReaderFactory: nextTcpPeer: reusing existing peer  <*> 
TRACE org.apache.hadoop.hdfs.BlockReaderFactory: nextTcpPeer: created newConnectedPeer  <*> 
TRACE org.apache.hadoop.hdfs.BlockReaderFactory: nextTcpPeer: created newConnectedPeer  <*> 
TRACE org.apache.hadoop.hdfs.BlockReaderFactory: nextTcpPeer: failed to create newConnectedPeer connected to  <*> 
WARN org.apache.hadoop.hdfs.DFSInputStream: Failed to connect to  <*>  for block , add to deadNodes and continue.  <*> 
WARN org.apache.hadoop.hdfs.DFSInputStream: Found Checksum error for  <*>  from  <*>  at  <*> 
INFO org.apache.hadoop.hdfs.DFSInputStream: Access token was invalid when connecting to  targetAddr  :  ex 
INFO org.apache.hadoop.hdfs.DFSInputStream: Access token was invalid when connecting to  targetAddr  :  ex 
INFO org.apache.hadoop.hdfs.DFSInputStream: Access token was invalid when connecting to  targetAddr  :  ex 
WARN org.apache.hadoop.hdfs.DFSInputStream: Failed to connect to  <*>  for block , add to deadNodes and continue.  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NameNode: Setting fs.defaultFS to  <*> 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Edits URI  i$  listed multiple times in  dfs.namenode.shared.edits.dir  and  dfs.namenode.edits.dir . Ignoring duplicates. 
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: !!! WARNING !!!\n\tThe NameNode currently runs without persistent storage.\n\tAny changes to the file system meta-data may be lost.\n\tRecommended actions:\n\t\t- shutdown and restart NameNode with configured \ propertyName \ in hdfs-site.xml; \n\t\t- use Backup Node as a persistent and up-to-date storage  of the file system meta-data. 
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Edits URI  i$  listed multiple times in  dfs.namenode.shared.edits.dir  and  dfs.namenode.edits.dir . Ignoring duplicates. 
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: !!! WARNING !!!\n\tThe NameNode currently runs without persistent storage.\n\tAny changes to the file system meta-data may be lost.\n\tRecommended actions:\n\t\t- shutdown and restart NameNode with configured \ propertyName \ in hdfs-site.xml; \n\t\t- use Backup Node as a persistent and up-to-date storage  of the file system meta-data. 
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Edits URI  i$  listed multiple times in  dfs.namenode.shared.edits.dir  and  dfs.namenode.edits.dir . Ignoring duplicates. 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: set restore failed storage to  val 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: <*> 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.HeartbeatManager: Setting heartbeat recheck interval to  <*>  since  dfs.namenode.stale.datanode.interval  is less than  dfs.namenode.heartbeat.recheck-interval 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit= <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check= <*> 
WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: The given interval for marking stale datanode =  staleInterval , which is larger than heartbeat expire interval  heartbeatExpireInterval . 
INFO org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: dfs.namenode.startup.delay.block.deletion.sec is set to  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: The block deletion will start around  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable= <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.key.update.interval= <*>  min(s),  dfs.block.access.token.lifetime = <*>  min(s),  dfs.encrypt.data.transfer.algorithm = <*> 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled =  <*> 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Caching file names occuring more than  <*>  times 
INFO org.apache.hadoop.hdfs.server.namenode.CacheManager: Using minimum value {} for {}
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: The threshold value should\'t be greater than , threshold:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: dfs.namenode.safemode.threshold-pct =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: dfs.namenode.safemode.min.datanodes =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: dfs.namenode.safemode.extension     =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNConf: ACLs enabled?  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNConf: XAttrs enabled?  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNConf: Maximum size of an xattr:  <*> <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: STATE* Leaving safe mode after  <*>  secs 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: STATE* Network topology has  <*>  racks and  <*>  datanodes 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: STATE* UnderReplicatedBlocks has  <*>  blocks 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: <*>  does not exist. Creating ... 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Lock on  <*>  acquired by nodename  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Cannot lock storage  <*> . The directory is already locked 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Lock on  <*>  acquired by nodename  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: <*>  does not exist. Creating ... 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Locking is disabled for  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Recovering storage directory  <*>  from previous upgrade 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Recovering storage directory  <*>  from previous upgrade 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Completing previous checkpoint for storage directory  <*> 
ERROR org.apache.hadoop.hdfs.server.common.Storage: *********** Upgrade is not supported from this  older version  oldVersion  of storage to the current version.  Please upgrade to  Hadoop-.  or a later version and then upgrade to current  version. Old layout version is  <*>  and latest layout version this software version can  upgrade from is  $i . ************ 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: Clusterid mismatch - current clusterid:  <*> , Ignoring given clusterid:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: Using clusterid:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: Storage directory  sd  contains no VERSION file. Skipping... 
ERROR org.apache.hadoop.hdfs.server.namenode.FSEditLog: No edits directories configured!
INFO org.apache.hadoop.hdfs.server.namenode.JournalSet: Skipping jas  jas  since it\'s disabled 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load edit log stream:  elis 
INFO org.apache.hadoop.hdfs.server.namenode.FSImage: No edit log streams selected.
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: About to load edits:\n   <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading  editIn  expecting start txid # <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: BUG: Namespace quota violation in image for  <*>  quota =  <*>  < consumed =  child# 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: BUG: Diskspace quota violation in image for  <*>  quota =  <*>  < consumed =  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: BUG: Diskspace quota violation in image for  <*>  quota =  <*>  < consumed =  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: BUG: Namespace quota violation in image for  <*>  quota =  <*>  < consumed =  child# 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: BUG: Diskspace quota violation in image for  <*>  quota =  <*>  < consumed =  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: BUG: Namespace quota violation in image for  <*>  quota =  <*>  < consumed =  child# 
ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: BUG: Diskspace quota violation in image for  <*>  quota =  <*>  < consumed =  <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: BUG: Namespace quota violation in image for  <*>  quota =  <*>  < consumed =  child# 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image?  <*>#_  (staleImage= <*> , haEnabled= <*> , isRollingUpgrade= <*> ) 
INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Save namespace ...
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: NNStorage.attemptRestoreRemovedStorage: check removed(failed) storarge. removedStorages size =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: currently disabled dir  <*> ; type= <*> ;canwrite= <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: restoring dir  <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain  <*>  images with txid >=  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at  segmentTxId 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: NNStorage.attemptRestoreRemovedStorage: check removed(failed) storarge. removedStorages size =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: currently disabled dir  <*> ; type= <*> ;canwrite= <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.JournalSet: Disabling journal  j 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
<INIT> org.apache.hadoop.hdfs.server.namenode.FSImage: editLog must be initialized
INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with  <*>  entries  <*>  lookups 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in  ioe  msecs 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: set restore failed storage to  val 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: Error converting file to URI
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: Error converting file to URI
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: Error converting file to URI
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: Storage directory  <*>  has been successfully formatted. 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: Storage directory  <*>  has been successfully formatted. 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSEditLog: Closing log when already closed
WARN org.apache.hadoop.hdfs.server.namenode.FSEditLog: No class configured for  uriScheme ,  <*>  is empty 
WARN org.apache.hadoop.hdfs.server.namenode.FSEditLog: No class configured for  uriScheme ,  <*>  is empty 
ERROR org.apache.hadoop.hdfs.server.namenode.JournalSet: Disabling journal  j 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: Error converting file to URI
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: Error converting file to URI
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: Error converting file to URI
ERROR org.apache.hadoop.hdfs.server.namenode.FSEditLog: No edits directories configured!
ERROR org.apache.hadoop.hdfs.server.namenode.JournalSet: Disabling journal  j 
INFO org.apache.hadoop.hdfs.server.namenode.JournalSet: Skipping jas  jas  since it\'s disabled 
DEBUG org.apache.hadoop.hdfs.server.namenode.NameNode: Beginning to copy stream  stream  to shared edits 
INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment  <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.JournalSet: Disabling journal  j 
INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment  <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.JournalSet: Disabling journal  j 
ERROR org.apache.hadoop.hdfs.server.namenode.JournalSet: Error:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NameNode: ending log segment because of END_LOG_SEGMENT op in  stream 
DEBUG org.apache.hadoop.hdfs.server.namenode.NameNode: ending log segment because of end of stream in  stream 
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Waiting until the NameNode rolls its edit logs in order to freeze the BackupNode namespace.
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: BackupNode namespace frozen.
DEBUG org.apache.hadoop.hdfs.server.namenode.Checkpointer: Doing checkpoint. Last applied:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Unable to roll forward using only logs. Downloading image with txid  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to  <*> 
DEBUG org.apache.hadoop.hdfs.web.URLConnectionFactory: open URL connection
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to  <*>  milliseconds 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file  <*>  size  <*>  bytes. 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to  <*> 
DEBUG org.apache.hadoop.hdfs.web.URLConnectionFactory: open AuthenticatedURL connection url 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to  <*>  milliseconds 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file  <*>  size  <*>  bytes. 
WARN org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Unable to rename edits file from  <*>  to  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Dest file:  f 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to  <*> 
DEBUG org.apache.hadoop.hdfs.web.URLConnectionFactory: open AuthenticatedURL connection url 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to  <*>  milliseconds 
WARN org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Overwriting existing file  f  with file downloaded from  url 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file  <*>  size  <*>  bytes. 
DEBUG org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Renaming  <*>  to  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Unable to rename edits file from  <*>  to  <*> 
CLOSEALLSTREAMS org.apache.hadoop.hdfs.server.namenode.BackupImage:  <*> <*>
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from  <*>  stream(s). 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: About to load edits:\n   <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading  editIn  expecting start txid # <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: BUG: Diskspace quota violation in image for  <*>  quota =  <*>  < consumed =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with  <*>  entries  <*>  lookups 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: renaming   <*>  to  <*> 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
DEBUG org.apache.hadoop.hdfs.util.MD5FileUtils: Saved MD  digestString  to  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: renaming   <*>  to  <*> 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
DEBUG org.apache.hadoop.hdfs.util.MD5FileUtils: Saved MD  digestString  to  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: renaming   <*>  to  <*> 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
WARN org.apache.hadoop.hdfs.util.MD5FileUtils: deleting   <*>  FAILED 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain  <*>  images with txid >=  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Loading edits into backupnode to try to catch up from txid  <*>  to  <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.JournalSet: Disabling journal  j 
ERROR org.apache.hadoop.hdfs.server.namenode.JournalSet: Disabling journal  j 
INFO org.apache.hadoop.hdfs.server.namenode.JournalSet: Skipping jas  jas  since it\'s disabled 
INFO org.apache.hadoop.hdfs.server.namenode.JournalSet: Skipping jas  jas  since it\'s disabled 
INFO org.apache.hadoop.hdfs.server.namenode.JournalSet: Skipping jas  jas  since it\'s disabled 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: About to load edits:\n   <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading  editIn  expecting start txid # <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: BUG: Namespace quota violation in image for  <*>  quota =  <*>  < consumed =  child# 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: BUG: Diskspace quota violation in image for  <*>  quota =  <*>  < consumed =  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: BUG: Namespace quota violation in image for  <*>  quota =  <*>  < consumed =  child# 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: BUG: Diskspace quota violation in image for  <*>  quota =  <*>  < consumed =  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: BUG: Diskspace quota violation in image for  <*>  quota =  <*>  < consumed =  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: About to load edits:\n   <*> 
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Going to finish converging with remaining  editStreamsAll  txns from in-progress stream  stream 
CLOSEALLSTREAMS org.apache.hadoop.hdfs.server.namenode.BackupImage:  <*> <*>
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Successfully synced BackupNode with NameNode at txnid  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.BackupImage: State transition  <*>  ->  newState 
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Going to finish converging with remaining  editStreamsAll  txns from in-progress stream  stream 
CLOSEALLSTREAMS org.apache.hadoop.hdfs.server.namenode.BackupImage:  <*> <*>
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Successfully synced BackupNode with NameNode at txnid  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.BackupImage: State transition  <*>  ->  newState 
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Going to finish converging with remaining  editStreamsAll  txns from in-progress stream  stream 
CLOSEALLSTREAMS org.apache.hadoop.hdfs.server.namenode.BackupImage:  <*> <*>
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Successfully synced BackupNode with NameNode at txnid  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.BackupImage: State transition  <*>  ->  newState 
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpoint completed in  <*>  seconds.  New Image Size:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Waiting until the NameNode rolls its edit logs in order to freeze the BackupNode namespace.
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: BackupNode namespace frozen.
DEBUG org.apache.hadoop.hdfs.server.namenode.Checkpointer: Doing checkpoint. Last applied:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Loading image with txid  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: Reloading namespace from  file 
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from  <*>  stream(s). 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: About to load edits:\n   <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with  <*>  entries  <*>  lookups 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain  <*>  images with txid >=  <*> 
DEBUG org.apache.hadoop.hdfs.web.URLConnectionFactory: open AuthenticatedURL connection url 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to  <*>  milliseconds 
WARN org.apache.hadoop.hdfs.server.namenode.TransferFsImage: SIMULATING A CORRUPT BYTE IN IMAGE TRANSFER!
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Connection closed by client
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid  txid  to namenode at  fsName  in  <*>  seconds 
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpoint completed in  <*>  seconds.  New Image Size:  <*> 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.server.namenode.ClusterJspHelper: Cluster console encounters a not handled situtation.
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: Resolved path is  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
INFO org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : closing 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: demoteOldEvictable: demoting  <*> :  <*> :  <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica: closed  this suffix 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: demoteOldEvictable: demoting  <*> :  <*> :  <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : trimEvictionMaps is purging  replica <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: demoteOldEvictable: demoting  <*> :  <*> :  <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : trimEvictionMaps is purging  replica <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NameNode: Setting fs.defaultFS to  <*> 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: !!! WARNING !!!\n\tThe NameNode currently runs without persistent storage.\n\tAny changes to the file system meta-data may be lost.\n\tRecommended actions:\n\t\t- shutdown and restart NameNode with configured \ propertyName \ in hdfs-site.xml; \n\t\t- use Backup Node as a persistent and up-to-date storage  of the file system meta-data. 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Enabling async auditlog
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Logj is required to enable async auditlog
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: <*> 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.HeartbeatManager: Setting heartbeat recheck interval to  <*>  since  dfs.namenode.stale.datanode.interval  is less than  dfs.namenode.heartbeat.recheck-interval 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit= <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check= <*> 
WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: The given interval for marking stale datanode =  <*> , which is less than    heartbeat intervals. This may cause too frequent changes of  stale states of DataNodes since a heartbeat msg may be missing  due to temporary short-term failures. Reset stale interval to  <*> . 
INFO org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: dfs.namenode.startup.delay.block.deletion.sec is set to  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: The block deletion will start around  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable= <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         =  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             =  $i 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             =  $i 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      =  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval =  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        =  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled =  <*> 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Determined nameservice ID:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled:  <*> 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Configured NNs:\n <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: !!! WARNING !!!\n\tThe NameNode currently runs without persistent storage.\n\tAny changes to the file system meta-data may be lost.\n\tRecommended actions:\n\t\t- shutdown and restart NameNode with configured \ propertyName \ in hdfs-site.xml; \n\t\t- use Backup Node as a persistent and up-to-date storage  of the file system meta-data. 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: Error converting file to URI
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: Error converting file to URI
WARN org.apache.hadoop.hdfs.server.namenode.NameNode: Encountered exception during format: 
INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment  <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.JournalSet: Disabling journal  j 
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.Sender: Sending DataTransferOp  <*> :  proto 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm: this : unregisterSlot  slotIdx 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm: this : freed 
ERROR org.apache.hadoop.hdfs.NameNodeProxies: Unsupported protocol found when creating the proxy connection to NameNode:  <*> 
WARN org.apache.hadoop.hdfs.DFSUtil: hadoop.ssl.enabled is deprecated. Please use dfs.http.policy.
WARN org.apache.hadoop.hdfs.DFSUtil: dfs.https.enable is deprecated. Please use dfs.http.policy.
WARN org.apache.hadoop.hdfs.DFSUtil: hadoop.ssl.enabled is deprecated. Please use dfs.http.policy.
INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for  name  at:  <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer: WebHDFS and security are enabled, but configuration property \'dfs.web.authentication.kerberos.principal\' is not set.
INFO org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer: Added filter \' <*> \' (class= <*> ) 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Edits URI  i$  listed multiple times in  dfs.namenode.shared.edits.dir  and  dfs.namenode.edits.dir . Ignoring duplicates. 
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: !!! WARNING !!!\n\tThe NameNode currently runs without persistent storage.\n\tAny changes to the file system meta-data may be lost.\n\tRecommended actions:\n\t\t- shutdown and restart NameNode with configured \ propertyName \ in hdfs-site.xml; \n\t\t- use Backup Node as a persistent and up-to-date storage  of the file system meta-data. 
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: !!! WARNING !!!\n\tThe NameNode currently runs without persistent storage.\n\tAny changes to the file system meta-data may be lost.\n\tRecommended actions:\n\t\t- shutdown and restart NameNode with configured \ propertyName \ in hdfs-site.xml; \n\t\t- use Backup Node as a persistent and up-to-date storage  of the file system meta-data. 
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: !!! WARNING !!!\n\tThe NameNode currently runs without persistent storage.\n\tAny changes to the file system meta-data may be lost.\n\tRecommended actions:\n\t\t- shutdown and restart NameNode with configured \ propertyName \ in hdfs-site.xml; \n\t\t- use Backup Node as a persistent and up-to-date storage  of the file system meta-data. 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: set restore failed storage to  val 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Enabling async auditlog
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Logj is required to enable async auditlog
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: <*> 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.HeartbeatManager: Setting heartbeat recheck interval to  <*>  since  dfs.namenode.stale.datanode.interval  is less than  dfs.namenode.heartbeat.recheck-interval 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit= <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check= <*> 
WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: The given interval for marking stale datanode =  <*> , which is less than    heartbeat intervals. This may cause too frequent changes of  stale states of DataNodes since a heartbeat msg may be missing  due to temporary short-term failures. Reset stale interval to  <*> . 
WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: The given interval for marking stale datanode =  staleInterval , which is larger than heartbeat expire interval  heartbeatExpireInterval . 
INFO org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: dfs.namenode.startup.delay.block.deletion.sec is set to  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: The block deletion will start around  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable= <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         =  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             =  $i 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             =  $i 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      =  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval =  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        =  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled =  <*> 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Caching file names occuring more than  <*>  times 
INFO org.apache.hadoop.hdfs.server.namenode.CacheManager: Using minimum value {} for {}
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: dfs.namenode.safemode.threshold-pct =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: dfs.namenode.safemode.min.datanodes =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: dfs.namenode.safemode.extension     =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNConf: ACLs enabled?  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNConf: XAttrs enabled?  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNConf: Maximum size of an xattr:  <*> <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: current cluster id for sd= <*> ;lv= <*> ;cid= <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: couldn\'t find any VERSION file containing valid ClusterId
INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Allocated new BlockPoolId:  <*> 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: Storage directory  <*>  has been successfully formatted. 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: Storage directory  <*>  has been successfully formatted. 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: Storage directory  <*>  has been successfully formatted. 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: <*> is not a directory 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Cannot access storage directory  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Lock on  <*>  acquired by nodename  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Cannot lock storage  <*> . The directory is already locked 
ERROR org.apache.hadoop.hdfs.server.common.Storage: *********** Upgrade is not supported from this  older version  oldVersion  of storage to the current version.  Please upgrade to  Hadoop-.  or a later version and then upgrade to current  version. Old layout version is  <*>  and latest layout version this software version can  upgrade from is  $i . ************ 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: Using clusterid:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image?  <*>#_  (staleImage= <*> , haEnabled= <*> , isRollingUpgrade= <*> ) 
<INIT> org.apache.hadoop.hdfs.server.namenode.FSImage: editLog must be initialized
INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with  <*>  entries  <*>  lookups 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in  ioe  msecs 
INFO org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: Service RPC server is binding to  bindHost : <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Setting ADDRESS  address 
INFO org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: RPC server is binding to  serviceHandlerCount : <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use  <*>  to access  this namenode/service. 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: !!! WARNING !!!\n\tThe NameNode currently runs without persistent storage.\n\tAny changes to the file system meta-data may be lost.\n\tRecommended actions:\n\t\t- shutdown and restart NameNode with configured \ propertyName \ in hdfs-site.xml; \n\t\t- use Backup Node as a persistent and up-to-date storage  of the file system meta-data. 
WARN org.apache.hadoop.hdfs.DFSUtil: hadoop.ssl.enabled is deprecated. Please use dfs.http.policy.
WARN org.apache.hadoop.hdfs.DFSUtil: dfs.https.enable is deprecated. Please use dfs.http.policy.
WARN org.apache.hadoop.hdfs.DFSUtil: dfs.https.enable is deprecated. Please use dfs.http.policy.
INFO org.apache.hadoop.hdfs.DFSUtil: Starting web server as:  <*> 
INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for  name  at:  <*> 
INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for  name  at:  <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer: WebHDFS and security are enabled, but configuration property \'dfs.web.authentication.kerberos.principal\' is not set.
INFO org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer: Added filter \' <*> \' (class= <*> ) 
INFO org.apache.hadoop.hdfs.server.namenode.NameNode: <*>  RPC up at:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NameNode: <*>  service RPC up at:  <*> 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.server.namenode.FSEditLog: No class configured for  uriScheme ,  <*>  is empty 
WARN org.apache.hadoop.hdfs.server.namenode.FSEditLog: No class configured for  uriScheme ,  <*>  is empty 
WARN org.apache.hadoop.hdfs.server.namenode.FSEditLog: No class configured for  uriScheme ,  <*>  is empty 
ERROR org.apache.hadoop.hdfs.server.namenode.JournalSet: Disabling journal  j 
ERROR org.apache.hadoop.hdfs.server.namenode.JournalSet: Error:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.CheckpointConf: Configuration key  key  is deprecated! Ignoring...  Instead please specify a value for  dfs.namenode.checkpoint.txns 
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpoint Period :  <*>  secs  ( <*>  min) 
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Transactions count is  :  <*> , to trigger checkpoint 
INFO org.apache.hadoop.hdfs.qjournal.server.JournalNode: Initializing journal in directory  <*> 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Cannot access storage directory  <*> 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Cannot access storage directory  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Completing previous finalize for storage directory  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Scanning storage  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: No files in  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Updating lastPromisedEpoch from  <*>  to  newEpoch  for client  <*> 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file  <*>  ->  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSNamesystem: DIR* NameSystem.mkdirs:  srcArg 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: Resolved path is  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
DEBUG org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker: ACCESS CHECK:  this , doCheckOwner= doCheckOwner , ancestorAccess= ancestorAccess , parentAccess= parentAccess , access= access , subAccess= subAccess , ignoreEmptyDir= ignoreEmptyDir , resolveLink= resolveLink 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.JournalNode: Initializing journal in directory  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Lock on  <*>  acquired by nodename  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Cannot lock storage  <*> . The directory is already locked 
ERROR org.apache.hadoop.hdfs.server.common.Storage: *********** Upgrade is not supported from this  older version  oldVersion  of storage to the current version.  Please upgrade to  Hadoop-.  or a later version and then upgrade to current  version. Old layout version is  <*>  and latest layout version this software version can  upgrade from is  $i . ************ 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Recovering storage directory  <*>  from previous upgrade 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Scanning storage  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Latest log is  latestLog 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: getSegmentInfo( segmentTxId ):  <*>  ->  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Skipping download of log  <*> : already have up-to-date logs 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Accepted recovery for segment  <*> :  <*> 
INFO org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : closing 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica: this  is stale because it\'s  stale#  ms old, and staleThresholdMs =  <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica: closed  this suffix 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: demoteOldEvictable: demoting  <*> :  <*> :  <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : trimEvictionMaps is purging  replica <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : unref replica  replica :  purgeReason  refCount  <*>  ->  <*> <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  priLevel 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  priLevel 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  i 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  i 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* Removing  <*>  from neededReplications as it has enough replicas 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* ask  <*>  to replicate  <*>  to  $u 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* neededReplications =  <*>  pendingReplications =  <*> 
FATAL org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: BLOCK* NameSystem.getDatanode:  <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: Block deletion is delayed during NameNode startup. The deletion will start after  <*>  ms. 
FATAL org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: BLOCK* NameSystem.getDatanode:  <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: Block deletion is delayed during NameNode startup. The deletion will start after  <*>  ms. 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK*  <*> : ask  dn  to delete  <*> 
FATAL org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: BLOCK* NameSystem.getDatanode:  <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: Block deletion is delayed during NameNode startup. The deletion will start after  <*>  ms. 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* Removing  block  from neededReplications as it has enough replicas 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* block  <*>  is moved from neededReplications to pendingReplications 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.add: block  has only  curReplicas  replicas and need  expectedReplicas  replicas so is added to neededReplications  at priority level  <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.add: block  has only  curReplicas  replicas and need  expectedReplicas  replicas so is added to neededReplications  at priority level  <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.add: block  has only  curReplicas  replicas and need  expectedReplicas  replicas so is added to neededReplications  at priority level  <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.add: block  has only  curReplicas  replicas and need  expectedReplicas  replicas so is added to neededReplications  at priority level  <*> 
WARN org.apache.hadoop.hdfs.DFSInputStream: Found Checksum error for  <*>  from  <*>  at  <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer: Connecting to datanode  <*> 
WARN org.apache.hadoop.hdfs.DFSInputStream: <*> <*> . Throwing a BlockMissingException 
DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer: Connecting to datanode  <*> 
WARN org.apache.hadoop.hdfs.DFSInputStream: <*> <*> . Throwing a BlockMissingException 
TRACE org.apache.hadoop.hdfs.BlockReaderFactory: this : trying to construct a BlockReaderLocal  for short-circuit reads. 
TRACE org.apache.hadoop.hdfs.DFSClient: Address  targetAddr <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : found waitable for  key 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : replica  refCount  <*>  ->  <*> <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : loading  key 
DEBUG org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : starting cache cleaner thread which will run  every  <*>  ms 
TRACE org.apache.hadoop.hdfs.DFSClient: Address  targetAddr <*> 
TRACE org.apache.hadoop.hdfs.BlockReaderFactory: this : trying to create a remote block reader from the  UNIX domain socket at  <*> 
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.Sender: Sending DataTransferOp  <*> :  proto 
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.Sender: Sending DataTransferOp  <*> :  proto 
TRACE org.apache.hadoop.hdfs.DFSClient: Address  targetAddr <*> 
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.Sender: Sending DataTransferOp  <*> :  proto 
TRACE org.apache.hadoop.hdfs.DFSClient: Address  targetAddr <*> 
TRACE org.apache.hadoop.hdfs.BlockReaderFactory: this : got security exception while constructing  a remote block reader from the unix domain socket at  <*> 
TRACE org.apache.hadoop.hdfs.BlockReaderFactory: this : returning new remote block reader using  UNIX domain socket on  <*> 
TRACE org.apache.hadoop.hdfs.DFSClient: Address  targetAddr <*> 
TRACE org.apache.hadoop.hdfs.BlockReaderFactory: this : trying to create a remote block reader from the  UNIX domain socket at  <*> 
DEBUG org.apache.hadoop.hdfs.BlockReaderFactory: Closed potentially stale domain peer  <*> 
TRACE org.apache.hadoop.hdfs.BlockReaderFactory: this : returning new remote block reader using  UNIX domain socket on  <*> 
INFO org.apache.hadoop.hdfs.DFSInputStream: Will fetch a new encryption key and retry, encryption key was invalid when connecting to  <*>  :  <*> 
DEBUG org.apache.hadoop.hdfs.DFSClient: Clearing encryption key
DEBUG org.apache.hadoop.hdfs.DFSClient: Clearing encryption key
DEBUG org.apache.hadoop.hdfs.DFSClient: Clearing encryption key
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
INFO org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: BLOCK*  <*> : add  block  to  datanode 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: BLOCK*  <*> : add  block  to  datanode 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  priLevel 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* addToInvalidates:  b   <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  i 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  i 
DEBUG org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove: Start moving  this 
DEBUG org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient: SASL client skipping handshake in secured configuration with no SASL protection configured for addr = {}, datanodeId = {}
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.Sender: Sending DataTransferOp  <*> :  proto 
INFO org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove: Successfully moved  this 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Block  block  cannot be repl from any node 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* block  <*>  is moved from neededReplications to pendingReplications 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* ask  <*>  to replicate  <*>  to  $u 
FATAL org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: BLOCK* NameSystem.getDatanode:  <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: Block deletion is delayed during NameNode startup. The deletion will start after  <*>  ms. 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK*  <*> : ask  dn  to delete  <*> 
FATAL org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: BLOCK* NameSystem.getDatanode:  <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: Block deletion is delayed during NameNode startup. The deletion will start after  <*>  ms. 
FATAL org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: BLOCK* NameSystem.getDatanode:  <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: Block deletion is delayed during NameNode startup. The deletion will start after  <*>  ms. 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK*  <*> : ask  dn  to delete  <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Block  block  cannot be repl from any node 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  priLevel 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  i 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  i 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* Removing  <*>  from neededReplications as it has enough replicas 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  i 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  priLevel 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* Removing  block  from neededReplications as it has enough replicas 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* Removing  <*>  from neededReplications as it has enough replicas 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* neededReplications =  <*>  pendingReplications =  <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.add: block  has only  curReplicas  replicas and need  expectedReplicas  replicas so is added to neededReplications  at priority level  <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.add: block  has only  curReplicas  replicas and need  expectedReplicas  replicas so is added to neededReplications  at priority level  <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.add: block  has only  curReplicas  replicas and need  expectedReplicas  replicas so is added to neededReplications  at priority level  <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.add: block  has only  curReplicas  replicas and need  expectedReplicas  replicas so is added to neededReplications  at priority level  <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.add: block  has only  curReplicas  replicas and need  expectedReplicas  replicas so is added to neededReplications  at priority level  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.JournalNode: Initializing journal in directory  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: <*>  does not exist. Creating ... 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: <*> is not a directory 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Cannot access storage directory  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Scanning storage  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Latest log is  latestLog 
WARN org.apache.hadoop.hdfs.qjournal.server.Journal: Latest log  latestLog  has no transactions.  moving it aside and looking for previous log 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: No files in  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Updating lastPromisedEpoch from  <*>  to  newEpoch  for client  <*> 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Rolling forward previously half-completed synchronization:  <*>  ->  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Edit log file  <*>  appears to be empty.  Moving it aside... 
DEBUG org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$CacheCleaner: this : cache cleaner running at  <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: demoteOldEvictable: demoting  <*> :  <*> :  <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica: closed  this suffix 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : unref replica  replica :  purgeReason  refCount  <*>  ->  <*> <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica: this  is stale because it\'s  stale#  ms old, and staleThresholdMs =  <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica: closed  this suffix 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : unref replica  replica :  purgeReason  refCount  <*>  ->  <*> <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: demoteOldEvictable: demoting  <*> :  <*> :  <*> 
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.Sender: Sending DataTransferOp  <*> :  proto 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$SlotReleaser: <*> : released  <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm: this : unregisterSlot  slotIdx 
TRACE org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager$EndpointShmManager: this : freeing empty stale  shm 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm: this : freed 
DEBUG org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache: Cancelling caching for block with id {}, pool {}.
DEBUG org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache: Block with id {}, pool {} does not need to be uncached, because it is not currently in the mappableBlockMap.
DEBUG org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache: Block with id {}, pool {} does not need to be uncached, because it is in state {}.
DEBUG org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager: Checking access for user= userId , block= block , access mode= mode  using  <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer: <*> :  b  (numBytes= <*> ) , stage= stage , clientname= clientname , targets= <*> , target storage types= <*> 
DEBUG org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient: SASL client skipping handshake in secured configuration with unsecured cluster for addr = {}, datanodeId = {}
DEBUG org.apache.hadoop.hdfs.server.datanode.BlockSender: Bumping up the client provided block\'s genstamp to latest  <*>  for block  block 
DEBUG org.apache.hadoop.hdfs.server.datanode.BlockSender: block= block , replica= <*> 
WARN org.apache.hadoop.hdfs.server.datanode.BlockSender: Could not find metadata file for  block 
DEBUG org.apache.hadoop.hdfs.server.datanode.BlockSender: replica= <*> 
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.Sender: Sending DataTransferOp  <*> :  proto 
INFO org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer: <*> : Transmitted  <*>  (numBytes= <*> ) to  <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer: <*> : close-ack= <*> 
INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode  <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.DataXceiver: Checking block access token for block \' <*> \' with mode \' mode \' 
DEBUG org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager: Checking access for user= userId , block= block , access mode= mode  using  <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.BlockSender: Bumping up the client provided block\'s genstamp to latest  <*>  for block  block 
DEBUG org.apache.hadoop.hdfs.server.datanode.BlockSender: block= block , replica= <*> 
WARN org.apache.hadoop.hdfs.server.datanode.BlockMetadataHeader: Unexpected meta-file version for  name : version in file is  $i  but expected version is   
DEBUG org.apache.hadoop.hdfs.server.datanode.BlockSender: replica= <*> 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: Storage directory  <*>  has been successfully formatted. 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: Storage directory  <*>  has been successfully formatted. 
WARN org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer: Invalid namespaceID in journal request - expected  <*>  actual  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: NameNode started a new log segment at txid  txid 
WARN org.apache.hadoop.hdfs.server.namenode.BackupImage: NN started new log segment at txid  txid , but BN had only written up to txid  <*> in the log segment starting at  <*> . Aborting this  log segment. 
INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at  segmentTxId 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: NNStorage.attemptRestoreRemovedStorage: check removed(failed) storarge. removedStorages size =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: currently disabled dir  <*> ; type= <*> ;canwrite= <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.JournalSet: Disabling journal  j 
ERROR org.apache.hadoop.hdfs.server.namenode.JournalSet: Error:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.BackupImage: State transition  <*>  ->  newState 
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Stopped applying edits to prepare for checkpoint.
DEBUG org.apache.hadoop.hdfs.server.namenode.BackupImage: State transition  <*>  ->  newState 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: storageInfo  failed. 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: storageInfo  failed. 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of storages reported in heartbeat= <*> ; Number of storages in storageMap= <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Removed storage  len$  from DataNode this 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: getDatanodeListForReport with includedNodes =  <*> , excludedNodes =  <*> , foundNodes =  <*> , nodes =  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Stopped plug-in  p 
INFO org.apache.hadoop.hdfs.server.datanode.DataXceiver: Sending OOB to peer:  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder: Sending an out of band ack of type  ackStatus 
DEBUG org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder: <*> , replyAck= $u 
INFO org.apache.hadoop.hdfs.server.datanode.DataXceiver: Sending OOB to peer:  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.DataXceiver: Sending OOB to peer:  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder: Cannot send OOB response  ackStatus . Responder not running. 
WARN org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: DirectoryScanner: shutdown has been called
INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Waiting for threadgroup to exit, active threads is  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Shutdown complete.
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver: readNextPacket: dataPlusChecksumLen =  dataPlusChecksumLen  headerLen =  $i 
TRACE org.apache.hadoop.hdfs.RemoteBlockReader2: Reading empty packet at end of read
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver: readNextPacket: dataPlusChecksumLen =  dataPlusChecksumLen  headerLen =  $i 
INFO org.apache.hadoop.hdfs.qjournal.server.JournalNode: Initializing journal in directory  <*> 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: <*> is not a directory 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Cannot access storage directory  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Scanning storage  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Latest log is  latestLog 
WARN org.apache.hadoop.hdfs.qjournal.server.Journal: Latest log  latestLog  has no transactions.  moving it aside and looking for previous log 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: No files in  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Updating lastPromisedEpoch from  <*>  to  newEpoch  for client  <*> 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.JNStorage: Purging no-longer needed file  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.JNStorage: Purging no-longer needed file  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool  bpid 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList: Total time to scan all replicas for block pool  bpid :  arr$# ms 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList: Total time to add all replicas to map:  arr$# ms 
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeResourceMonitor: NameNode low on available disk space.  Entering safe mode. 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: STATE* Safe mode is ON <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: STATE* Safe mode is ON <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: STATE* Safe mode is ON <*> 
DEBUG org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager: Checking access for user= userId , block= block , access mode= mode  using  <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode: Got:  <*> 
DEBUG org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager: Checking access for user= userId , block= block , access mode= mode  using  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: Resolved path is  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* fsync:  src  for  clientName 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: Resolved path is  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSNamesystem: persistBlocks:  path  with  <*>  blocks is persisted to  the file system 
INFO org.apache.hadoop.hdfs.server.datanode.BPServiceActor: <*> : scheduling an incremental block report. 
INFO org.apache.hadoop.hdfs.server.datanode.BPServiceActor: <*> : scheduling an incremental block report. 
INFO org.apache.hadoop.hdfs.server.datanode.BPServiceActor: <*> : scheduling a full block report. 
WARN org.apache.hadoop.hdfs.DFSInputStream: Found Checksum error for  <*>  from  <*>  at  <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer: Connecting to datanode  <*> 
WARN org.apache.hadoop.hdfs.DFSInputStream: <*> <*> . Throwing a BlockMissingException 
DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer: Connecting to datanode  <*> 
WARN org.apache.hadoop.hdfs.DFSInputStream: <*> <*> . Throwing a BlockMissingException 
DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer: Connecting to datanode  <*> 
WARN org.apache.hadoop.hdfs.DFSInputStream: <*> <*> . Throwing a BlockMissingException 
TRACE org.apache.hadoop.hdfs.DFSClient: Address  targetAddr <*> 
DEBUG org.apache.hadoop.hdfs.BlockReaderFactory: this :  <*>  is not  usable for short circuit; giving up on BlockReaderLocal. 
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.Sender: Sending DataTransferOp  <*> :  proto 
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.Sender: Sending DataTransferOp  <*> :  proto 
TRACE org.apache.hadoop.hdfs.DFSClient: Address  targetAddr <*> 
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.Sender: Sending DataTransferOp  <*> :  proto 
TRACE org.apache.hadoop.hdfs.DFSClient: Address  targetAddr <*> 
TRACE org.apache.hadoop.hdfs.BlockReaderFactory: this : trying to construct a BlockReaderLocal  for short-circuit reads. 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica: this  is stale because it\'s  stale#  ms old, and staleThresholdMs =  <*> 
INFO org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : got stale replica  <*> .  Removing  this replica from the replicaInfoMap and retrying. 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica: this  is not stale because it\'s only  stale#  ms old, and staleThresholdMs =  <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : unref replica  replica :  purgeReason  refCount  <*>  ->  <*> <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: demoteOldEvictable: demoting  <*> :  <*> :  <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : loading  key 
WARN org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : failed to load  key 
DEBUG org.apache.hadoop.hdfs.BlockReaderFactory: this : failed to get  ShortCircuitReplica. Cannot construct  BlockReaderLocal via  <*> 
TRACE org.apache.hadoop.hdfs.BlockReaderFactory: this : trying to construct BlockReaderLocalLegacy 
TRACE org.apache.hadoop.hdfs.DFSClient: Address  targetAddr <*> 
DEBUG org.apache.hadoop.hdfs.BlockReaderLocalLegacy: Cached location of block  blk  as  <*> 
DEBUG org.apache.hadoop.hdfs.BlockReaderLocalLegacy: New BlockReaderLocalLegacy for file  <*>  of size  <*>  startOffset  startOffset  length  length  short circuit checksum  <*> 
WARN org.apache.hadoop.hdfs.server.datanode.BlockMetadataHeader: Unexpected meta-file version for  name : version in file is  $i  but expected version is   
TRACE org.apache.hadoop.hdfs.DFSClient: Address  targetAddr <*> 
DEBUG org.apache.hadoop.hdfs.BlockReaderFactory: this : not trying to create a  remote block reader because the UNIX domain socket at  <*>  is not usable. 
TRACE org.apache.hadoop.hdfs.BlockReaderFactory: this : returning new remote block reader using  UNIX domain socket on  <*> 
WARN org.apache.hadoop.hdfs.DFSInputStream: Failed to connect to  <*>  for block , add to deadNodes and continue.  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: Resolved path is  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: Resolved path is  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
DEBUG org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl$LazyWriter: LazyWriter: Start persisting RamDisk block: block pool Id:  <*>  block id:  <*>  on target volume  <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskAsyncLazyPersistService: LazyWriter schedule async task to persist RamDisk block pool id:  bpId  block id:  blockId 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskAsyncLazyPersistService: LazyWriter failed to create  <*> 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskAsyncLazyPersistService: LazyWriter failed to create  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Moved  blockFile  to  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Moved  metaFile  to  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Moved  blockFile  to  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Moved  metaFile  to  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Moved  blockFile  to  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Moved  metaFile  to  <*> 
WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Cannot find BPOfferService for reporting block received for bpid= <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl$LazyWriter: Evicting block  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Moved  blockFile  to  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Moved  metaFile  to  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Moved  blockFile  to  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Moved  metaFile  to  <*> 
INFO org.apache.hadoop.hdfs.DFSClient$Renewer: Cancelling  <*> 
DEBUG org.apache.hadoop.hdfs.NameNodeProxies: Couldn\'t create proxy provider  failoverProxyProviderClass 
WARN org.apache.hadoop.hdfs.DFSClient$Conf: Bad checksum type:  <*> . Using default  CRCC 
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Waiting until the NameNode rolls its edit logs in order to freeze the BackupNode namespace.
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: BackupNode namespace frozen.
DEBUG org.apache.hadoop.hdfs.server.namenode.Checkpointer: Doing checkpoint. Last applied:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Unable to roll forward using only logs. Downloading image with txid  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to  <*> 
DEBUG org.apache.hadoop.hdfs.web.URLConnectionFactory: open AuthenticatedURL connection url 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to  <*>  milliseconds 
WARN org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Overwriting existing file  f  with file downloaded from  url 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file  <*>  size  <*>  bytes. 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
DEBUG org.apache.hadoop.hdfs.util.MD5FileUtils: Saved MD  digestString  to  <*> 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
DEBUG org.apache.hadoop.hdfs.util.MD5FileUtils: Saved MD  digestString  to  <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: renaming   <*>  to  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: renaming   <*>  to  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: renaming   <*>  to  <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Dest file:  f 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to  <*> 
DEBUG org.apache.hadoop.hdfs.web.URLConnectionFactory: open URL connection
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to  <*>  milliseconds 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file  <*>  size  <*>  bytes. 
DEBUG org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Dest file:  f 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to  <*> 
DEBUG org.apache.hadoop.hdfs.web.URLConnectionFactory: open URL connection
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to  <*>  milliseconds 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file  <*>  size  <*>  bytes. 
DEBUG org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Renaming  <*>  to  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Unable to rename edits file from  <*>  to  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from  <*>  stream(s). 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: About to load edits:\n   <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading  editIn  expecting start txid # <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: BUG: Diskspace quota violation in image for  <*>  quota =  <*>  < consumed =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with  <*>  entries  <*>  lookups 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: msg  \n <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSImage: Unable to delete cancelled checkpoint in  sd 
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpoint completed in  <*>  seconds.  New Image Size:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: Resolved path is  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker: ACCESS CHECK:  this , doCheckOwner= doCheckOwner , ancestorAccess= ancestorAccess , parentAccess= parentAccess , access= access , subAccess= subAccess , ignoreEmptyDir= ignoreEmptyDir , resolveLink= resolveLink 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for standby state
INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Not going to trigger log rolls on active node because dfs.ha.log-roll.period is negative.
DEBUG org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: logRollPeriodMs= <*>  sleepTime= <*> 
WARN org.apache.hadoop.hdfs.server.namenode.CheckpointConf: Configuration key  key  is deprecated! Ignoring...  Instead please specify a value for  dfs.namenode.checkpoint.txns 
WARN org.apache.hadoop.hdfs.DFSUtil: dfs.https.enable is deprecated. Please use dfs.http.policy.
DEBUG org.apache.hadoop.hdfs.server.namenode.NameNode: Setting fs.defaultFS to  <*> 
WARN org.apache.hadoop.hdfs.DFSUtil: dfs.https.enable is deprecated. Please use dfs.http.policy.
INFO org.apache.hadoop.hdfs.server.namenode.ha.StandbyCheckpointer: Starting standby checkpoint thread...\nCheckpointing active NN at  <*> \n Serving checkpoints at  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.DataNode: who  calls recoverBlock( <*> , targets=[ <*> ] , newGenerationStamp= <*> ) 
INFO org.apache.hadoop.hdfs.server.datanode.DataNode: who  calls recoverBlock( <*> , targets=[ <*> ] , newGenerationStamp= <*> ) 
DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode: Connecting to datanode  <*>  addr= <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode: Connecting to datanode  <*>  addr= <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode: block= <*> , (length= <*> ), syncList= syncList 
DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode: Connecting to datanode  <*>  addr= <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode: Connecting to datanode  <*>  addr= <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode: Connecting to datanode  <*>  addr= <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode: block= <*> , (length= <*> ), syncList= syncList 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: Resolved path is  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: Resolved path is  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
DEBUG org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker: ACCESS CHECK:  this , doCheckOwner= doCheckOwner , ancestorAccess= ancestorAccess , parentAccess= parentAccess , access= access , subAccess= subAccess , ignoreEmptyDir= ignoreEmptyDir , resolveLink= resolveLink 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: DIR* FSDirectory.delete:  src 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: DIR* FSDirectory.unprotectedDelete: failed to remove  src  because the root is not allowed to be deleted 
DEBUG org.apache.hadoop.hdfs.server.namenode.LeaseManager: <*> .findLease: prefix= prefix 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Adjusting safe-mode totals for deletion.decreasing safeBlocks by  numRemovedSafe , totalBlocks by  numRemovedComplete 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: STATE* Leaving safe mode after  <*>  secs 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: STATE* Network topology has  <*>  racks and  <*>  datanodes 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: STATE* UnderReplicatedBlocks has  <*>  blocks 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSNamesystem: mkdirs: created directory  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSNamesystem: DIR* NameSystem.startFile: added  src  inode  <*>   holder 
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: DIR* NameSystem.startFile:  src   <*> 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: BLOCK*  <*> : add  block  to  datanode 
INFO org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: BLOCK*  <*> : add  block  to  datanode 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* addToInvalidates:  b   <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  priLevel 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* addToInvalidates:  b   <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  priLevel 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* addToInvalidates:  b   <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  priLevel 
DEBUG org.apache.hadoop.hdfs.DFSClient: src : masked= <*> 
DEBUG org.apache.hadoop.hdfs.DFSOutputStream: Set non-null progress callback on DFSOutputStream  src 
DEBUG org.apache.hadoop.hdfs.DFSOutputStream: computePacketChunkSize: src= <*> , chunkSize= chunkSize , chunksPerPacket= <*> , packetSize= <*> 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from  <*>  to  volFailures 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: storageInfo  failed. 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from  <*>  to  volFailures 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: storageInfo  failed. 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Deferring removal of stale storage  len$  with  <*>  blocks 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: Resolved path is  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker: ACCESS CHECK:  this , doCheckOwner= doCheckOwner , ancestorAccess= ancestorAccess , parentAccess= parentAccess , access= access , subAccess= subAccess , ignoreEmptyDir= ignoreEmptyDir , resolveLink= resolveLink 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool  bpid 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList: Total time to scan all replicas for block pool  bpid :  arr$# ms 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList: Total time to add all replicas to map:  arr$# ms 
TRACE org.apache.hadoop.hdfs.server.namenode.CacheManager: Validating directive {} pool maxRelativeExpiryTime {}
INFO org.apache.hadoop.hdfs.server.namenode.CacheManager: modifyDirective of {} successfully applied {}.
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber: Removing lazyPersist file  <*>  with no replicas. 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: Resolved path is  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: DIR* FSDirectory.delete:  src 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: DIR* FSDirectory.unprotectedDelete: failed to remove  src  because it does not exist 
DEBUG org.apache.hadoop.hdfs.server.namenode.LeaseManager: <*> .findLease: prefix= prefix 
DEBUG org.apache.hadoop.hdfs.server.namenode.LeaseManager: <*> .removeLeaseWithPrefixPath: entry= entry 
DEBUG org.apache.hadoop.hdfs.server.namenode.LeaseManager: src  not found in lease.paths (= <*> ) 
ERROR org.apache.hadoop.hdfs.server.namenode.LeaseManager: lease  not found in sortedLeases 
ERROR org.apache.hadoop.hdfs.server.namenode.LeaseManager: lease  not found in sortedLeases 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: BLOCK*  <*> : add  block  to  datanode 
INFO org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: BLOCK*  <*> : add  block  to  datanode 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  i 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* addToInvalidates:  b   <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  i 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSNamesystem: DIR* Namesystem.delete:  <*>  is removed 
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber: Removing lazyPersist file  <*>  with no replicas. 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.DFSClient: Failed to renew lease for  <*>  for  <*>  seconds (>= hard-limit = L  seconds.)  Closing all files being written ... 
WARN org.apache.hadoop.hdfs.DFSClient: Failed to renew lease for  <*>  for  <*>  seconds (>= hard-limit = L  seconds.)  Closing all files being written ... 
DEBUG org.apache.hadoop.hdfs.LeaseRenewer: Lease renewed for client  <*> 
DEBUG org.apache.hadoop.hdfs.LeaseRenewer: Lease renewer daemon for  <*>  with renew id  id  executed 
DEBUG org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient: SASL client skipping handshake in secured configuration with unsecured cluster for addr = {}, datanodeId = {}
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.Sender: Sending DataTransferOp  <*> :  proto 
INFO org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove: Successfully moved  this 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
DEBUG org.apache.hadoop.hdfs.server.namenode.NameNode: Setting fs.defaultFS to  <*> 
ERROR org.apache.hadoop.hdfs.NameNodeProxies: Unsupported protocol found when creating the proxy connection to NameNode:  <*> 
WARN org.apache.hadoop.hdfs.DFSUtil: dfs.https.enable is deprecated. Please use dfs.http.policy.
DEBUG org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Will connect to NameNode at  <*> 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: NNStorage.attemptRestoreRemovedStorage: check removed(failed) storarge. removedStorages size =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: currently disabled dir  <*> ; type= <*> ;canwrite= <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: <*>  does not exist. Creating ... 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: <*> is not a directory 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Cannot access storage directory  <*> 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: <*> is not a directory 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Cannot access storage directory  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage: Failed to delete temporary edits file:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Enabling async auditlog
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Logj is required to enable async auditlog
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: <*> 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.HeartbeatManager: Setting heartbeat recheck interval to  <*>  since  dfs.namenode.stale.datanode.interval  is less than  dfs.namenode.heartbeat.recheck-interval 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit= <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check= <*> 
WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: The given interval for marking stale datanode =  <*> , which is less than    heartbeat intervals. This may cause too frequent changes of  stale states of DataNodes since a heartbeat msg may be missing  due to temporary short-term failures. Reset stale interval to  <*> . 
INFO org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: dfs.namenode.startup.delay.block.deletion.sec is set to  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: The block deletion will start around  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable= <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.key.update.interval= <*>  min(s),  dfs.block.access.token.lifetime = <*>  min(s),  dfs.encrypt.data.transfer.algorithm = <*> 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         =  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             =  $i 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             =  $i 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      =  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval =  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        =  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Determined nameservice ID:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Caching file names occuring more than  <*>  times 
INFO org.apache.hadoop.hdfs.server.namenode.CacheManager: Using minimum value {} for {}
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: dfs.namenode.safemode.threshold-pct =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: dfs.namenode.safemode.min.datanodes =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: dfs.namenode.safemode.extension     =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNConf: ACLs enabled?  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNConf: XAttrs enabled?  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNConf: Maximum size of an xattr:  <*> <*> 
WARN org.apache.hadoop.hdfs.server.namenode.CheckpointConf: Configuration key  key  is deprecated! Ignoring...  Instead please specify a value for  dfs.namenode.checkpoint.txns 
WARN org.apache.hadoop.hdfs.DFSUtil: dfs.https.enable is deprecated. Please use dfs.http.policy.
INFO org.apache.hadoop.hdfs.DFSUtil: Starting web server as:  <*> 
INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for  name  at:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
WARN org.apache.hadoop.hdfs.DFSUtil: hadoop.ssl.enabled is deprecated. Please use dfs.http.policy.
WARN org.apache.hadoop.hdfs.DFSUtil: dfs.https.enable is deprecated. Please use dfs.http.policy.
INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   : <*>  secs  ( <*>  min) 
INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    : <*>  txns 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
DEBUG org.apache.hadoop.hdfs.web.TokenAspect: Created new DT for  <*> 
TRACE org.apache.hadoop.hdfs.web.WebHdfsFileSystem: url= <*> 
DEBUG org.apache.hadoop.hdfs.web.URLConnectionFactory: open URL connection
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* addToInvalidates:  b   <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  priLevel 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: BLOCK*  <*> : add  block  to  datanode 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  i 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  i 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: getDatanodeListForReport with includedNodes =  <*> , excludedNodes =  <*> , foundNodes =  <*> , nodes =  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* NameSystem.abandonBlock:  b of file  src 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: Resolved path is  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NamenodeFsck: FSCK started by  <*>  from  <*>  for path  <*>  at  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: Resolved path is  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: Resolved path is  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: Resolved path is  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker: ACCESS CHECK:  this , doCheckOwner= doCheckOwner , ancestorAccess= ancestorAccess , parentAccess= parentAccess , access= access , subAccess= subAccess , ignoreEmptyDir= ignoreEmptyDir , resolveLink= resolveLink 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
DEBUG org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker: ACCESS CHECK:  this , doCheckOwner= doCheckOwner , ancestorAccess= ancestorAccess , parentAccess= parentAccess , access= access , subAccess= subAccess , ignoreEmptyDir= ignoreEmptyDir , resolveLink= resolveLink 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.JournalNode: Initializing journal in directory  <*> 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Cannot access storage directory  <*> 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Cannot access storage directory  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Scanning storage  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Latest log is  latestLog 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Rolling forward previously half-completed synchronization:  <*>  ->  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Edit log file  <*>  appears to be empty.  Moving it aside... 
WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault: Could not find a target for file  src  with favored node  favoredNode 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault: Failed to choose with favored nodes (= favoredNodes ), disregard favored nodes hint and retry. 
WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is  expectedSize  but only  <*>  storage types can be selected  (replication= $i , selected= <*> , unavailable= unavailables , removed= $u , policy= this ) 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault: Failed to choose from local rack (location =  <*> ), retry with the rack of the next replica (location =  <*> ) 
WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault: <*>   <*> 
WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is  expectedSize  but only  <*>  storage types can be selected  (replication= $i , selected= <*> , unavailable= unavailables , removed= $u , policy= this ) 
TRACE org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault: storageTypes= <*> 
WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault: <*>   <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: Resolved path is  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.JournalNode: Initializing journal in directory  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: <*>  does not exist. Creating ... 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Lock on  <*>  acquired by nodename  <*> 
ERROR org.apache.hadoop.hdfs.server.common.Storage: *********** Upgrade is not supported from this  older version  oldVersion  of storage to the current version.  Please upgrade to  Hadoop-.  or a later version and then upgrade to current  version. Old layout version is  <*>  and latest layout version this software version can  upgrade from is  $i . ************ 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Scanning storage  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: No files in  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Rolling forward previously half-completed synchronization:  <*>  ->  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Edit log file  <*>  appears to be empty.  Moving it aside... 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager$DeletionStoragePurger: Purging old image  image 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager$DeletionStoragePurger: Could not delete  file 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager$DeletionStoragePurger: Could not delete  file 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
DEBUG org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker: ACCESS CHECK:  this , doCheckOwner= doCheckOwner , ancestorAccess= ancestorAccess , parentAccess= parentAccess , access= access , subAccess= subAccess , ignoreEmptyDir= ignoreEmptyDir , resolveLink= resolveLink 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: Resolved path is  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList$1: Adding replicas to map for block pool  <*>  on volume  <*> ... 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil: Block  blockFile  does not have a metafile! 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil: Block  blockFile  does not have a metafile! 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil: Block  blockFile  does not have a metafile! 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil: Block  blockFile  does not have a metafile! 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil: Block  blockFile  does not have a metafile! 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Failed to delete restart meta file:  <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: resolveDuplicateReplicas decide to keep  replicaToKeep .  Will try to delete  <*> 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaTracker$RamDiskReplica: Failed to delete block file  <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: resolveDuplicateReplicas decide to keep  replicaToKeep .  Will try to delete  <*> 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Failed to delete restart meta file:  <*> 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Failed to delete restart meta file:  <*> 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Failed to delete restart meta file:  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList$1: Time to add replicas to map for block pool  <*>  on volume  <*> :  timeTaken ms 
DEBUG org.apache.hadoop.hdfs.web.TokenAspect: Created new DT for  <*> 
TRACE org.apache.hadoop.hdfs.web.WebHdfsFileSystem: url= <*> 
DEBUG org.apache.hadoop.hdfs.web.URLConnectionFactory: open URL connection
INFO org.apache.hadoop.hdfs.server.datanode.BPServiceActor: this  starting to offer service 
DEBUG org.apache.hadoop.hdfs.server.datanode.BPServiceActor: this  received versionRequest response:  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Generated and persisted new Datanode UUID  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Periodic Block Verification scan disabled because  reason#_ 
INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Periodic Directory Tree Verification scan is disabled because  reason#_ 
INFO org.apache.hadoop.hdfs.server.datanode.BPServiceActor: this  beginning handshake with NN 
INFO org.apache.hadoop.hdfs.server.datanode.BPServiceActor: Block pool  this  successfully registered with NN 
INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block token params received from NN: for block pool  blockPoolId  keyUpdateInterval= <*>  min(s), tokenLifetime= <*>  min(s) 
INFO org.apache.hadoop.hdfs.server.datanode.BPServiceActor: For namenode  <*>  using  DELETEREPORT_INTERVAL of  <*>  msec   BLOCKREPORT_INTERVAL of  <*> msec  CACHEREPORT_INTERVAL of  <*> msec  Initial delay:  <*> msec ; heartBeatInterval= <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.BPServiceActor: Sending heartbeat with  <*>  storage reports from service actor:  this 
INFO org.apache.hadoop.hdfs.server.datanode.BPOfferService: Namenode  actor  trying to claim ACTIVE state with  txid= <*> 
INFO org.apache.hadoop.hdfs.server.datanode.BPOfferService: Namenode  actor  taking over ACTIVE state from  <*>  at higher txid= <*> 
INFO org.apache.hadoop.hdfs.server.datanode.BPOfferService: Namenode  actor  trying to claim ACTIVE state with  txid= <*> 
INFO org.apache.hadoop.hdfs.server.datanode.BPOfferService: Namenode  actor  taking over ACTIVE state from  <*>  at higher txid= <*> 
ERROR org.apache.hadoop.hdfs.server.datanode.BPServiceActor: Invalid BlockPoolId  <*>  in HeartbeatResponse. Expected  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.BPOfferService: DatanodeCommand action : DNA_REGISTER from  <*>  with  <*>  state 
DEBUG org.apache.hadoop.hdfs.server.datanode.BPServiceActor: this  received versionRequest response:  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.BPServiceActor: this  beginning handshake with NN 
INFO org.apache.hadoop.hdfs.server.datanode.BPServiceActor: Block pool  this  successfully registered with NN 
INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block token params received from NN: for block pool  blockPoolId  keyUpdateInterval= <*>  min(s), tokenLifetime= <*>  min(s) 
INFO org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager: Setting block keys
INFO org.apache.hadoop.hdfs.server.datanode.BPOfferService: DatanodeCommand action : DNA_REGISTER from  <*>  with  <*>  state 
DEBUG org.apache.hadoop.hdfs.server.datanode.BPServiceActor: this  received versionRequest response:  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.BPServiceActor: this  beginning handshake with NN 
INFO org.apache.hadoop.hdfs.server.datanode.BPServiceActor: Block pool  this  successfully registered with NN 
INFO org.apache.hadoop.hdfs.server.datanode.BPOfferService: DatanodeCommand action : DNA_REGISTER from  <*>  with  <*>  state 
INFO org.apache.hadoop.hdfs.server.datanode.BPServiceActor: this  beginning handshake with NN 
INFO org.apache.hadoop.hdfs.server.datanode.BPServiceActor: Block pool  this  successfully registered with NN 
INFO org.apache.hadoop.hdfs.server.datanode.BPServiceActor: <*> uccessfully sent block report x <*> ,  containing  <*>  storage report(s), of which we sent  i$ .  The reports had  totalBlockCount  total blocks and used  kvPair#  RPC(s). This took  <*>  msec to generate and  <*>  msecs for RPC and NN processing.  Got back  <*> . 
INFO org.apache.hadoop.hdfs.server.datanode.BPServiceActor: <*> uccessfully sent block report x <*> ,  containing  <*>  storage report(s), of which we sent  i$ .  The reports had  totalBlockCount  total blocks and used  kvPair#  RPC(s). This took  <*>  msec to generate and  <*>  msecs for RPC and NN processing.  Got back  <*> . 
INFO org.apache.hadoop.hdfs.server.datanode.BPServiceActor: <*> uccessfully sent block report x <*> ,  containing  <*>  storage report(s), of which we sent  i$ .  The reports had  totalBlockCount  total blocks and used  kvPair#  RPC(s). This took  <*>  msec to generate and  <*>  msecs for RPC and NN processing.  Got back  <*> . 
DEBUG org.apache.hadoop.hdfs.server.datanode.BPServiceActor: Sending cacheReport from service actor:  this 
DEBUG org.apache.hadoop.hdfs.server.datanode.BPServiceActor: CacheReport of  <*>  block(s) took  <*>  msec to generate and  <*>  msecs for RPC and NN processing 
DEBUG org.apache.hadoop.hdfs.server.datanode.BPServiceActor: Sending cacheReport from service actor:  this 
DEBUG org.apache.hadoop.hdfs.server.datanode.BPServiceActor: CacheReport of  <*>  block(s) took  <*>  msec to generate and  <*>  msecs for RPC and NN processing 
INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval  hours  hours for block pool  bpid 
INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid= blockPoolId  to blockPoolScannerMap, new size= <*> 
INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval  hours  hours for block pool  bpid 
INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid= blockPoolId  to blockPoolScannerMap, new size= <*> 
INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval  hours  hours for block pool  bpid 
INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid= blockPoolId  to blockPoolScannerMap, new size= <*> 
INFO org.apache.hadoop.hdfs.server.datanode.BPServiceActor: For namenode  <*>  using  DELETEREPORT_INTERVAL of  <*>  msec   BLOCKREPORT_INTERVAL of  <*> msec  CACHEREPORT_INTERVAL of  <*> msec  Initial delay:  <*> msec ; heartBeatInterval= <*> 
INFO org.apache.hadoop.hdfs.server.datanode.BPServiceActor: For namenode  <*>  using  DELETEREPORT_INTERVAL of  <*>  msec   BLOCKREPORT_INTERVAL of  <*> msec  CACHEREPORT_INTERVAL of  <*> msec  Initial delay:  <*> msec ; heartBeatInterval= <*> 
INFO org.apache.hadoop.hdfs.server.datanode.BPOfferService: Namenode  actor  trying to claim ACTIVE state with  txid= <*> 
INFO org.apache.hadoop.hdfs.server.datanode.BPOfferService: Acknowledging ACTIVE Namenode  actor 
INFO org.apache.hadoop.hdfs.server.datanode.BPOfferService: Namenode  actor  relinquishing ACTIVE state with  txid= <*> 
INFO org.apache.hadoop.hdfs.server.datanode.BPServiceActor: Took  <*> ms to process  <*>  commands from NN 
WARN org.apache.hadoop.hdfs.server.datanode.BPServiceActor: Ending block pool service for:  this 
INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolManager: Removed  bpos 
WARN org.apache.hadoop.hdfs.server.datanode.BlockPoolManager: Couldn\'t remove BPOS  t  from bpByNameserviceId map 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* addToInvalidates:  b   <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  i 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: BLOCK*  <*> : add  block  to  datanode 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: BLOCK*  <*> : add  block  to  datanode 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  i 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* addToInvalidates:  b   <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  priLevel 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: Error converting file to URI
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: Error converting file to URI
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: Error converting file to URI
ERROR org.apache.hadoop.hdfs.server.namenode.JournalSet: Disabling journal  j 
ERROR org.apache.hadoop.hdfs.server.namenode.JournalSet: Error:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Catching up to latest edits from old active before taking over writer role in edits logs
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: Marking all datandoes as stale
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: UnderReplicationBlocks.update  block  curReplicas  curReplicas  curExpectedReplicas  curExpectedReplicas  oldReplicas  oldReplicas  oldExpectedReplicas   oldExpectedReplicas  curPri   <*>  oldPri   <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.update: block  has only  curReplicas  replicas and needs  curExpectedReplicas  replicas so is added to neededReplications  at priority level  <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* removeStoredBlock:  block  has already been removed from node  node 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* removeStoredBlock:  block  from  node 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: UnderReplicationBlocks.update  block  curReplicas  curReplicas  curExpectedReplicas  curExpectedReplicas  oldReplicas  oldReplicas  oldExpectedReplicas   oldExpectedReplicas  curPri   <*>  oldPri   <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  priLevel 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* removeStoredBlock:  block  is removed from excessBlocks 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: storageInfo  failed. 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Removed storage  len$  from DataNode this 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from  <*>  to  volFailures 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of storages reported in heartbeat= <*> ; Number of storages in storageMap= <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: getDatanodeListForReport with includedNodes =  <*> , excludedNodes =  <*> , foundNodes =  <*> , nodes =  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSNamesystem: NameNode metadata after re-processing replication and invalidation queues during failover:\n <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Will take over writing edit logs at txnid  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.JournalSet: Skipping jas  jas  since it\'s disabled 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from  <*>  to  volFailures 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Deferring removal of stale storage  len$  with  <*>  blocks 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from  <*>  to  volFailures 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of storages reported in heartbeat= <*> ; Number of storages in storageMap= <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: getDatanodeListForReport with includedNodes =  <*> , excludedNodes =  <*> , foundNodes =  <*> , nodes =  <*> 
INFO org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : closing 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica: closed  this suffix 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: demoteOldEvictable: demoting  <*> :  <*> :  <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica: this  is stale because it\'s  stale#  ms old, and staleThresholdMs =  <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica: closed  this suffix 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : unref replica  replica :  purgeReason  refCount  <*>  ->  <*> <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: demoteOldEvictable: demoting  <*> :  <*> :  <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : trimEvictionMaps is purging  replica <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica: closed  this suffix 
DEBUG org.apache.hadoop.hdfs.DFSInotifyEventInputStream: poll(): lastReadTxid is -, reading current txid from NN
DEBUG org.apache.hadoop.hdfs.DFSInotifyEventInputStream: poll(): read no edits from the NN when requesting edits after txid {}
DEBUG org.apache.hadoop.hdfs.DFSInotifyEventInputStream: take(): poll() returned null, sleeping for {} ms
DEBUG org.apache.hadoop.hdfs.DFSInotifyEventInputStream: poll(): lastReadTxid is -, reading current txid from NN
DEBUG org.apache.hadoop.hdfs.DFSInotifyEventInputStream: poll(): read no edits from the NN when requesting edits after txid {}
DEBUG org.apache.hadoop.hdfs.DFSInotifyEventInputStream: poll(): lastReadTxid is -, reading current txid from NN
INFO org.apache.hadoop.hdfs.server.namenode.CacheManager: removeCachePool of  poolName  successful. 
DEBUG org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskAsyncLazyPersistService: LazyWriter schedule async task to persist RamDisk block pool id:  bpId  block id:  blockId 
DEBUG org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskAsyncLazyPersistService: LazyWriter schedule async task to persist RamDisk block pool id:  bpId  block id:  blockId 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskAsyncLazyPersistService: LazyWriter failed to create  <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl$LazyWriter: Evicting block  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Moved  blockFile  to  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Moved  metaFile  to  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Moved  blockFile  to  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Moved  metaFile  to  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Moved  blockFile  to  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Moved  metaFile  to  <*> 
WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Cannot find BPOfferService for reporting block received for bpid= <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Moved  blockFile  to  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Moved  metaFile  to  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Moved  blockFile  to  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Moved  metaFile  to  <*> 
WARN org.apache.hadoop.hdfs.server.datanode.DataStorage: Storage directory is in use.
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil: Block  blockFile  does not have a metafile! 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil: Block  blockFile  does not have a metafile! 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil: Block  blockFile  does not have a metafile! 
DEBUG org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: resolveDuplicateReplicas decide to keep  replicaToKeep .  Will try to delete  <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: resolveDuplicateReplicas decide to keep  replicaToKeep .  Will try to delete  <*> 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaTracker$RamDiskReplica: Failed to delete block file  <*> 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaTracker$RamDiskReplica: Failed to delete meta file  <*> 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil: Block  blockFile  does not have a metafile! 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil: Block  blockFile  does not have a metafile! 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Failed to delete restart meta file:  <*> 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Failed to delete restart meta file:  <*> 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Failed to delete restart meta file:  <*> 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Failed to delete restart meta file:  <*> 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Failed to delete restart meta file:  <*> 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Failed to delete restart meta file:  <*> 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Failed to delete restart meta file:  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList: Added new volume:  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume -  <*> , StorageType:  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: Start Decommissioning  node   storage  with  <*>  blocks 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.add: block  has only  curReplicas  replicas and need  expectedReplicas  replicas so is added to neededReplications  at priority level  <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.add: block  has only  curReplicas  replicas and need  expectedReplicas  replicas so is added to neededReplications  at priority level  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: Decommission complete for  node 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: Start Decommissioning  node   storage  with  <*>  blocks 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Block:  block , Expected Replicas:  <*> , live replicas:  <*> , corrupt replicas:  <*> , decommissioned replicas:  <*> , excess replicas:  <*> , Is Open File:  <*> , Datanodes having this block:  <*> , Current Datanode:  srcNode , Is current datanode decommissioning:  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Block:  block , Expected Replicas:  <*> , live replicas:  <*> , corrupt replicas:  <*> , decommissioned replicas:  <*> , excess replicas:  <*> , Is Open File:  <*> , Datanodes having this block:  <*> , Current Datanode:  srcNode , Is current datanode decommissioning:  <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.add: block  has only  curReplicas  replicas and need  expectedReplicas  replicas so is added to neededReplications  at priority level  <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.add: block  has only  curReplicas  replicas and need  expectedReplicas  replicas so is added to neededReplications  at priority level  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.JournalNode: Initializing journal in directory  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Locking is disabled for  <*> 
ERROR org.apache.hadoop.hdfs.server.common.Storage: *********** Upgrade is not supported from this  older version  oldVersion  of storage to the current version.  Please upgrade to  Hadoop-.  or a later version and then upgrade to current  version. Old layout version is  <*>  and latest layout version this software version can  upgrade from is  $i . ************ 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Scanning storage  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Latest log is  latestLog 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Updating lastPromisedEpoch from  <*>  to  newEpoch  for client  <*> 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Edit log file  <*>  appears to be empty.  Moving it aside... 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Synchronizing log  <*> : old segment  <*>  is not the right length 
LOGFINALIZEROLLINGUPGRADE org.apache.hadoop.hdfs.server.namenode.FSNamesystem: <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Synchronizing log  <*>  from  url 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Accepted recovery for segment  <*> :  <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.DataXceiver: Checking block access token for block \' <*> \' with mode \' mode \' 
DEBUG org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager: Checking access for user= userId , block= block , access mode= mode  using  <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.DataXceiver: block= block , bytesPerCRC= <*> , crcPerBlock= crcPerBlock , md= <*> 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : unref replica  replica :  purgeReason  refCount  <*>  ->  <*> <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: demoteOldEvictable: demoting  <*> :  <*> :  <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : trimEvictionMaps is purging  replica <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : unref replica  replica :  purgeReason  refCount  <*>  ->  <*> <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : unref replica  replica :  purgeReason  refCount  <*>  ->  <*> <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: demoteOldEvictable: demoting  <*> :  <*> :  <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : trimEvictionMaps is purging  replica <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : unref replica  replica :  purgeReason  refCount  <*>  ->  <*> <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* addToInvalidates:  b   <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  i 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* addToInvalidates:  b   <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  i 
INFO org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: BLOCK*  <*> : add  block  to  datanode 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: BLOCK*  <*> : add  block  to  datanode 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  i 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Block:  block , Expected Replicas:  <*> , live replicas:  <*> , corrupt replicas:  <*> , decommissioned replicas:  <*> , excess replicas:  <*> , Is Open File:  <*> , Datanodes having this block:  <*> , Current Datanode:  srcNode , Is current datanode decommissioning:  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Block:  block , Expected Replicas:  <*> , live replicas:  <*> , corrupt replicas:  <*> , decommissioned replicas:  <*> , excess replicas:  <*> , Is Open File:  <*> , Datanodes having this block:  <*> , Current Datanode:  srcNode , Is current datanode decommissioning:  <*> 
WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: srcNode  srcNode  is dead  when decommission is in progress. Continue to mark  it as decommission in progress. In that way, when it rejoins the  cluster it can continue the decommission process. 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: Decommission complete for  node 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Block:  block , Expected Replicas:  <*> , live replicas:  <*> , corrupt replicas:  <*> , decommissioned replicas:  <*> , excess replicas:  <*> , Is Open File:  <*> , Datanodes having this block:  <*> , Current Datanode:  srcNode , Is current datanode decommissioning:  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Block:  block , Expected Replicas:  <*> , live replicas:  <*> , corrupt replicas:  <*> , decommissioned replicas:  <*> , excess replicas:  <*> , Is Open File:  <*> , Datanodes having this block:  <*> , Current Datanode:  srcNode , Is current datanode decommissioning:  <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.add: block  has only  curReplicas  replicas and need  expectedReplicas  replicas so is added to neededReplications  at priority level  <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.add: block  has only  curReplicas  replicas and need  expectedReplicas  replicas so is added to neededReplications  at priority level  <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.add: block  has only  curReplicas  replicas and need  expectedReplicas  replicas so is added to neededReplications  at priority level  <*> 
WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: srcNode  srcNode  is dead  when decommission is in progress. Continue to mark  it as decommission in progress. In that way, when it rejoins the  cluster it can continue the decommission process. 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: Decommission complete for  node 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
INFO org.apache.hadoop.hdfs.qjournal.server.JournalNode: Initializing journal in directory  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: <*>  does not exist. Creating ... 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Locking is disabled for  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Completing previous checkpoint for storage directory  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Scanning storage  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: No files in  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Discard the EditLog files, the given start txid is  startTxId 
INFO org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : closing 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica: this  is stale because it\'s  stale#  ms old, and staleThresholdMs =  <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica: closed  this suffix 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : unref replica  replica :  purgeReason  refCount  <*>  ->  <*> <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : unref replica  replica :  purgeReason  refCount  <*>  ->  <*> <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica: closed  this suffix 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : unref replica  replica :  purgeReason  refCount  <*>  ->  <*> <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica: this  is not stale because it\'s only  stale#  ms old, and staleThresholdMs =  <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl$LazyWriter: LazyWriter: Start persisting RamDisk block: block pool Id:  <*>  block id:  <*>  on target volume  <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskAsyncLazyPersistService: LazyWriter schedule async task to persist RamDisk block pool id:  bpId  block id:  blockId 
DEBUG org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskAsyncLazyPersistService: LazyWriter schedule async task to persist RamDisk block pool id:  bpId  block id:  blockId 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskAsyncLazyPersistService: LazyWriter failed to create  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Moved  blockFile  to  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Moved  metaFile  to  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Moved  blockFile  to  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Moved  metaFile  to  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Moved  blockFile  to  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Moved  metaFile  to  <*> 
WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Cannot find BPOfferService for reporting block received for bpid= <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl$LazyWriter: Evicting block  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Moved  blockFile  to  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Moved  metaFile  to  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Moved  blockFile  to  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Moved  metaFile  to  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Reconfiguring  property  to  newVal 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Adding new volumes:  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Storage directory is loaded:  <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.add: block  has only  curReplicas  replicas and need  expectedReplicas  replicas so is added to neededReplications  at priority level  <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.add: block  has only  curReplicas  replicas and need  expectedReplicas  replicas so is added to neededReplications  at priority level  <*> 
TRACE org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: block  block :  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            =  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          =  nrInvalid 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks =  nrUnderReplicated 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks =  nrOverReplicated <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    =  nrUnderConstruction 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in  <*>  msec 
INFO org.apache.hadoop.hdfs.qjournal.server.JournalNode: Initializing journal in directory  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Locking is disabled for  <*> 
ERROR org.apache.hadoop.hdfs.server.common.Storage: *********** Upgrade is not supported from this  older version  oldVersion  of storage to the current version.  Please upgrade to  Hadoop-.  or a later version and then upgrade to current  version. Old layout version is  <*>  and latest layout version this software version can  upgrade from is  $i . ************ 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Scanning storage  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: No files in  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Updating lastPromisedEpoch from  <*>  to  newEpoch  for client  <*> 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Updating lastWriterEpoch from  <*>  to  <*>  for client  <*> 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Unable to start log segment  txid  at  <*> :  <*> 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
INFO org.apache.hadoop.hdfs.server.namenode.ImageServlet: ImageServlet rejecting:  remoteUser 
WARN org.apache.hadoop.hdfs.server.namenode.ImageServlet: Received non-NN/SNN/administrator request for image or edits from  <*>  at  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSNamesystem: concat  <*>  to  target 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker: ACCESS CHECK:  this , doCheckOwner= doCheckOwner , ancestorAccess= ancestorAccess , parentAccess= parentAccess , access= access , subAccess= subAccess , ignoreEmptyDir= ignoreEmptyDir , resolveLink= resolveLink 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.JournalNode: Initializing journal in directory  <*> 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Storage directory  <*>  does not exist 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Cannot access storage directory  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Scanning storage  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Latest log is  latestLog 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: msg  \n <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: msg  \n <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: msg  \n <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: STATE* Leaving safe mode after  <*>  secs 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: STATE* Safe mode is OFF
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: STATE* Network topology has  <*>  racks and  <*>  datanodes 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: STATE* UnderReplicatedBlocks has  <*>  blocks 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
DEBUG org.apache.hadoop.hdfs.server.namenode.NameNode: Setting fs.defaultFS to  <*> 
ERROR org.apache.hadoop.hdfs.NameNodeProxies: Unsupported protocol found when creating the proxy connection to NameNode:  <*> 
WARN org.apache.hadoop.hdfs.DFSUtil: hadoop.ssl.enabled is deprecated. Please use dfs.http.policy.
DEBUG org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Will connect to NameNode at  <*> 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: set restore failed storage to  val 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: NNStorage.attemptRestoreRemovedStorage: check removed(failed) storarge. removedStorages size =  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Lock on  <*>  acquired by nodename  <*> 
ERROR org.apache.hadoop.hdfs.server.common.Storage: *********** Upgrade is not supported from this  older version  oldVersion  of storage to the current version.  Please upgrade to  Hadoop-.  or a later version and then upgrade to current  version. Old layout version is  <*>  and latest layout version this software version can  upgrade from is  $i . ************ 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: <*> is not a directory 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Cannot access storage directory  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Locking is disabled for  <*> 
ERROR org.apache.hadoop.hdfs.server.common.Storage: *********** Upgrade is not supported from this  older version  oldVersion  of storage to the current version.  Please upgrade to  Hadoop-.  or a later version and then upgrade to current  version. Old layout version is  <*>  and latest layout version this software version can  upgrade from is  $i . ************ 
WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage: Failed to delete temporary edits file:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Found KeyProvider:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: <*> 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.HeartbeatManager: Setting heartbeat recheck interval to  <*>  since  dfs.namenode.stale.datanode.interval  is less than  dfs.namenode.heartbeat.recheck-interval 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit= <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check= <*> 
WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: The given interval for marking stale datanode =  staleInterval , which is larger than heartbeat expire interval  heartbeatExpireInterval . 
INFO org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: dfs.namenode.startup.delay.block.deletion.sec is set to  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: The block deletion will start around  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable= <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         =  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             =  $i 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             =  $i 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      =  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval =  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        =  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled =  <*> 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Determined nameservice ID:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Configured NNs:\n <*> 
WARN org.apache.hadoop.hdfs.server.namenode.CheckpointConf: Configuration key  key  is deprecated! Ignoring...  Instead please specify a value for  dfs.namenode.checkpoint.txns 
WARN org.apache.hadoop.hdfs.DFSUtil: hadoop.ssl.enabled is deprecated. Please use dfs.http.policy.
INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for  name  at:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
WARN org.apache.hadoop.hdfs.DFSUtil: hadoop.ssl.enabled is deprecated. Please use dfs.http.policy.
INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   : <*>  secs  ( <*>  min) 
INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    : <*>  txns 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: Reloading namespace from  file 
INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Loader: Upgrading to sequential block IDs. Generation stamp for new blocks set to  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Loader: Loading image file  curFile  using  stampAtIdSwitch 
INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Loader: Number of files =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormat: Renamed root path .reserved to  renameString 
INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormat: Renamed root path .reserved to  renameString 
INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormat: Renamed root path .reserved to  renameString 
INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Loader: Number of files under construction =  <*> 
WARN org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSecretManager: No KEY found for persisted identifier  <*> 
<INIT> org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSecretManager: Same delegation token being added twice; invalid entry in fsimage or editlogs
WARN org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSecretManager: No KEY found for persisted identifier  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.CacheManager: addCachePool of {} successful.
INFO org.apache.hadoop.hdfs.server.namenode.CacheManager: addCachePool of {} successful.
INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Loader: Image file  curFile  of size  <*>  bytes loaded in  <*>  seconds. 
INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with  <*>  entries  <*>  lookups 
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from  <*>  stream(s). 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: About to load edits:\n   <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: BUG: Diskspace quota violation in image for  <*>  quota =  <*>  < consumed =  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: BUG: Diskspace quota violation in image for  <*>  quota =  <*>  < consumed =  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: BUG: Namespace quota violation in image for  <*>  quota =  <*>  < consumed =  child# 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: BUG: Diskspace quota violation in image for  <*>  quota =  <*>  < consumed =  <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: renaming   <*>  to  <*> 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
DEBUG org.apache.hadoop.hdfs.util.MD5FileUtils: Saved MD  digestString  to  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: renaming   <*>  to  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: renaming   <*>  to  <*> 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
DEBUG org.apache.hadoop.hdfs.util.MD5FileUtils: Saved MD  digestString  to  <*> 
WARN org.apache.hadoop.hdfs.util.MD5FileUtils: deleting   <*>  FAILED 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain  <*>  images with txid >=  <*> 
DEBUG org.apache.hadoop.hdfs.web.URLConnectionFactory: open URL connection
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to  <*>  milliseconds 
WARN org.apache.hadoop.hdfs.server.namenode.TransferFsImage: SIMULATING A CORRUPT BYTE IN IMAGE TRANSFER!
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Connection closed by client
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid  txid  to namenode at  fsName  in  <*>  seconds 
WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Saver: Saving image file  newFile  using  compression 
INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Saver: Image file  newFile  of size  <*>  bytes saved in  <*>  seconds. 
INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage: Deleting  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Failed to delete image file:  $u 
INFO org.apache.hadoop.hdfs.qjournal.server.JournalNode: Initializing journal in directory  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: <*>  does not exist. Creating ... 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Completing previous finalize for storage directory  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Scanning storage  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Latest log is  latestLog 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Synchronizing log  <*> : no current segment in place 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Synchronizing log  <*>  from  url 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Accepted recovery for segment  <*> :  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer: Invalid clusterId in journal request - expected  <*>  actual  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.BackupImage: data: <*> 
TRACE org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: op= <*> , startOpt= startOpt , numEdits= numEdits , totalEdits= <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormat: Upgrade process renamed reserved path  oldPath  to  path 
INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormat: Renamed root path .reserved to  renameString 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: DIR* FSDirectory.unprotectedDelete: failed to remove  src  because the root is not allowed to be deleted 
DEBUG org.apache.hadoop.hdfs.server.namenode.LeaseManager: <*> .findLease: prefix= prefix 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: Adjusting block totals from  <*> / <*>  to  <*> / <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  priLevel 
WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Inconsistent number of corrupt replicas for  blk  blockMap has  <*>  but corrupt replicas map has  <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* removeStoredBlock:  block  is removed from excessBlocks 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  i 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  i 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* removeStoredBlock:  block  is removed from excessBlocks 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Processing previouly queued message  rbi 
INFO org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: replaying edit log:  deltaTxId / <*>  transactions completed. ( <*> %) 
TRACE org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: replaying edit log finished
APPEND org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: Summary of operations loaded from edit log:\n  
TRACE org.apache.hadoop.hdfs.server.namenode.CacheManager: Validating directive {} pool maxRelativeExpiryTime {}
INFO org.apache.hadoop.hdfs.server.namenode.CacheManager: addDirective of {} successful.
DEBUG org.apache.hadoop.hdfs.server.namenode.FSNamesystem: DIR* NameSystem.delete:  src 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: Resolved path is  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: DIR* FSDirectory.unprotectedDelete: failed to remove  src  because it does not exist 
DEBUG org.apache.hadoop.hdfs.server.namenode.LeaseManager: <*> .findLease: prefix= prefix 
INFO org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: BLOCK*  <*> : add  block  to  datanode 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: BLOCK*  <*> : add  block  to  datanode 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* addToInvalidates:  b   <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  i 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* addToInvalidates:  b   <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  priLevel 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* addToInvalidates:  b   <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  i 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: Resolved path is  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: Resolved path is  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: Resolved path is  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker: ACCESS CHECK:  this , doCheckOwner= doCheckOwner , ancestorAccess= ancestorAccess , parentAccess= parentAccess , access= access , subAccess= subAccess , ignoreEmptyDir= ignoreEmptyDir , resolveLink= resolveLink 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: <*> . Note: This is normal during a rolling upgrade. 
DEBUG org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager: Exporting access keys
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: BLOCK* registerDatanode: from  nodeReg  storage  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: BLOCK* registerDatanode:  <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* removeStoredBlock:  block  from  node 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* removeStoredBlock:  block  is removed from excessBlocks 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* removeStoredBlock:  block  from  node 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  i 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: remove datanode  nodeInfo 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: <*> .wipeDatanode( node ): storage  <*>  is removed from datanodeMap. 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: BLOCK* registerDatanode: node restarted.
REMOVE org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: <*>
ERROR org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: The resolve call returned null!
<INIT> org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: Unresolved topology mapping for host  <*> 
ERROR org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: The dependency call returned null for host  <*> 
ERROR org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: The dependency call returned null for host  <*> 
REMOVE org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: <*>
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from  <*>  to  volFailures 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Block:  block , Expected Replicas:  <*> , live replicas:  <*> , corrupt replicas:  <*> , decommissioned replicas:  <*> , excess replicas:  <*> , Is Open File:  <*> , Datanodes having this block:  <*> , Current Datanode:  srcNode , Is current datanode decommissioning:  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Block:  block , Expected Replicas:  <*> , live replicas:  <*> , corrupt replicas:  <*> , decommissioned replicas:  <*> , excess replicas:  <*> , Is Open File:  <*> , Datanodes having this block:  <*> , Current Datanode:  srcNode , Is current datanode decommissioning:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
INFO org.apache.hadoop.hdfs.qjournal.server.JournalNode: Initializing journal in directory  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: <*>  does not exist. Creating ... 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Lock on  <*>  acquired by nodename  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Cannot lock storage  <*> . The directory is already locked 
ERROR org.apache.hadoop.hdfs.server.common.Storage: *********** Upgrade is not supported from this  older version  oldVersion  of storage to the current version.  Please upgrade to  Hadoop-.  or a later version and then upgrade to current  version. Old layout version is  <*>  and latest layout version this software version can  upgrade from is  $i . ************ 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Scanning storage  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: No files in  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil: Rollback of  <*>  is complete. 
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Waiting until the NameNode rolls its edit logs in order to freeze the BackupNode namespace.
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: BackupNode namespace frozen.
DEBUG org.apache.hadoop.hdfs.server.namenode.Checkpointer: Doing checkpoint. Last applied:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Unable to roll forward using only logs. Downloading image with txid  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to  <*> 
DEBUG org.apache.hadoop.hdfs.web.URLConnectionFactory: open AuthenticatedURL connection url 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to  <*>  milliseconds 
WARN org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Overwriting existing file  f  with file downloaded from  url 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file  <*>  size  <*>  bytes. 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Loading image with txid  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: Reloading namespace from  file 
CLOSEALLSTREAMS org.apache.hadoop.hdfs.server.namenode.BackupImage:  <*> <*>
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from  <*>  stream(s). 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: About to load edits:\n   <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: BUG: Diskspace quota violation in image for  <*>  quota =  <*>  < consumed =  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: BUG: Diskspace quota violation in image for  <*>  quota =  <*>  < consumed =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with  <*>  entries  <*>  lookups 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: renaming   <*>  to  <*> 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
WARN org.apache.hadoop.hdfs.util.MD5FileUtils: deleting   <*>  FAILED 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain  <*>  images with txid >=  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpoint completed in  <*>  seconds.  New Image Size:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Waiting until the NameNode rolls its edit logs in order to freeze the BackupNode namespace.
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: BackupNode namespace frozen.
DEBUG org.apache.hadoop.hdfs.server.namenode.Checkpointer: Doing checkpoint. Last applied:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with  <*>  entries  <*>  lookups 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain  <*>  images with txid >=  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Loading edits into backupnode to try to catch up from txid  <*>  to  <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.JournalSet: Disabling journal  j 
ERROR org.apache.hadoop.hdfs.server.namenode.JournalSet: Disabling journal  j 
INFO org.apache.hadoop.hdfs.server.namenode.JournalSet: Skipping jas  jas  since it\'s disabled 
INFO org.apache.hadoop.hdfs.server.namenode.JournalSet: Skipping jas  jas  since it\'s disabled 
INFO org.apache.hadoop.hdfs.server.namenode.JournalSet: Skipping jas  jas  since it\'s disabled 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: About to load edits:\n   <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading  editIn  expecting start txid # <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: BUG: Namespace quota violation in image for  <*>  quota =  <*>  < consumed =  child# 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: About to load edits:\n   <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: About to load edits:\n   <*> 
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Going to finish converging with remaining  editStreamsAll  txns from in-progress stream  stream 
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Loading edits into backupnode to try to catch up from txid  <*>  to  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Going to finish converging with remaining  editStreamsAll  txns from in-progress stream  stream 
CLOSEALLSTREAMS org.apache.hadoop.hdfs.server.namenode.BackupImage:  <*> <*>
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Successfully synced BackupNode with NameNode at txnid  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.BackupImage: State transition  <*>  ->  newState 
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpoint completed in  <*>  seconds.  New Image Size:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Waiting until the NameNode rolls its edit logs in order to freeze the BackupNode namespace.
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: BackupNode namespace frozen.
DEBUG org.apache.hadoop.hdfs.server.namenode.Checkpointer: Doing checkpoint. Last applied:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Unable to roll forward using only logs. Downloading image with txid  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to  <*> 
DEBUG org.apache.hadoop.hdfs.web.URLConnectionFactory: open AuthenticatedURL connection url 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to  <*>  milliseconds 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file  <*>  size  <*>  bytes. 
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Loading image with txid  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: Reloading namespace from  file 
<INIT> org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSecretManager: Same delegation token being added twice; invalid entry in fsimage or editlogs
WARN org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSecretManager: No KEY found for persisted identifier  <*> 
<INIT> org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSecretManager: Same delegation token being added twice; invalid entry in fsimage or editlogs
<INIT> org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSecretManager: Same delegation token being added twice; invalid entry in fsimage or editlogs
WARN org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSecretManager: No KEY found for persisted identifier  <*> 
WARN org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSecretManager: No KEY found for persisted identifier  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Loader: Loaded FSImage in  <*>  seconds. 
CLOSEALLSTREAMS org.apache.hadoop.hdfs.server.namenode.BackupImage:  <*> <*>
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from  <*>  stream(s). 
INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with  <*>  entries  <*>  lookups 
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpoint completed in  <*>  seconds.  New Image Size:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSNamesystem: DIR* NameSystem.delete:  src 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: Resolved path is  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: DIR* FSDirectory.unprotectedDelete: failed to remove  src  because the root is not allowed to be deleted 
DEBUG org.apache.hadoop.hdfs.server.namenode.LeaseManager: <*> .findLease: prefix= prefix 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: BLOCK*  <*> : add  block  to  datanode 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* addToInvalidates:  b   <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  priLevel 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* addToInvalidates:  b   <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  priLevel 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* addToInvalidates:  b   <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  i 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Adjusting safe-mode totals for deletion.decreasing safeBlocks by  numRemovedSafe , totalBlocks by  numRemovedComplete 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: Adjusting block totals from  <*> / <*>  to  <*> / <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: msg  \n <*> 
INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolManager: Refresh request received for nameservices:  <*> 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolManager: Starting BPOfferServices for nameservices:  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolManager: Refreshing list of NNs for nameservices:  <*> 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
TRACE org.apache.hadoop.hdfs.BlockReaderFactory: nextDomainPeer: reusing existing peer  <*> 
TRACE org.apache.hadoop.hdfs.BlockReaderFactory: nextDomainPeer: reusing existing peer  <*> 
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.Sender: Sending DataTransferOp  <*> :  proto 
TRACE org.apache.hadoop.hdfs.BlockReaderFactory: Sending receipt verification byte for slot  slot 
WARN org.apache.hadoop.hdfs.BlockReaderFactory: this : error creating ShortCircuitReplica. 
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.Sender: Sending DataTransferOp  <*> :  proto 
WARN org.apache.hadoop.hdfs.BlockReaderFactory: short-circuit read access is disabled for DataNode  <*> .  reason:  <*> 
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.Sender: Sending DataTransferOp  <*> :  proto 
DEBUG org.apache.hadoop.hdfs.BlockReaderFactory: this : <*> 
WARN org.apache.hadoop.hdfs.BlockReaderFactory: this : I/O error requesting file descriptors.   Disabling domain socket  <*> 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from  <*>  to  volFailures 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: storageInfo  failed. 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Removed storage  len$  from DataNode this 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
DEBUG org.apache.hadoop.hdfs.web.TokenAspect: Created new DT for  <*> 
TRACE org.apache.hadoop.hdfs.web.WebHdfsFileSystem: url= <*> 
DEBUG org.apache.hadoop.hdfs.web.URLConnectionFactory: open URL connection
WARN org.apache.hadoop.hdfs.DFSClient$Conf: Bad checksum type:  <*> . Using default  CRCC 
DEBUG org.apache.hadoop.hdfs.DFSClient$Conf: dfs.client.use.legacy.blockreader.local =  <*> 
DEBUG org.apache.hadoop.hdfs.DFSClient$Conf: dfs.client.read.shortcircuit =  <*> 
DEBUG org.apache.hadoop.hdfs.DFSClient$Conf: dfs.client.domain.socket.data.traffic =  <*> 
DEBUG org.apache.hadoop.hdfs.DFSClient$Conf: dfs.domain.socket.path =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Found KeyProvider:  <*> 
INFO org.apache.hadoop.hdfs.PeerCache: SocketCache disabled.
DEBUG org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory: Both short-circuit local reads and UNIX domain socket are disabled.
DEBUG org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil: DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for {}
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: startFile: recover  <*> , src= src  client  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Recovering  lease , src= src 
WARN org.apache.hadoop.hdfs.server.namenode.LeaseManager: Removing non-existent lease! holder= holder  src= src 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSNamesystem: closeFile:  path  with  <*>  blocks is persisted to the file system 
TRACE org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* processOverReplicatedBlock: Postponing  block  since storage  storage  does not yet have up-to-date information. 
TRACE org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* processOverReplicatedBlock: Postponing  block  since storage  storage  does not yet have up-to-date information. 
TRACE org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* processOverReplicatedBlock: Postponing  block  since storage  storage  does not yet have up-to-date information. 
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* internalReleaseLease: Removed empty last block and closed file.
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
DEBUG org.apache.hadoop.hdfs.server.namenode.NameNode: Setting fs.defaultFS to  <*> 
ERROR org.apache.hadoop.hdfs.NameNodeProxies: Unsupported protocol found when creating the proxy connection to NameNode:  <*> 
WARN org.apache.hadoop.hdfs.DFSUtil: hadoop.ssl.enabled is deprecated. Please use dfs.http.policy.
DEBUG org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Will connect to NameNode at  <*> 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: set restore failed storage to  val 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: NNStorage.attemptRestoreRemovedStorage: check removed(failed) storarge. removedStorages size =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: currently disabled dir  <*> ; type= <*> ;canwrite= <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: restoring dir  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: <*>  does not exist. Creating ... 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Cannot access storage directory  <*> 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Cannot access storage directory  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: <*>  does not exist. Creating ... 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Cannot access storage directory  <*> 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Cannot access storage directory  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Completing previous finalize for storage directory  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Completing previous checkpoint for storage directory  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage: Failed to delete temporary edits file:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Found KeyProvider:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: <*> 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.HeartbeatManager: Setting heartbeat recheck interval to  <*>  since  dfs.namenode.stale.datanode.interval  is less than  dfs.namenode.heartbeat.recheck-interval 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit= <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check= <*> 
WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: The given interval for marking stale datanode =  <*> , which is less than    heartbeat intervals. This may cause too frequent changes of  stale states of DataNodes since a heartbeat msg may be missing  due to temporary short-term failures. Reset stale interval to  <*> . 
INFO org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: dfs.namenode.startup.delay.block.deletion.sec is set to  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: The block deletion will start around  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable= <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.key.update.interval= <*>  min(s),  dfs.block.access.token.lifetime = <*>  min(s),  dfs.encrypt.data.transfer.algorithm = <*> 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Caching file names occuring more than  <*>  times 
INFO org.apache.hadoop.hdfs.server.namenode.CacheManager: Using minimum value {} for {}
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: The threshold value should\'t be greater than , threshold:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: dfs.namenode.safemode.threshold-pct =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: dfs.namenode.safemode.min.datanodes =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: dfs.namenode.safemode.extension     =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNConf: ACLs enabled?  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNConf: XAttrs enabled?  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNConf: Maximum size of an xattr:  <*> <*> 
WARN org.apache.hadoop.hdfs.server.namenode.CheckpointConf: Configuration key  key  is deprecated! Ignoring...  Instead please specify a value for  dfs.namenode.checkpoint.txns 
WARN org.apache.hadoop.hdfs.DFSUtil: hadoop.ssl.enabled is deprecated. Please use dfs.http.policy.
INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for  name  at:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
WARN org.apache.hadoop.hdfs.DFSUtil: hadoop.ssl.enabled is deprecated. Please use dfs.http.policy.
INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   : <*>  secs  ( <*>  min) 
INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    : <*>  txns 
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.Sender: Sending DataTransferOp  <*> :  proto 
WARN org.apache.hadoop.hdfs.BlockReaderFactory: short-circuit read access is disabled for DataNode  <*> .  reason:  <*> 
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.Sender: Sending DataTransferOp  <*> :  proto 
DEBUG org.apache.hadoop.hdfs.BlockReaderFactory: this : <*> 
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.Sender: Sending DataTransferOp  <*> :  proto 
WARN org.apache.hadoop.hdfs.BlockReaderFactory: this : unknown response code  <*>  while attempting to set up short-circuit access.  <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm: this : unregisterSlot  slotIdx 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm: this : unregisterSlot  slotIdx 
TRACE org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager$EndpointShmManager: this : freeing empty stale  shm 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm: this : freed 
DEBUG org.apache.hadoop.hdfs.BlockReaderFactory: this : closing stale domain peer  peer 
DEBUG org.apache.hadoop.hdfs.DFSOutputStream: Queued packet  <*> 
DEBUG org.apache.hadoop.hdfs.DFSOutputStream: Waiting for ack for:  seqno 
WARN org.apache.hadoop.hdfs.DFSOutputStream: Slow waitForAckedSeqno took  <*> ms (threshold= <*> ms) 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: Resolved path is  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from  <*>  to  volFailures 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: storageInfo  failed. 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of storages reported in heartbeat= <*> ; Number of storages in storageMap= <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Removed storage  len$  from DataNode this 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from  <*>  to  volFailures 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: storageInfo  failed. 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
WARN org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: Error Recovery for block  <*>  in pipeline  $u : bad datanode  <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer: Connecting to datanode  <*> 
DEBUG org.apache.hadoop.hdfs.DFSClient: Using local interface  addr 
DEBUG org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient: SASL client skipping handshake on trusted connection for addr = {}, datanodeId = {}
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.Sender: Sending DataTransferOp  <*> :  proto 
DEBUG org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: lastAckedSeqno =  <*> 
DEBUG org.apache.hadoop.hdfs.DFSClient: Using local interface  addr 
DEBUG org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient: SASL client skipping handshake on trusted connection for addr = {}, datanodeId = {}
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.Sender: Sending DataTransferOp  <*> :  proto 
INFO org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: nodes are empty for write pipeline of block  <*> 
DEBUG org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: pipeline =  <*> 
DEBUG org.apache.hadoop.hdfs.DFSClient: Using local interface  addr 
DEBUG org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient: SASL client skipping handshake in secured configuration with no SASL protection configured for addr = {}, datanodeId = {}
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.Sender: Sending DataTransferOp  <*> :  proto 
DEBUG org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: pipeline =  <*> 
WARN org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: Datanode did not restart in time:  <*> 
DEBUG org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: Closing old block  <*> 
WARN org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: Datanode did not restart in time:  <*> 
INFO org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: Error Recovery for  <*>  waiting for responder to exit.  
DEBUG org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: Allocating new block
INFO org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: Abandoning  <*> 
INFO org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: Excluding datanode  <*> 
INFO org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: Abandoning  <*> 
INFO org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: Excluding datanode  <*> 
INFO org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: Abandoning  <*> 
INFO org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: Excluding datanode  <*> 
DEBUG org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: DataStreamer block  <*>  sending packet  e_ 
INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolManager: Refresh request received for nameservices:  <*> 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolManager: Starting BPOfferServices for nameservices:  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool  bpid 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList: Total time to scan all replicas for block pool  bpid :  arr$# ms 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList: Total time to add all replicas to map:  arr$# ms 
TRACE org.apache.hadoop.hdfs.server.namenode.CacheManager: Validating directive {} pool maxRelativeExpiryTime {}
INFO org.apache.hadoop.hdfs.server.namenode.CacheManager: modifyDirective of {} successfully applied {}.
DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer: Connecting to datanode  <*> 
WARN org.apache.hadoop.hdfs.DFSInputStream: <*> <*> . Throwing a BlockMissingException 
DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer: Connecting to datanode  <*> 
WARN org.apache.hadoop.hdfs.DFSInputStream: <*> <*> . Throwing a BlockMissingException 
TRACE org.apache.hadoop.hdfs.BlockReaderFactory: this : trying to construct a BlockReaderLocal  for short-circuit reads. 
TRACE org.apache.hadoop.hdfs.DFSClient: Address  targetAddr <*> 
DEBUG org.apache.hadoop.hdfs.BlockReaderFactory: this :  <*>  is not  usable for short circuit; giving up on BlockReaderLocal. 
TRACE org.apache.hadoop.hdfs.BlockReaderFactory: this : trying to create a remote block reader from a  TCP socket 
TRACE org.apache.hadoop.hdfs.BlockReaderFactory: nextTcpPeer: failed to create newConnectedPeer connected to  <*> 
TRACE org.apache.hadoop.hdfs.BlockReaderFactory: nextTcpPeer: failed to create newConnectedPeer connected to  <*> 
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.Sender: Sending DataTransferOp  <*> :  proto 
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.Sender: Sending DataTransferOp  <*> :  proto 
TRACE org.apache.hadoop.hdfs.DFSClient: Address  targetAddr <*> 
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.Sender: Sending DataTransferOp  <*> :  proto 
TRACE org.apache.hadoop.hdfs.DFSClient: Address  targetAddr <*> 
WARN org.apache.hadoop.hdfs.BlockReaderFactory: I/O error constructing remote block reader.
TRACE org.apache.hadoop.hdfs.DFSClient: Address  targetAddr <*> 
WARN org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : failed to get  key 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : loading  key 
WARN org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : failed to load  key 
TRACE org.apache.hadoop.hdfs.BlockReaderFactory: this : got security exception while constructing  a remote block reader from the unix domain socket at  <*> 
TRACE org.apache.hadoop.hdfs.BlockReaderFactory: this : returning new remote block reader using  UNIX domain socket on  <*> 
INFO org.apache.hadoop.hdfs.DFSInputStream: Access token was invalid when connecting to  targetAddr  :  ex 
INFO org.apache.hadoop.hdfs.DFSInputStream: Access token was invalid when connecting to  targetAddr  :  ex 
INFO org.apache.hadoop.hdfs.DFSInputStream: Access token was invalid when connecting to  targetAddr  :  ex 
WARN org.apache.hadoop.hdfs.DFSInputStream: Failed to connect to  <*>  for block , add to deadNodes and continue.  <*> 
WARN org.apache.hadoop.hdfs.DFSInputStream: Found Checksum error for  <*>  from  <*>  at  <*> 
INFO org.apache.hadoop.hdfs.DFSInputStream: Successfully connected to  <*>  for  <*> 
INFO org.apache.hadoop.hdfs.DFSInputStream: Will fetch a new encryption key and retry, encryption key was invalid when connecting to  <*>  :  <*> 
DEBUG org.apache.hadoop.hdfs.DFSClient: Clearing encryption key
DEBUG org.apache.hadoop.hdfs.DFSClient: Clearing encryption key
DEBUG org.apache.hadoop.hdfs.DFSClient: Clearing encryption key
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: STATE* Leaving safe mode after  <*>  secs 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: STATE* Network topology has  <*>  racks and  <*>  datanodes 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: STATE* UnderReplicatedBlocks has  <*>  blocks 
DEBUG org.apache.hadoop.hdfs.server.namenode.NameNode: Setting fs.defaultFS to  <*> 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: set restore failed storage to  val 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Enabling async auditlog
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Logj is required to enable async auditlog
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: <*> 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.HeartbeatManager: Setting heartbeat recheck interval to  <*>  since  dfs.namenode.stale.datanode.interval  is less than  dfs.namenode.heartbeat.recheck-interval 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit= <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check= <*> 
WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: The given interval for marking stale datanode =  <*> , which is less than    heartbeat intervals. This may cause too frequent changes of  stale states of DataNodes since a heartbeat msg may be missing  due to temporary short-term failures. Reset stale interval to  <*> . 
WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: The given interval for marking stale datanode =  staleInterval , which is larger than heartbeat expire interval  heartbeatExpireInterval . 
INFO org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: dfs.namenode.startup.delay.block.deletion.sec is set to  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: The block deletion will start around  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable= <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.key.update.interval= <*>  min(s),  dfs.block.access.token.lifetime = <*>  min(s),  dfs.encrypt.data.transfer.algorithm = <*> 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled =  <*> 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Determined nameservice ID:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: !!! WARNING !!!\n\tThe NameNode currently runs without persistent storage.\n\tAny changes to the file system meta-data may be lost.\n\tRecommended actions:\n\t\t- shutdown and restart NameNode with configured \ propertyName \ in hdfs-site.xml; \n\t\t- use Backup Node as a persistent and up-to-date storage  of the file system meta-data. 
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: !!! WARNING !!!\n\tThe NameNode currently runs without persistent storage.\n\tAny changes to the file system meta-data may be lost.\n\tRecommended actions:\n\t\t- shutdown and restart NameNode with configured \ propertyName \ in hdfs-site.xml; \n\t\t- use Backup Node as a persistent and up-to-date storage  of the file system meta-data. 
WARN org.apache.hadoop.hdfs.server.namenode.FSEditLog: No class configured for  uriScheme ,  <*>  is empty 
WARN org.apache.hadoop.hdfs.server.namenode.FSEditLog: No class configured for  uriScheme ,  <*>  is empty 
WARN org.apache.hadoop.hdfs.server.namenode.FSEditLog: No class configured for  uriScheme ,  <*>  is empty 
WARN org.apache.hadoop.hdfs.server.namenode.NameNode: Encountered exception during format: 
ERROR org.apache.hadoop.hdfs.server.namenode.JournalSet: Disabling journal  j 
INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Reconfiguring  property  to  newVal 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Adding new volumes:  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Storage directory is loaded:  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.JournalNode: Initializing journal in directory  <*> 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Storage directory  <*>  does not exist 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Cannot access storage directory  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Scanning storage  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: No files in  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Edit log file  <*>  appears to be empty.  Moving it aside... 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Synchronizing log  <*> : old segment  <*>  is not the right length 
DEBUG org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove: Start moving  this 
DEBUG org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient: SASL client skipping handshake on trusted connection for addr = {}, datanodeId = {}
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.Sender: Sending DataTransferOp  <*> :  proto 
INFO org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove: Successfully moved  this 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from  <*>  to  volFailures 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of storages reported in heartbeat= <*> ; Number of storages in storageMap= <*> 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Deferring removal of stale storage  len$  with  <*>  blocks 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: getDatanodeListForReport with includedNodes =  <*> , excludedNodes =  <*> , foundNodes =  <*> , nodes =  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.JournalNode: Initializing journal in directory  <*> 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: <*> is not a directory 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Cannot access storage directory  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Scanning storage  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Latest log is  latestLog 
WARN org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Unable to start log segment  txid  at  <*> :  <*> 
DEBUG org.apache.hadoop.hdfs.server.common.Storage: Failed to preserve last modified date from\' srcFile \' to \' destFile \' 
DEBUG org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Copied  srcMeta  to  <*>  and calculated checksum 
DEBUG org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Copied  srcFile  to  <*> 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Storage directory  <*>  does not exist 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Cannot access storage directory  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.DataStorage: Storage directory  dataDir  is not formatted for  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Formatting ...
ERROR org.apache.hadoop.hdfs.server.common.Storage: *********** Upgrade is not supported from this  older version  oldVersion  of storage to the current version.  Please upgrade to  Hadoop-.  or a later version and then upgrade to current  version. Old layout version is  <*>  and latest layout version this software version can  upgrade from is  $i . ************ 
INFO org.apache.hadoop.hdfs.server.datanode.DataStorage: Generated new storageID  <*>  for directory  <*> <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: <*>  does not exist. Creating ... 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Locking is disabled for  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage: Rolling back storage directory  <*> .\n   target LV =  <*> ; target CTime =  <*> 
ERROR org.apache.hadoop.hdfs.server.common.Storage: *********** Upgrade is not supported from this  older version  oldVersion  of storage to the current version.  Please upgrade to  Hadoop-.  or a later version and then upgrade to current  version. Old layout version is  <*>  and latest layout version this software version can  upgrade from is  $i . ************ 
INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage: Upgrading block pool storage directory  <*> .\n   old LV =  <*> ; old CTime =  <*> .\n   new LV =  <*> ; new CTime =  <*> 
ERROR org.apache.hadoop.hdfs.server.datanode.DataStorage: There are  <*>  duplicate block  entries within the same volume. 
WARN org.apache.hadoop.hdfs.server.datanode.DataStorage: Unexpectedly short length on  <*> . 
ERROR org.apache.hadoop.hdfs.server.datanode.DataStorage: There are  <*>  duplicate block  entries within the same volume. 
WARN org.apache.hadoop.hdfs.server.datanode.DataStorage: Unexpectedly low genstamp on  <*> . 
INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage: Upgrade of block pool  <*>  at  <*>  is complete 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Locking is disabled for  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Completing previous checkpoint for storage directory  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage: Not overwriting  $u  with smaller file from  trash directory. This message can be safely ignored. 
INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage: Restored  <*>  block files from trash. 
ERROR org.apache.hadoop.hdfs.server.common.Storage: *********** Upgrade is not supported from this  older version  oldVersion  of storage to the current version.  Please upgrade to  Hadoop-.  or a later version and then upgrade to current  version. Old layout version is  <*>  and latest layout version this software version can  upgrade from is  $i . ************ 
INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage: Upgrading block pool storage directory  <*> .\n   old LV =  <*> ; old CTime =  <*> .\n   new LV =  <*> ; new CTime =  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: <*>  does not exist. Creating ... 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Lock on  <*>  acquired by nodename  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Cannot lock storage  <*> . The directory is already locked 
ERROR org.apache.hadoop.hdfs.server.common.Storage: *********** Upgrade is not supported from this  older version  oldVersion  of storage to the current version.  Please upgrade to  Hadoop-.  or a later version and then upgrade to current  version. Old layout version is  <*>  and latest layout version this software version can  upgrade from is  $i . ************ 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Recovering storage directory  <*>  from previous rollback 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Failed to move  <*>  to  <*> 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Failed to move  <*>  to  <*> 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Failed to mkdirs  <*> 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Failed to mkdirs  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Recovered  <*>  replicas from  <*> 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil: Block  blockFile  does not have a metafile! 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil: Block  blockFile  does not have a metafile! 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil: Block  blockFile  does not have a metafile! 
DEBUG org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: resolveDuplicateReplicas decide to keep  replicaToKeep .  Will try to delete  <*> 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaTracker$RamDiskReplica: Failed to delete meta file  <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: resolveDuplicateReplicas decide to keep  replicaToKeep .  Will try to delete  <*> 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaTracker$RamDiskReplica: Failed to delete block file  <*> 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaTracker$RamDiskReplica: Failed to delete meta file  <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: resolveDuplicateReplicas decide to keep  replicaToKeep .  Will try to delete  <*> 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil: Block  blockFile  does not have a metafile! 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil: Block  blockFile  does not have a metafile! 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil: Block  blockFile  does not have a metafile! 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Failed to delete restart meta file:  <*> 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Failed to delete restart meta file:  <*> 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Failed to delete restart meta file:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: Resolved path is  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
DEBUG org.apache.hadoop.hdfs.DFSOutputStream: Queued packet  <*> 
DEBUG org.apache.hadoop.hdfs.DFSOutputStream: computePacketChunkSize: src= <*> , chunkSize= chunkSize , chunksPerPacket= <*> , packetSize= <*> 
DEBUG org.apache.hadoop.hdfs.DFSOutputStream: Queued packet  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: Resolved path is  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker: ACCESS CHECK:  this , doCheckOwner= doCheckOwner , ancestorAccess= ancestorAccess , parentAccess= parentAccess , access= access , subAccess= subAccess , ignoreEmptyDir= ignoreEmptyDir , resolveLink= resolveLink 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
DEBUG org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: lastAckedSeqno =  <*> 
DEBUG org.apache.hadoop.hdfs.DFSClient: Using local interface  addr 
DEBUG org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient: SASL client skipping handshake in secured configuration with no SASL protection configured for addr = {}, datanodeId = {}
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.Sender: Sending DataTransferOp  <*> :  proto 
DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer: Connecting to datanode  <*> 
DEBUG org.apache.hadoop.hdfs.DFSClient: Using local interface  addr 
DEBUG org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient: SASL client skipping handshake in unsecured configuration for addr = {}, datanodeId = {}
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.Sender: Sending DataTransferOp  <*> :  proto 
DEBUG org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: lastAckedSeqno =  <*> 
DEBUG org.apache.hadoop.hdfs.DFSClient: Using local interface  addr 
DEBUG org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient: SASL client skipping handshake in secured configuration with privileged port for addr = {}, datanodeId = {}
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.Sender: Sending DataTransferOp  <*> :  proto 
INFO org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: nodes are empty for write pipeline of block  <*> 
WARN org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: Error recovering pipeline for writing  <*> . Already retried  times for the same packet. 
DEBUG org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: Append to block  <*> 
WARN org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: Datanode did not restart in time:  <*> 
DEBUG org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: DataStreamer block  <*>  sending packet  e_ 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: Resolved path is  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker: ACCESS CHECK:  this , doCheckOwner= doCheckOwner , ancestorAccess= ancestorAccess , parentAccess= parentAccess , access= access , subAccess= subAccess , ignoreEmptyDir= ignoreEmptyDir , resolveLink= resolveLink 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
DEBUG org.apache.hadoop.hdfs.server.datanode.DataXceiver: Checking block access token for block \' <*> \' with mode \' mode \' 
DEBUG org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager: Checking access for user= userId , block= block , access mode= mode  using  <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.BlockSender: Bumping up the client provided block\'s genstamp to latest  <*>  for block  block 
DEBUG org.apache.hadoop.hdfs.server.datanode.BlockSender: block= block , replica= <*> 
WARN org.apache.hadoop.hdfs.server.datanode.BlockMetadataHeader: Unexpected meta-file version for  name : version in file is  $i  but expected version is   
WARN org.apache.hadoop.hdfs.server.datanode.BlockSender: <*> :sendBlock() :  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.DataXceiver: Copied  block  to  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Waiting until the NameNode rolls its edit logs in order to freeze the BackupNode namespace.
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: BackupNode namespace frozen.
DEBUG org.apache.hadoop.hdfs.server.namenode.Checkpointer: Doing checkpoint. Last applied:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Unable to roll forward using only logs. Downloading image with txid  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to  <*> 
DEBUG org.apache.hadoop.hdfs.web.URLConnectionFactory: open AuthenticatedURL connection url 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to  <*>  milliseconds 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file  <*>  size  <*>  bytes. 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
DEBUG org.apache.hadoop.hdfs.util.MD5FileUtils: Saved MD  digestString  to  <*> 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: renaming   <*>  to  <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Dest file:  f 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to  <*> 
DEBUG org.apache.hadoop.hdfs.web.URLConnectionFactory: open AuthenticatedURL connection url 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to  <*>  milliseconds 
WARN org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Overwriting existing file  f  with file downloaded from  url 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file  <*>  size  <*>  bytes. 
DEBUG org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Renaming  <*>  to  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Unable to rename edits file from  <*>  to  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Dest file:  f 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to  <*> 
DEBUG org.apache.hadoop.hdfs.web.URLConnectionFactory: open URL connection
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to  <*>  milliseconds 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file  <*>  size  <*>  bytes. 
DEBUG org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Renaming  <*>  to  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from  <*>  stream(s). 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: About to load edits:\n   <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: BUG: Namespace quota violation in image for  <*>  quota =  <*>  < consumed =  child# 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: BUG: Diskspace quota violation in image for  <*>  quota =  <*>  < consumed =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with  <*>  entries  <*>  lookups 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpoint completed in  <*>  seconds.  New Image Size:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Waiting until the NameNode rolls its edit logs in order to freeze the BackupNode namespace.
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: BackupNode namespace frozen.
DEBUG org.apache.hadoop.hdfs.server.namenode.Checkpointer: Doing checkpoint. Last applied:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Loading image with txid  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: Reloading namespace from  file 
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from  <*>  stream(s). 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: About to load edits:\n   <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading  editIn  expecting start txid # <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: BUG: Diskspace quota violation in image for  <*>  quota =  <*>  < consumed =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with  <*>  entries  <*>  lookups 
DEBUG org.apache.hadoop.hdfs.web.URLConnectionFactory: open URL connection
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to  <*>  milliseconds 
WARN org.apache.hadoop.hdfs.server.namenode.TransferFsImage: SIMULATING A CORRUPT BYTE IN IMAGE TRANSFER!
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Connection closed by client
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid  txid  to namenode at  fsName  in  <*>  seconds 
INFO org.apache.hadoop.hdfs.server.namenode.JournalSet: Skipping jas  jas  since it\'s disabled 
INFO org.apache.hadoop.hdfs.server.namenode.JournalSet: Skipping jas  jas  since it\'s disabled 
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Going to finish converging with remaining  editStreamsAll  txns from in-progress stream  stream 
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Loading edits into backupnode to try to catch up from txid  <*>  to  <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.JournalSet: Disabling journal  j 
ERROR org.apache.hadoop.hdfs.server.namenode.JournalSet: Error:  <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.JournalSet: Disabling journal  j 
ERROR org.apache.hadoop.hdfs.server.namenode.JournalSet: Error:  <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.JournalSet: Disabling journal  j 
ERROR org.apache.hadoop.hdfs.server.namenode.JournalSet: Error:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Going to finish converging with remaining  editStreamsAll  txns from in-progress stream  stream 
CLOSEALLSTREAMS org.apache.hadoop.hdfs.server.namenode.BackupImage:  <*> <*>
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Successfully synced BackupNode with NameNode at txnid  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.BackupImage: State transition  <*>  ->  newState 
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Loading edits into backupnode to try to catch up from txid  <*>  to  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Going to finish converging with remaining  editStreamsAll  txns from in-progress stream  stream 
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpoint completed in  <*>  seconds.  New Image Size:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Waiting until the NameNode rolls its edit logs in order to freeze the BackupNode namespace.
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: BackupNode namespace frozen.
DEBUG org.apache.hadoop.hdfs.server.namenode.Checkpointer: Doing checkpoint. Last applied:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Loading image with txid  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: Reloading namespace from  file 
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from  <*>  stream(s). 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: About to load edits:\n   <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: BUG: Diskspace quota violation in image for  <*>  quota =  <*>  < consumed =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with  <*>  entries  <*>  lookups 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: msg  \n <*> 
DEBUG org.apache.hadoop.hdfs.web.URLConnectionFactory: open URL connection
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to  <*>  milliseconds 
WARN org.apache.hadoop.hdfs.server.namenode.TransferFsImage: SIMULATING A CORRUPT BYTE IN IMAGE TRANSFER!
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid  txid  to namenode at  fsName  in  <*>  seconds 
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpoint completed in  <*>  seconds.  New Image Size:  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: initReplicaRecovery:  block , recoveryId= recoveryId , replica= <*> 
WARN org.apache.hadoop.hdfs.server.datanode.BlockReceiver: <*> \n <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: initReplicaRecovery: changing replica state for  block  from  <*>  to  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NameNode: Setting fs.defaultFS to  <*> 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Edits URI  dir  listed multiple times in  dfs.namenode.shared.edits.dir . Ignoring duplicates. 
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: !!! WARNING !!!\n\tThe NameNode currently runs without persistent storage.\n\tAny changes to the file system meta-data may be lost.\n\tRecommended actions:\n\t\t- shutdown and restart NameNode with configured \ propertyName \ in hdfs-site.xml; \n\t\t- use Backup Node as a persistent and up-to-date storage  of the file system meta-data. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Edits URI  i$  listed multiple times in  dfs.namenode.shared.edits.dir  and  dfs.namenode.edits.dir . Ignoring duplicates. 
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: !!! WARNING !!!\n\tThe NameNode currently runs without persistent storage.\n\tAny changes to the file system meta-data may be lost.\n\tRecommended actions:\n\t\t- shutdown and restart NameNode with configured \ propertyName \ in hdfs-site.xml; \n\t\t- use Backup Node as a persistent and up-to-date storage  of the file system meta-data. 
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Edits URI  dir  listed multiple times in  dfs.namenode.shared.edits.dir . Ignoring duplicates. 
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: !!! WARNING !!!\n\tThe NameNode currently runs without persistent storage.\n\tAny changes to the file system meta-data may be lost.\n\tRecommended actions:\n\t\t- shutdown and restart NameNode with configured \ propertyName \ in hdfs-site.xml; \n\t\t- use Backup Node as a persistent and up-to-date storage  of the file system meta-data. 
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: !!! WARNING !!!\n\tThe NameNode currently runs without persistent storage.\n\tAny changes to the file system meta-data may be lost.\n\tRecommended actions:\n\t\t- shutdown and restart NameNode with configured \ propertyName \ in hdfs-site.xml; \n\t\t- use Backup Node as a persistent and up-to-date storage  of the file system meta-data. 
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: !!! WARNING !!!\n\tThe NameNode currently runs without persistent storage.\n\tAny changes to the file system meta-data may be lost.\n\tRecommended actions:\n\t\t- shutdown and restart NameNode with configured \ propertyName \ in hdfs-site.xml; \n\t\t- use Backup Node as a persistent and up-to-date storage  of the file system meta-data. 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: set restore failed storage to  val 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Found KeyProvider:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: <*> 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.HeartbeatManager: Setting heartbeat recheck interval to  <*>  since  dfs.namenode.stale.datanode.interval  is less than  dfs.namenode.heartbeat.recheck-interval 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit= <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check= <*> 
WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: The given interval for marking stale datanode =  <*> , which is less than    heartbeat intervals. This may cause too frequent changes of  stale states of DataNodes since a heartbeat msg may be missing  due to temporary short-term failures. Reset stale interval to  <*> . 
INFO org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: dfs.namenode.startup.delay.block.deletion.sec is set to  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: The block deletion will start around  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable= <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.key.update.interval= <*>  min(s),  dfs.block.access.token.lifetime = <*>  min(s),  dfs.encrypt.data.transfer.algorithm = <*> 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled =  <*> 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Determined nameservice ID:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: STATE* Leaving safe mode after  <*>  secs 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: STATE* Network topology has  <*>  racks and  <*>  datanodes 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: STATE* UnderReplicatedBlocks has  <*>  blocks 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Storage directory  <*>  does not exist 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Cannot access storage directory  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: <*>  does not exist. Creating ... 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Locking is disabled for  <*> 
TRACE org.apache.hadoop.hdfs.server.namenode.FSImage: Data dir states:\n   <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: Clusterid mismatch - current clusterid:  <*> , Ignoring given clusterid:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: Using clusterid:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Storage directory  <*>  is not formatted. 
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Formatting ...
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image?  <*>#_  (staleImage= <*> , haEnabled= <*> , isRollingUpgrade= <*> ) 
INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with  <*>  entries  <*>  lookups 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in  ioe  msecs 
WARN org.apache.hadoop.hdfs.server.namenode.FSEditLog: No class configured for  uriScheme ,  <*>  is empty 
WARN org.apache.hadoop.hdfs.server.namenode.FSEditLog: No class configured for  uriScheme ,  <*>  is empty 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: Storage directory  <*>  has been successfully formatted. 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: Storage directory  <*>  has been successfully formatted. 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSEditLog: Closing log when already closed
ERROR org.apache.hadoop.hdfs.server.namenode.FSEditLog: No edits directories configured!
ERROR org.apache.hadoop.hdfs.server.namenode.JournalSet: Disabling journal  j 
ERROR org.apache.hadoop.hdfs.server.namenode.JournalSet: Error:  <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.JournalSet: Disabling journal  j 
INFO org.apache.hadoop.hdfs.server.namenode.JournalSet: Skipping jas  jas  since it\'s disabled 
DEBUG org.apache.hadoop.hdfs.server.namenode.NameNode: Beginning to copy stream  stream  to shared edits 
TRACE org.apache.hadoop.hdfs.server.namenode.NameNode: copying op:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at  segmentTxId 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: NNStorage.attemptRestoreRemovedStorage: check removed(failed) storarge. removedStorages size =  <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.JournalSet: Disabling journal  j 
INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at  segmentTxId 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: NNStorage.attemptRestoreRemovedStorage: check removed(failed) storarge. removedStorages size =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: currently disabled dir  <*> ; type= <*> ;canwrite= <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.JournalSet: Disabling journal  j 
ERROR org.apache.hadoop.hdfs.server.namenode.JournalSet: Error:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NameNode: ending log segment because of end of stream in  stream 
INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment  <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.JournalSet: Disabling journal  j 
INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment  <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.JournalSet: Disabling journal  j 
ERROR org.apache.hadoop.hdfs.server.namenode.JournalSet: Error:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: Stop Decommissioning  node 
TRACE org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* processOverReplicatedBlock: Postponing  block  since storage  storage  does not yet have up-to-date information. 
TRACE org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* processOverReplicatedBlock: Postponing  block  since storage  storage  does not yet have up-to-date information. 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Invalidated  numOverReplicated  over-replicated blocks on  srcNode  during recommissioning 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: Stop Decommissioning  node 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from  <*>  to  volFailures 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: storageInfo  failed. 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from  <*>  to  volFailures 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: storageInfo  failed. 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: storageInfo  failed. 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Block:  block , Expected Replicas:  <*> , live replicas:  <*> , corrupt replicas:  <*> , decommissioned replicas:  <*> , excess replicas:  <*> , Is Open File:  <*> , Datanodes having this block:  <*> , Current Datanode:  srcNode , Is current datanode decommissioning:  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Block:  block , Expected Replicas:  <*> , live replicas:  <*> , corrupt replicas:  <*> , decommissioned replicas:  <*> , excess replicas:  <*> , Is Open File:  <*> , Datanodes having this block:  <*> , Current Datanode:  srcNode , Is current datanode decommissioning:  <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.add: block  has only  curReplicas  replicas and need  expectedReplicas  replicas so is added to neededReplications  at priority level  <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.add: block  has only  curReplicas  replicas and need  expectedReplicas  replicas so is added to neededReplications  at priority level  <*> 
WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: srcNode  srcNode  is dead  when decommission is in progress. Continue to mark  it as decommission in progress. In that way, when it rejoins the  cluster it can continue the decommission process. 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Block:  block , Expected Replicas:  <*> , live replicas:  <*> , corrupt replicas:  <*> , decommissioned replicas:  <*> , excess replicas:  <*> , Is Open File:  <*> , Datanodes having this block:  <*> , Current Datanode:  srcNode , Is current datanode decommissioning:  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Block:  block , Expected Replicas:  <*> , live replicas:  <*> , corrupt replicas:  <*> , decommissioned replicas:  <*> , excess replicas:  <*> , Is Open File:  <*> , Datanodes having this block:  <*> , Current Datanode:  srcNode , Is current datanode decommissioning:  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Block:  block , Expected Replicas:  <*> , live replicas:  <*> , corrupt replicas:  <*> , decommissioned replicas:  <*> , excess replicas:  <*> , Is Open File:  <*> , Datanodes having this block:  <*> , Current Datanode:  srcNode , Is current datanode decommissioning:  <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.add: block  has only  curReplicas  replicas and need  expectedReplicas  replicas so is added to neededReplications  at priority level  <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.add: block  has only  curReplicas  replicas and need  expectedReplicas  replicas so is added to neededReplications  at priority level  <*> 
WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: srcNode  srcNode  is dead  when decommission is in progress. Continue to mark  it as decommission in progress. In that way, when it rejoins the  cluster it can continue the decommission process. 
INFO org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager: Setting block keys
INFO org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager: Setting block keys
INFO org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager: Setting block keys
INFO org.apache.hadoop.hdfs.server.datanode.DataNode: who  calls recoverBlock( <*> , targets=[ <*> ] , newGenerationStamp= <*> ) 
INFO org.apache.hadoop.hdfs.server.datanode.DataNode: who  calls recoverBlock( <*> , targets=[ <*> ] , newGenerationStamp= <*> ) 
INFO org.apache.hadoop.hdfs.server.datanode.DataNode: who  calls recoverBlock( <*> , targets=[ <*> ] , newGenerationStamp= <*> ) 
DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode: Connecting to datanode  <*>  addr= <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode: Connecting to datanode  <*>  addr= <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode: Connecting to datanode  <*>  addr= <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode: Connecting to datanode  <*>  addr= <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode: Connecting to datanode  <*>  addr= <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode: block= <*> , (length= <*> ), syncList= syncList 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.server.namenode.ClusterJspHelper: Cluster console encounters a not handled situtation.
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: Resolved path is  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
DEBUG org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker: ACCESS CHECK:  this , doCheckOwner= doCheckOwner , ancestorAccess= ancestorAccess , parentAccess= parentAccess , access= access , subAccess= subAccess , ignoreEmptyDir= ignoreEmptyDir , resolveLink= resolveLink 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Inconsistent number of corrupt replicas for  blk  blockMap has  <*>  but corrupt replicas map has  <*> 
WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Inconsistent number of corrupt replicas for  blk  blockMap has  <*>  but corrupt replicas map has  <*> 
WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Inconsistent number of corrupt replicas for  blk  blockMap has  <*>  but corrupt replicas map has  <*> 
WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Inconsistent number of corrupt replicas for  blk  blockMap has  <*>  but corrupt replicas map has  <*> 
DEBUG org.apache.hadoop.hdfs.web.TokenAspect: Created new DT for  <*> 
TRACE org.apache.hadoop.hdfs.web.WebHdfsFileSystem: url= <*> 
DEBUG org.apache.hadoop.hdfs.web.URLConnectionFactory: open URL connection
WARN org.apache.hadoop.hdfs.DFSClient$Conf: Bad checksum type:  <*> . Using default  CRCC 
DEBUG org.apache.hadoop.hdfs.DFSClient$Conf: dfs.client.use.legacy.blockreader.local =  <*> 
DEBUG org.apache.hadoop.hdfs.DFSClient$Conf: dfs.client.read.shortcircuit =  <*> 
DEBUG org.apache.hadoop.hdfs.DFSClient$Conf: dfs.client.domain.socket.data.traffic =  <*> 
DEBUG org.apache.hadoop.hdfs.DFSClient$Conf: dfs.domain.socket.path =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Found KeyProvider:  <*> 
DEBUG org.apache.hadoop.hdfs.DFSClient: Using local interfaces [ <*> ] with addresses [ <*> ] 
INFO org.apache.hadoop.hdfs.PeerCache: SocketCache disabled.
DEBUG org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory: feature  is enabled. 
DEBUG org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil: DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for {}
DEBUG org.apache.hadoop.hdfs.server.common.JspHelper: getUGI is returning:  <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer: Connecting to datanode  <*> 
DEBUG org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient: SASL client doing general handshake for addr = {}, datanodeId = {}
DEBUG org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil: Verifying QOP, requested QOP = {}, negotiated QOP = {}
DEBUG org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil: Creating IOStreamPair of CryptoInputStream and CryptoOutputStream.
DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer: Connecting to datanode  <*> 
DEBUG org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient: SASL client doing general handshake for addr = {}, datanodeId = {}
DEBUG org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil: Verifying QOP, requested QOP = {}, negotiated QOP = {}
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.Sender: Sending DataTransferOp  <*> :  proto 
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.Sender: Sending DataTransferOp  <*> :  proto 
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.Sender: Sending DataTransferOp  <*> :  proto 
DEBUG org.apache.hadoop.hdfs.DFSClient: set bytesPerCRC= bytesPerCRC , crcPerBlock= crcPerBlock 
DEBUG org.apache.hadoop.hdfs.DFSClient: got reply from  <*> : md= $u 
INFO org.apache.hadoop.hdfs.qjournal.server.JournalNode: Initializing journal in directory  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: <*>  does not exist. Creating ... 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Scanning storage  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Latest log is  latestLog 
WARN org.apache.hadoop.hdfs.qjournal.server.Journal: Latest log  latestLog  has no transactions.  moving it aside and looking for previous log 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: No files in  <*> 
DEBUG org.apache.hadoop.hdfs.DFSInotifyEventInputStream: poll(): lastReadTxid is -, reading current txid from NN
DEBUG org.apache.hadoop.hdfs.DFSInotifyEventInputStream: poll(): read no edits from the NN when requesting edits after txid {}
DEBUG org.apache.hadoop.hdfs.DFSInotifyEventInputStream: take(): poll() returned null, sleeping for {} ms
DEBUG org.apache.hadoop.hdfs.DFSInotifyEventInputStream: poll(): read no edits from the NN when requesting edits after txid {}
DEBUG org.apache.hadoop.hdfs.DFSInotifyEventInputStream: poll(): lastReadTxid is -, reading current txid from NN
DEBUG org.apache.hadoop.hdfs.DFSInotifyEventInputStream: poll(): read no edits from the NN when requesting edits after txid {}
DEBUG org.apache.hadoop.hdfs.server.namenode.FileJournalManager: selecting edit log stream  elf 
DEBUG org.apache.hadoop.hdfs.server.namenode.FileJournalManager: passing over  elf  because it ends at  <*> , but we only care about transactions  as new as  fromTxId 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            =  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          =  nrInvalid 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks =  nrUnderReplicated 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks =  nrOverReplicated <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    =  nrUnderConstruction 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in  <*>  msec 
INFO org.apache.hadoop.hdfs.server.namenode.CacheManager: modifyCachePool of {} successful; {}
INFO org.apache.hadoop.hdfs.qjournal.server.JournalNode: Initializing journal in directory  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: <*>  does not exist. Creating ... 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Lock on  <*>  acquired by nodename  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Cannot lock storage  <*> . The directory is already locked 
ERROR org.apache.hadoop.hdfs.server.common.Storage: *********** Upgrade is not supported from this  older version  oldVersion  of storage to the current version.  Please upgrade to  Hadoop-.  or a later version and then upgrade to current  version. Old layout version is  <*>  and latest layout version this software version can  upgrade from is  $i . ************ 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Scanning storage  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: No files in  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Updating lastPromisedEpoch from  <*>  to  newEpoch  for client  <*> 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Edit log file  <*>  appears to be empty.  Moving it aside... 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Synchronizing log  <*> : old segment  <*>  is not the right length 
LOGFINALIZEROLLINGUPGRADE org.apache.hadoop.hdfs.server.namenode.FSNamesystem: <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Synchronizing log  <*>  from  url 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Accepted recovery for segment  <*> :  <*> 
DEBUG org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager: Checking access for user= userId , block= block , access mode= mode  using  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.DataXceiver: Receiving  block  src:  <*>  dest:  <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.BlockReceiver: <*> :  block \n  isClient  = <*> , clientname= clientname \n  isDatanode= <*> , srcDataNode= srcDataNode \n  inAddr= inAddr , myAddr= myAddr \n  cachingStrategy =  cachingStrategy 
WARN org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: No block pool scanner found for block pool id:  poolId 
ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: Cannot find BPOfferService for reporting block receiving for bpid= <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer: Connecting to datanode  <*> 
DEBUG org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient: SASL client skipping handshake in secured configuration with unsecured cluster for addr = {}, datanodeId = {}
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.Sender: Sending DataTransferOp  <*> :  proto 
INFO org.apache.hadoop.hdfs.server.datanode.DataXceiver: Datanode  <*>  forwarding connect ack to upstream firstbadlink is  firstBadLink 
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver: readNextPacket: dataPlusChecksumLen =  dataPlusChecksumLen  headerLen =  $i 
DEBUG org.apache.hadoop.hdfs.server.datanode.BlockReceiver: Receiving one packet for block  <*> :  <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder: <*> : enqueue  <*> 
WARN org.apache.hadoop.hdfs.server.datanode.BlockReceiver: Slow BlockReceiver write packet to mirror took  duration_ ms (threshold= <*> ms) 
DEBUG org.apache.hadoop.hdfs.server.datanode.BlockReceiver: Receiving an empty packet or the end of the block  <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder: <*> : enqueue  <*> 
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver: readNextPacket: dataPlusChecksumLen =  dataPlusChecksumLen  headerLen =  $i 
DEBUG org.apache.hadoop.hdfs.server.datanode.BlockReceiver: Receiving one packet for block  <*> :  <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder: <*> : enqueue  <*> 
WARN org.apache.hadoop.hdfs.server.datanode.BlockReceiver: Slow BlockReceiver write packet to mirror took  duration_ ms (threshold= <*> ms) 
DEBUG org.apache.hadoop.hdfs.server.datanode.BlockReceiver: Read in partial CRC chunk from disk for  <*> 
WARN org.apache.hadoop.hdfs.server.datanode.BlockReceiver: Slow flushOrSync took  <*> ms (threshold= <*> ms), isSync: isSync , flushTotalNanos= flushTotalNanos ns 
WARN org.apache.hadoop.hdfs.server.datanode.BlockReceiver: Slow manageWriterOsCache took  <*> ms (threshold= <*> ms) 
DEBUG org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder: <*> : closing 
INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting CheckDiskError Thread
TRACE org.apache.hadoop.hdfs.server.datanode.DataXceiver: TRANSFER: send close-ack
DEBUG org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager: Checking access for user= userId , block= block , access mode= mode  using  <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.BlockSender: Bumping up the client provided block\'s genstamp to latest  <*>  for block  block 
DEBUG org.apache.hadoop.hdfs.server.datanode.BlockSender: block= block , replica= <*> 
WARN org.apache.hadoop.hdfs.server.datanode.BlockSender: Could not find metadata file for  block 
WARN org.apache.hadoop.hdfs.server.datanode.BlockSender: <*> :sendBlock() :  <*> 
DEBUG org.apache.hadoop.hdfs.tools.GetGroups: Using NN principal:  <*> 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: hadoop.ssl.enabled is deprecated. Please use dfs.http.policy.
WARN org.apache.hadoop.hdfs.DFSUtil: dfs.https.enable is deprecated. Please use dfs.http.policy.
INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for  name  at:  <*> 
WARN org.apache.hadoop.hdfs.DFSUtil: hadoop.ssl.enabled is deprecated. Please use dfs.http.policy.
DEBUG org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer: SASL server skipping handshake in secured configuration for peer = {}, datanodeId = {}
INFO org.apache.hadoop.hdfs.server.datanode.DataXceiver: <*> ;  <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.DataXceiver: <*> :Number of active connections is:  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.JournalNode: Initializing journal in directory  <*> 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Storage directory  <*>  does not exist 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Cannot access storage directory  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Scanning storage  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Latest log is  latestLog 
WARN org.apache.hadoop.hdfs.qjournal.server.Journal: Latest log  latestLog  has no transactions.  moving it aside and looking for previous log 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: No files in  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil: Rollback of  <*>  is complete. 
INFO org.apache.hadoop.hdfs.qjournal.server.JournalNode: Initializing journal in directory  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: <*>  does not exist. Creating ... 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: <*> is not a directory 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Cannot access storage directory  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Scanning storage  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Latest log is  latestLog 
WARN org.apache.hadoop.hdfs.qjournal.server.Journal: Latest log  latestLog  has no transactions.  moving it aside and looking for previous log 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: No files in  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Starting upgrade of edits directory: .\n   old LV =  <*> ; old CTime =  <*> .\n   new LV =  <*> ; new CTime =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil: Performing upgrade of storage directory  <*> 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
DEBUG org.apache.hadoop.hdfs.web.TokenAspect: Created new DT for  <*> 
TRACE org.apache.hadoop.hdfs.web.WebHdfsFileSystem: url= <*> 
DEBUG org.apache.hadoop.hdfs.web.URLConnectionFactory: open URL connection
WARN org.apache.hadoop.hdfs.DFSClient$Conf: Bad checksum type:  <*> . Using default  CRCC 
DEBUG org.apache.hadoop.hdfs.DFSClient$Conf: dfs.client.use.legacy.blockreader.local =  <*> 
DEBUG org.apache.hadoop.hdfs.DFSClient$Conf: dfs.client.read.shortcircuit =  <*> 
DEBUG org.apache.hadoop.hdfs.DFSClient$Conf: dfs.client.domain.socket.data.traffic =  <*> 
DEBUG org.apache.hadoop.hdfs.DFSClient$Conf: dfs.domain.socket.path =  <*> 
DEBUG org.apache.hadoop.hdfs.DFSClient: Using legacy short-circuit local reads.
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Found KeyProvider:  <*> 
INFO org.apache.hadoop.hdfs.PeerCache: SocketCache disabled.
DEBUG org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory: Both short-circuit local reads and UNIX domain socket are disabled.
DEBUG org.apache.hadoop.hdfs.DFSClient: Using hedged reads; pool threads= num 
DEBUG org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil: DataTransferProtocol using SaslPropertiesResolver, configured QOP {} = {}, configured class {} = {}
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
INFO org.apache.hadoop.hdfs.server.namenode.ImageServlet: ImageServlet rejecting:  remoteUser 
WARN org.apache.hadoop.hdfs.server.namenode.ImageServlet: Received non-NN/SNN/administrator request for image or edits from  <*>  at  <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer: Connecting to datanode  <*> 
WARN org.apache.hadoop.hdfs.DFSInputStream: <*> <*> . Throwing a BlockMissingException 
DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer: Connecting to datanode  <*> 
WARN org.apache.hadoop.hdfs.DFSInputStream: <*> <*> . Throwing a BlockMissingException 
DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer: Connecting to datanode  <*> 
WARN org.apache.hadoop.hdfs.DFSInputStream: <*> <*> . Throwing a BlockMissingException 
TRACE org.apache.hadoop.hdfs.DFSClient: Address  targetAddr <*> 
TRACE org.apache.hadoop.hdfs.BlockReaderFactory: this : returning new remote block reader using  UNIX domain socket on  <*> 
INFO org.apache.hadoop.hdfs.DFSInputStream: Successfully connected to  <*>  for  <*> 
INFO org.apache.hadoop.hdfs.DFSInputStream: Access token was invalid when connecting to  targetAddr  :  ex 
INFO org.apache.hadoop.hdfs.DFSInputStream: Access token was invalid when connecting to  targetAddr  :  ex 
WARN org.apache.hadoop.hdfs.DFSInputStream: Failed to connect to  <*>  for block , add to deadNodes and continue.  <*> 
WARN org.apache.hadoop.hdfs.DFSInputStream: Found Checksum error for  <*>  from  <*>  at  <*> 
INFO org.apache.hadoop.hdfs.DFSInputStream: Successfully connected to  <*>  for  <*> 
WARN org.apache.hadoop.hdfs.DFSInputStream: Failed to connect to  <*>  for block , add to deadNodes and continue.  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: STATE* Leaving safe mode after  <*>  secs 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: STATE* Network topology has  <*>  racks and  <*>  datanodes 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: STATE* UnderReplicatedBlocks has  <*>  blocks 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: BLOCK*  <*> : add  block  to  datanode 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: BLOCK*  <*> : add  block  to  datanode 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* addToInvalidates:  b   <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  i 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* addToInvalidates:  b   <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  priLevel 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$SlotReleaser: <*> : about to release  <*> 
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.Sender: Sending DataTransferOp  <*> :  proto 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$SlotReleaser: <*> : released  <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm: this : unregisterSlot  slotIdx 
TRACE org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager$EndpointShmManager: this : freeing empty stale  shm 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm: this : freed 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica: this  is not stale because it\'s only  stale#  ms old, and staleThresholdMs =  <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : unref replica  replica :  purgeReason  refCount  <*>  ->  <*> <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: demoteOldEvictable: demoting  <*> :  <*> :  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: Resolved path is  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: Resolved path is  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
DEBUG org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker: ACCESS CHECK:  this , doCheckOwner= doCheckOwner , ancestorAccess= ancestorAccess , parentAccess= parentAccess , access= access , subAccess= subAccess , ignoreEmptyDir= ignoreEmptyDir , resolveLink= resolveLink 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: DIR* FSDirectory.unprotectedDelete: failed to remove  src  because the root is not allowed to be deleted 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.FSDirectory: ERROR in FSDirectory.verifyINodeName
ERROR org.apache.hadoop.hdfs.server.namenode.FSDirectory: FSDirectory.verifyMaxDirItems:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSNamesystem: mkdirs: created directory  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: DIR* NameSystem.startFile:  src   <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* addToInvalidates:  b   <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  priLevel 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* addToInvalidates:  b   <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  priLevel 
INFO org.apache.hadoop.hdfs.qjournal.server.JournalNode: Initializing journal in directory  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: <*>  does not exist. Creating ... 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Lock on  <*>  acquired by nodename  <*> 
ERROR org.apache.hadoop.hdfs.server.common.Storage: *********** Upgrade is not supported from this  older version  oldVersion  of storage to the current version.  Please upgrade to  Hadoop-.  or a later version and then upgrade to current  version. Old layout version is  <*>  and latest layout version this software version can  upgrade from is  $i . ************ 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Recovering storage directory  <*>  from failed checkpoint 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Scanning storage  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: No files in  <*> 
WARN org.apache.hadoop.hdfs.server.datanode.DataStorage: Storage directory is in use.
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList: Added new volume:  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume -  <*> , StorageType:  <*> 
TRACE org.apache.hadoop.hdfs.server.namenode.CacheManager: Validating directive {} pool maxRelativeExpiryTime {}
INFO org.apache.hadoop.hdfs.server.namenode.CacheManager: addDirective of {} successful.
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Block file in volumeMap  <*>  does not exist. Updating it to the file found during scan  <*> 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Updating generation stamp for block  blockId  from  <*>  to  <*> 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Updating size of block  blockId  from  <*>  to  <*> 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Reporting the block  corruptBlock  as corrupt due to length mismatch 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: storageInfo  failed. 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of storages reported in heartbeat= <*> ; Number of storages in storageMap= <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Deferring removal of stale storage  len$  with  <*>  blocks 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Removed storage  len$  from DataNode this 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: storageInfo  failed. 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: getDatanodeListForReport with includedNodes =  <*> , excludedNodes =  <*> , foundNodes =  <*> , nodes =  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: Resolved path is  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: Resolved path is  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: DIR* FSDirectory.renameTo:  src  to  dst 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: DIR* FSDirectory.unprotectedRenameTo:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: DIR* FSDirectory.unprotectedRenameTo:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: DIR* FSDirectory.unprotectedRenameTo: failed to rename  src  to  dst  because destination\'s parent does not exist 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from  <*>  to  volFailures 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: storageInfo  failed. 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of storages reported in heartbeat= <*> ; Number of storages in storageMap= <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Deferring removal of stale storage  len$  with  <*>  blocks 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: storageInfo  failed. 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Deferring removal of stale storage  len$  with  <*>  blocks 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* NameSystem.getAdditionalBlock:  src  inodeId  fileId  for  clientName 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: Resolved path is  <*> 
ERROR org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: The resolve call returned null!
<INIT> org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: Unresolved topology mapping for host  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* NameSystem.allocateBlock: handling block allocation writing to a file with a complete previous block: src= src  lastBlock= <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoUnderConstruction: BLOCK* Removing stale replica from location:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* allocateBlock:  src .  <*>   <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSNamesystem: persistNewBlock:  path  with new block  <*> , current total block count is  <*> 
INFO org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager: Setting block keys
INFO org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager: Setting block keys
INFO org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager: Setting block keys
DEBUG org.apache.hadoop.hdfs.server.namenode.FileJournalManager: this : selecting input streams starting at  fromTxId <*> from among  <*>  candidate file(s) 
DEBUG org.apache.hadoop.hdfs.server.namenode.FileJournalManager: selecting edit log stream  elf 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
DEBUG org.apache.hadoop.hdfs.DFSInotifyEventInputStream: poll(): lastReadTxid is -, reading current txid from NN
DEBUG org.apache.hadoop.hdfs.DFSInotifyEventInputStream: poll(): read no edits from the NN when requesting edits after txid {}
DEBUG org.apache.hadoop.hdfs.DFSInotifyEventInputStream: timed poll(): poll() returned null, sleeping for {} ms
DEBUG org.apache.hadoop.hdfs.DFSInotifyEventInputStream: poll(): lastReadTxid is -, reading current txid from NN
DEBUG org.apache.hadoop.hdfs.DFSInotifyEventInputStream: poll(): lastReadTxid is -, reading current txid from NN
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: Resolved path is  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker: ACCESS CHECK:  this , doCheckOwner= doCheckOwner , ancestorAccess= ancestorAccess , parentAccess= parentAccess , access= access , subAccess= subAccess , ignoreEmptyDir= ignoreEmptyDir , resolveLink= resolveLink 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
TRACE org.apache.hadoop.hdfs.server.namenode.CacheManager: Validating directive {} pool maxRelativeExpiryTime {}
INFO org.apache.hadoop.hdfs.server.namenode.CacheManager: addDirective of {} successful.
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
DEBUG org.apache.hadoop.hdfs.server.namenode.NameNode: Setting fs.defaultFS to  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.ImageServlet: ImageServlet rejecting:  remoteUser 
WARN org.apache.hadoop.hdfs.server.namenode.ImageServlet: Received non-NN/SNN/administrator request for image or edits from  <*>  at  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Reconfiguring  property  to  newVal 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Adding new volumes:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: updatePipeline(block= oldBlock , newGenerationStamp= <*> , newLength= <*> , newNodes= <*> , clientName= clientName ) 
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Update  oldBlock  (len =  <*> ) to an older state:  newBlock  (len =  <*> ) 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: updatePipeline( oldBlock ) successfully to  newBlock 
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver: readNextPacket: dataPlusChecksumLen =  dataPlusChecksumLen  headerLen =  $i 
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver: readNextPacket: dataPlusChecksumLen =  dataPlusChecksumLen  headerLen =  $i 
TRACE org.apache.hadoop.hdfs.RemoteBlockReader2: DFSClient readNextPacket got header  <*> 
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver: readNextPacket: dataPlusChecksumLen =  dataPlusChecksumLen  headerLen =  $i 
TRACE org.apache.hadoop.hdfs.RemoteBlockReader2: DFSClient readNextPacket got header  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.JournalNode: Initializing journal in directory  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: <*>  does not exist. Creating ... 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Locking is disabled for  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Scanning storage  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Latest log is  latestLog 
WARN org.apache.hadoop.hdfs.qjournal.server.Journal: Latest log  latestLog  has no transactions.  moving it aside and looking for previous log 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: No files in  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil: Rollback of  <*>  is complete. 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Recover failed close  b 
DEBUG org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Renaming  <*>  to  <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: addFinalizedBlock: Moved  <*>  to  <*>  and  srcfile  to  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.CacheManager: addCachePool of {} successful.
DEBUG org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager: Checking access for user= userId , block= block , access mode= mode  using  <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.BlockSender: Bumping up the client provided block\'s genstamp to latest  <*>  for block  block 
DEBUG org.apache.hadoop.hdfs.server.datanode.BlockSender: block= block , replica= <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: Start Decommissioning  node   storage  with  <*>  blocks 
WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: srcNode  srcNode  is dead  when decommission is in progress. Continue to mark  it as decommission in progress. In that way, when it rejoins the  cluster it can continue the decommission process. 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.add: block  has only  curReplicas  replicas and need  expectedReplicas  replicas so is added to neededReplications  at priority level  <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.add: block  has only  curReplicas  replicas and need  expectedReplicas  replicas so is added to neededReplications  at priority level  <*> 
WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: srcNode  srcNode  is dead  when decommission is in progress. Continue to mark  it as decommission in progress. In that way, when it rejoins the  cluster it can continue the decommission process. 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: Decommission complete for  node 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Block:  block , Expected Replicas:  <*> , live replicas:  <*> , corrupt replicas:  <*> , decommissioned replicas:  <*> , excess replicas:  <*> , Is Open File:  <*> , Datanodes having this block:  <*> , Current Datanode:  srcNode , Is current datanode decommissioning:  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Block:  block , Expected Replicas:  <*> , live replicas:  <*> , corrupt replicas:  <*> , decommissioned replicas:  <*> , excess replicas:  <*> , Is Open File:  <*> , Datanodes having this block:  <*> , Current Datanode:  srcNode , Is current datanode decommissioning:  <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.add: block  has only  curReplicas  replicas and need  expectedReplicas  replicas so is added to neededReplications  at priority level  <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.add: block  has only  curReplicas  replicas and need  expectedReplicas  replicas so is added to neededReplications  at priority level  <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.add: block  has only  curReplicas  replicas and need  expectedReplicas  replicas so is added to neededReplications  at priority level  <*> 
WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: srcNode  srcNode  is dead  when decommission is in progress. Continue to mark  it as decommission in progress. In that way, when it rejoins the  cluster it can continue the decommission process. 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: Decommission complete for  node 
INFO org.apache.hadoop.hdfs.qjournal.server.JournalNode: Initializing journal in directory  <*> 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: <*> is not a directory 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Cannot access storage directory  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Scanning storage  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Latest log is  latestLog 
INFO org.apache.hadoop.hdfs.qjournal.server.JournalNode: Initializing journal in directory  <*> 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Cannot access storage directory  <*> 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Cannot access storage directory  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Completing previous rollback for storage directory  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Scanning storage  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Latest log is  latestLog 
INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Discard the EditLog files, the given start txid is  startTxId 
INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Trash the EditLog file  elf 
DEBUG org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove: Start moving  this 
DEBUG org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient: SASL client skipping handshake on trusted connection for addr = {}, datanodeId = {}
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.Sender: Sending DataTransferOp  <*> :  proto 
INFO org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove: Successfully moved  this 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
INFO org.apache.hadoop.hdfs.qjournal.server.JournalNode: Initializing journal in directory  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: <*>  does not exist. Creating ... 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Lock on  <*>  acquired by nodename  <*> 
ERROR org.apache.hadoop.hdfs.server.common.Storage: *********** Upgrade is not supported from this  older version  oldVersion  of storage to the current version.  Please upgrade to  Hadoop-.  or a later version and then upgrade to current  version. Old layout version is  <*>  and latest layout version this software version can  upgrade from is  $i . ************ 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Completing previous rollback for storage directory  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Scanning storage  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Latest log is  latestLog 
WARN org.apache.hadoop.hdfs.qjournal.server.Journal: Latest log  latestLog  has no transactions.  moving it aside and looking for previous log 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: No files in  <*> 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from  <*>  to  volFailures 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: storageInfo  failed. 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of storages reported in heartbeat= <*> ; Number of storages in storageMap= <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: getDatanodeListForReport with includedNodes =  <*> , excludedNodes =  <*> , foundNodes =  <*> , nodes =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Waiting until the NameNode rolls its edit logs in order to freeze the BackupNode namespace.
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: BackupNode namespace frozen.
DEBUG org.apache.hadoop.hdfs.server.namenode.Checkpointer: Doing checkpoint. Last applied:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to  <*> 
DEBUG org.apache.hadoop.hdfs.web.URLConnectionFactory: open URL connection
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to  <*>  milliseconds 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file  <*>  size  <*>  bytes. 
WARN org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Unable to rename edits file from  <*>  to  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Dest file:  f 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to  <*> 
DEBUG org.apache.hadoop.hdfs.web.URLConnectionFactory: open AuthenticatedURL connection url 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to  <*>  milliseconds 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file  <*>  size  <*>  bytes. 
DEBUG org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Renaming  <*>  to  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to  <*> 
DEBUG org.apache.hadoop.hdfs.web.URLConnectionFactory: open URL connection
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to  <*>  milliseconds 
WARN org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Overwriting existing file  f  with file downloaded from  url 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file  <*>  size  <*>  bytes. 
DEBUG org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Renaming  <*>  to  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Loading image with txid  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: Reloading namespace from  file 
CLOSEALLSTREAMS org.apache.hadoop.hdfs.server.namenode.BackupImage:  <*> <*>
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from  <*>  stream(s). 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: About to load edits:\n   <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: BUG: Namespace quota violation in image for  <*>  quota =  <*>  < consumed =  child# 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: BUG: Namespace quota violation in image for  <*>  quota =  <*>  < consumed =  child# 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: BUG: Diskspace quota violation in image for  <*>  quota =  <*>  < consumed =  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: BUG: Diskspace quota violation in image for  <*>  quota =  <*>  < consumed =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with  <*>  entries  <*>  lookups 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: msg  \n <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSImage: Unable to delete cancelled checkpoint in  sd 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
INFO org.apache.hadoop.hdfs.server.namenode.JournalSet: Skipping jas  jas  since it\'s disabled 
INFO org.apache.hadoop.hdfs.server.namenode.JournalSet: Skipping jas  jas  since it\'s disabled 
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Going to finish converging with remaining  editStreamsAll  txns from in-progress stream  stream 
CLOSEALLSTREAMS org.apache.hadoop.hdfs.server.namenode.BackupImage:  <*> <*>
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Successfully synced BackupNode with NameNode at txnid  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.BackupImage: State transition  <*>  ->  newState 
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Loading edits into backupnode to try to catch up from txid  <*>  to  <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.JournalSet: Disabling journal  j 
ERROR org.apache.hadoop.hdfs.server.namenode.JournalSet: Disabling journal  j 
ERROR org.apache.hadoop.hdfs.server.namenode.JournalSet: Disabling journal  j 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: About to load edits:\n   <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: About to load edits:\n   <*> 
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Going to finish converging with remaining  editStreamsAll  txns from in-progress stream  stream 
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpoint completed in  <*>  seconds.  New Image Size:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Waiting until the NameNode rolls its edit logs in order to freeze the BackupNode namespace.
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: BackupNode namespace frozen.
DEBUG org.apache.hadoop.hdfs.server.namenode.Checkpointer: Doing checkpoint. Last applied:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Unable to roll forward using only logs. Downloading image with txid  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file  <*>  size  <*>  bytes. 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
DEBUG org.apache.hadoop.hdfs.util.MD5FileUtils: Saved MD  digestString  to  <*> 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: renaming   <*>  to  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: renaming   <*>  to  <*> 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
DEBUG org.apache.hadoop.hdfs.util.MD5FileUtils: Saved MD  digestString  to  <*> 
WARN org.apache.hadoop.hdfs.util.MD5FileUtils: deleting   <*>  FAILED 
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Loading image with txid  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: Reloading namespace from  file 
INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode$Loader: Loading  <*>  INodes. 
INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode$Loader: Loading  <*>  INodes. 
INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Loader: Loaded FSImage in  <*>  seconds. 
INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid  <*>  from  curFile 
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from  <*>  stream(s). 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: About to load edits:\n   <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with  <*>  entries  <*>  lookups 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: msg  \n <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain  <*>  images with txid >=  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpoint completed in  <*>  seconds.  New Image Size:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Waiting until the NameNode rolls its edit logs in order to freeze the BackupNode namespace.
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: BackupNode namespace frozen.
DEBUG org.apache.hadoop.hdfs.server.namenode.Checkpointer: Doing checkpoint. Last applied:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Unable to roll forward using only logs. Downloading image with txid  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file  <*>  size  <*>  bytes. 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: storageInfo  failed. 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from  <*>  to  volFailures 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from  <*>  to  volFailures 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: getDatanodeListForReport with includedNodes =  <*> , excludedNodes =  <*> , foundNodes =  <*> , nodes =  <*> 
WARN org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: Error recovering pipeline for writing  <*> . Already retried  times for the same packet. 
INFO org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: Error Recovery for  <*>  waiting for responder to exit.  
WARN org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: Error Recovery for block  <*>  in pipeline  $u : bad datanode  <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer: Connecting to datanode  <*> 
DEBUG org.apache.hadoop.hdfs.DFSClient: Using local interface  addr 
DEBUG org.apache.hadoop.hdfs.DFSOutputStream: Send buf size  <*> 
DEBUG org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient: SASL client skipping handshake in secured configuration with unsecured cluster for addr = {}, datanodeId = {}
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.Sender: Sending DataTransferOp  <*> :  proto 
INFO org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: nodes are empty for write pipeline of block  <*> 
DEBUG org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: pipeline =  <*> 
WARN org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: Datanode did not restart in time:  <*> 
DEBUG org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: Closing old block  <*> 
DEBUG org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: Allocating new block
INFO org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: Abandoning  <*> 
INFO org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: Excluding datanode  <*> 
DEBUG org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: DataStreamer block  <*>  sending packet  e_ 
DEBUG org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: Closing old block  <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl$LazyWriter: LazyWriter: Start persisting RamDisk block: block pool Id:  <*>  block id:  <*>  on target volume  <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskAsyncLazyPersistService: LazyWriter schedule async task to persist RamDisk block pool id:  bpId  block id:  blockId 
DEBUG org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl$LazyWriter: LazyWriter: Start persisting RamDisk block: block pool Id:  <*>  block id:  <*>  on target volume  <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskAsyncLazyPersistService: LazyWriter schedule async task to persist RamDisk block pool id:  bpId  block id:  blockId 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskAsyncLazyPersistService: LazyWriter failed to create  <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskAsyncLazyPersistService: LazyWriter schedule async task to persist RamDisk block pool id:  bpId  block id:  blockId 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskAsyncLazyPersistService: LazyWriter failed to create  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Moved  blockFile  to  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Moved  metaFile  to  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Moved  blockFile  to  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Moved  metaFile  to  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Moved  blockFile  to  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Moved  metaFile  to  <*> 
WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Cannot find BPOfferService for reporting block received for bpid= <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Moved  blockFile  to  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Moved  metaFile  to  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Moved  blockFile  to  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Moved  metaFile  to  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Removing block pool  bpid 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Failed to delete old dfsUsed file in  <*> 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Failed to delete old dfsUsed file in  <*> 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Failed to delete old dfsUsed file in  <*> 
DEBUG org.apache.hadoop.hdfs.DFSOutputStream: DFSClient flush() : bytesCurBlock  <*>  lastFlushOffset  <*> 
DEBUG org.apache.hadoop.hdfs.DFSOutputStream: Queued packet  <*> 
DEBUG org.apache.hadoop.hdfs.DFSOutputStream: Waiting for ack for:  seqno 
ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: -r, --rack arguments are not supported anymore. RackID resolution is handled by the NameNode.
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.datanode.DataNode: File descriptor passing is disabled because  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is  <*> 
DEBUG org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil: DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for {}
WARN org.apache.hadoop.hdfs.DFSUtil: dfs.https.enable is deprecated. Please use dfs.http.policy.
DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer: Connecting to datanode  <*> 
WARN org.apache.hadoop.hdfs.DFSInputStream: <*> <*> . Throwing a BlockMissingException 
DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer: Connecting to datanode  <*> 
WARN org.apache.hadoop.hdfs.DFSInputStream: <*> <*> . Throwing a BlockMissingException 
TRACE org.apache.hadoop.hdfs.BlockReaderFactory: this : trying to construct BlockReaderLocalLegacy 
TRACE org.apache.hadoop.hdfs.DFSClient: Address  targetAddr <*> 
TRACE org.apache.hadoop.hdfs.DFSClient: Address  targetAddr <*> 
DEBUG org.apache.hadoop.hdfs.BlockReaderFactory: this : not trying to create a  remote block reader because the UNIX domain socket at  <*>  is not usable. 
TRACE org.apache.hadoop.hdfs.DFSClient: Address  targetAddr <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : loading  key 
WARN org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : failed to load  key 
TRACE org.apache.hadoop.hdfs.BlockReaderFactory: this : got InvalidToken exception while trying to  construct BlockReaderLocal via  <*> 
TRACE org.apache.hadoop.hdfs.DFSClient: Address  targetAddr <*> 
DEBUG org.apache.hadoop.hdfs.BlockReaderLocalLegacy: Cached location of block  blk  as  <*> 
WARN org.apache.hadoop.hdfs.BlockReaderLocalLegacy: BlockReaderLocalLegacy: Removing  blk  from cache because local file  <*>  could not be opened. 
INFO org.apache.hadoop.hdfs.DFSInputStream: Access token was invalid when connecting to  targetAddr  :  ex 
INFO org.apache.hadoop.hdfs.DFSInputStream: Access token was invalid when connecting to  targetAddr  :  ex 
INFO org.apache.hadoop.hdfs.DFSInputStream: Access token was invalid when connecting to  targetAddr  :  ex 
WARN org.apache.hadoop.hdfs.DFSInputStream: Failed to connect to  <*>  for block , add to deadNodes and continue.  <*> 
WARN org.apache.hadoop.hdfs.DFSInputStream: Found Checksum error for  <*>  from  <*>  at  <*> 
INFO org.apache.hadoop.hdfs.DFSInputStream: Access token was invalid when connecting to  targetAddr  :  ex 
INFO org.apache.hadoop.hdfs.DFSInputStream: Access token was invalid when connecting to  targetAddr  :  ex 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager$DeletionStoragePurger: Purging old image  image 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager$DeletionStoragePurger: Could not delete  file 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager$DeletionStoragePurger: Could not delete  file 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: Resolved path is  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker: ACCESS CHECK:  this , doCheckOwner= doCheckOwner , ancestorAccess= ancestorAccess , parentAccess= parentAccess , access= access , subAccess= subAccess , ignoreEmptyDir= ignoreEmptyDir , resolveLink= resolveLink 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : unref replica  replica :  purgeReason  refCount  <*>  ->  <*> <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica: closed  this suffix 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : unref replica  replica :  purgeReason  refCount  <*>  ->  <*> <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: demoteOldEvictable: demoting  <*> :  <*> :  <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : unref replica  replica :  purgeReason  refCount  <*>  ->  <*> <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: demoteOldEvictable: demoting  <*> :  <*> :  <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : unref replica  replica :  purgeReason  refCount  <*>  ->  <*> <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: demoteOldEvictable: demoting  <*> :  <*> :  <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : unref replica  replica :  purgeReason  refCount  <*>  ->  <*> <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: Resolved path is  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker: ACCESS CHECK:  this , doCheckOwner= doCheckOwner , ancestorAccess= ancestorAccess , parentAccess= parentAccess , access= access , subAccess= subAccess , ignoreEmptyDir= ignoreEmptyDir , resolveLink= resolveLink 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Waiting until the NameNode rolls its edit logs in order to freeze the BackupNode namespace.
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: BackupNode namespace frozen.
DEBUG org.apache.hadoop.hdfs.server.namenode.Checkpointer: Doing checkpoint. Last applied:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Unable to roll forward using only logs. Downloading image with txid  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to  <*> 
DEBUG org.apache.hadoop.hdfs.web.URLConnectionFactory: open URL connection
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to  <*>  milliseconds 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file  <*>  size  <*>  bytes. 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to  <*> 
DEBUG org.apache.hadoop.hdfs.web.URLConnectionFactory: open URL connection
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to  <*>  milliseconds 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file  <*>  size  <*>  bytes. 
DEBUG org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Renaming  <*>  to  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Unable to rename edits file from  <*>  to  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Skipping download of remote edit log  log  since it already is stored locally at  f 
DEBUG org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Dest file:  f 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to  <*> 
DEBUG org.apache.hadoop.hdfs.web.URLConnectionFactory: open AuthenticatedURL connection url 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to  <*>  milliseconds 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file  <*>  size  <*>  bytes. 
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Loading image with txid  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: Reloading namespace from  file 
CLOSEALLSTREAMS org.apache.hadoop.hdfs.server.namenode.BackupImage:  <*> <*>
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from  <*>  stream(s). 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: About to load edits:\n   <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading  editIn  expecting start txid # <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: BUG: Diskspace quota violation in image for  <*>  quota =  <*>  < consumed =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with  <*>  entries  <*>  lookups 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain  <*>  images with txid >=  <*> 
DEBUG org.apache.hadoop.hdfs.web.URLConnectionFactory: open AuthenticatedURL connection url 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to  <*>  milliseconds 
WARN org.apache.hadoop.hdfs.server.namenode.TransferFsImage: SIMULATING A CORRUPT BYTE IN IMAGE TRANSFER!
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid  txid  to namenode at  fsName  in  <*>  seconds 
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpoint completed in  <*>  seconds.  New Image Size:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Waiting until the NameNode rolls its edit logs in order to freeze the BackupNode namespace.
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: BackupNode namespace frozen.
DEBUG org.apache.hadoop.hdfs.server.namenode.Checkpointer: Doing checkpoint. Last applied:  <*> 
CLOSEALLSTREAMS org.apache.hadoop.hdfs.server.namenode.BackupImage:  <*> <*>
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from  <*>  stream(s). 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: About to load edits:\n   <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: BUG: Diskspace quota violation in image for  <*>  quota =  <*>  < consumed =  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: BUG: Diskspace quota violation in image for  <*>  quota =  <*>  < consumed =  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: BUG: Diskspace quota violation in image for  <*>  quota =  <*>  < consumed =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with  <*>  entries  <*>  lookups 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain  <*>  images with txid >=  <*> 
DEBUG org.apache.hadoop.hdfs.web.URLConnectionFactory: open URL connection
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to  <*>  milliseconds 
WARN org.apache.hadoop.hdfs.server.namenode.TransferFsImage: SIMULATING A CORRUPT BYTE IN IMAGE TRANSFER!
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid  txid  to namenode at  fsName  in  <*>  seconds 
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpoint completed in  <*>  seconds.  New Image Size:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Waiting until the NameNode rolls its edit logs in order to freeze the BackupNode namespace.
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: BackupNode namespace frozen.
DEBUG org.apache.hadoop.hdfs.server.namenode.Checkpointer: Doing checkpoint. Last applied:  <*> 
CLOSEALLSTREAMS org.apache.hadoop.hdfs.server.namenode.BackupImage:  <*> <*>
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from  <*>  stream(s). 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: About to load edits:\n   <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading  editIn  expecting start txid # <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with  <*>  entries  <*>  lookups 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid  txid  to namenode at  fsName  in  <*>  seconds 
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Loading edits into backupnode to try to catch up from txid  <*>  to  <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.JournalSet: Disabling journal  j 
ERROR org.apache.hadoop.hdfs.server.namenode.JournalSet: Error:  <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.JournalSet: Disabling journal  j 
INFO org.apache.hadoop.hdfs.server.namenode.JournalSet: Skipping jas  jas  since it\'s disabled 
INFO org.apache.hadoop.hdfs.server.namenode.JournalSet: Skipping jas  jas  since it\'s disabled 
INFO org.apache.hadoop.hdfs.server.namenode.JournalSet: Skipping jas  jas  since it\'s disabled 
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Going to finish converging with remaining  editStreamsAll  txns from in-progress stream  stream 
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Going to finish converging with remaining  editStreamsAll  txns from in-progress stream  stream 
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Going to finish converging with remaining  editStreamsAll  txns from in-progress stream  stream 
CLOSEALLSTREAMS org.apache.hadoop.hdfs.server.namenode.BackupImage:  <*> <*>
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Successfully synced BackupNode with NameNode at txnid  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.BackupImage: State transition  <*>  ->  newState 
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpoint completed in  <*>  seconds.  New Image Size:  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.JournalNode: Initializing journal in directory  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: <*>  does not exist. Creating ... 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: <*> is not a directory 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Cannot access storage directory  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Recovering storage directory  <*>  from failed checkpoint 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Scanning storage  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: No files in  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer: Invalid clusterId in journal request - expected  <*>  actual  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: NameNode started a new log segment at txid  txid 
INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at  segmentTxId 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: NNStorage.attemptRestoreRemovedStorage: check removed(failed) storarge. removedStorages size =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: currently disabled dir  <*> ; type= <*> ;canwrite= <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: restoring dir  <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.JournalSet: Disabling journal  j 
DEBUG org.apache.hadoop.hdfs.server.namenode.BackupImage: State transition  <*>  ->  newState 
WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: srcNode  srcNode  is dead  when decommission is in progress. Continue to mark  it as decommission in progress. In that way, when it rejoins the  cluster it can continue the decommission process. 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: Decommission complete for  node 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Block:  block , Expected Replicas:  <*> , live replicas:  <*> , corrupt replicas:  <*> , decommissioned replicas:  <*> , excess replicas:  <*> , Is Open File:  <*> , Datanodes having this block:  <*> , Current Datanode:  srcNode , Is current datanode decommissioning:  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Block:  block , Expected Replicas:  <*> , live replicas:  <*> , corrupt replicas:  <*> , decommissioned replicas:  <*> , excess replicas:  <*> , Is Open File:  <*> , Datanodes having this block:  <*> , Current Datanode:  srcNode , Is current datanode decommissioning:  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Block:  block , Expected Replicas:  <*> , live replicas:  <*> , corrupt replicas:  <*> , decommissioned replicas:  <*> , excess replicas:  <*> , Is Open File:  <*> , Datanodes having this block:  <*> , Current Datanode:  srcNode , Is current datanode decommissioning:  <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.add: block  has only  curReplicas  replicas and need  expectedReplicas  replicas so is added to neededReplications  at priority level  <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.add: block  has only  curReplicas  replicas and need  expectedReplicas  replicas so is added to neededReplications  at priority level  <*> 
WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: srcNode  srcNode  is dead  when decommission is in progress. Continue to mark  it as decommission in progress. In that way, when it rejoins the  cluster it can continue the decommission process. 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: Decommission complete for  node 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for standby state
WARN org.apache.hadoop.hdfs.server.namenode.FSEditLog: Initializing shared journals for READ, already open for READ
DEBUG org.apache.hadoop.hdfs.server.namenode.NameNode: Setting fs.defaultFS to  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Will roll logs on active node at  <*>  every  <*>  seconds. 
DEBUG org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: logRollPeriodMs= <*>  sleepTime= <*> 
WARN org.apache.hadoop.hdfs.server.namenode.CheckpointConf: Configuration key  key  is deprecated! Ignoring...  Instead please specify a value for  dfs.namenode.checkpoint.txns 
WARN org.apache.hadoop.hdfs.DFSUtil: hadoop.ssl.enabled is deprecated. Please use dfs.http.policy.
WARN org.apache.hadoop.hdfs.DFSUtil: dfs.https.enable is deprecated. Please use dfs.http.policy.
DEBUG org.apache.hadoop.hdfs.server.namenode.NameNode: Setting fs.defaultFS to  <*> 
WARN org.apache.hadoop.hdfs.DFSUtil: hadoop.ssl.enabled is deprecated. Please use dfs.http.policy.
INFO org.apache.hadoop.hdfs.server.namenode.ha.StandbyCheckpointer: Starting standby checkpoint thread...\nCheckpointing active NN at  <*> \n Serving checkpoints at  <*> 
DEBUG org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove: Start moving  this 
DEBUG org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient: SASL client skipping handshake on trusted connection for addr = {}, datanodeId = {}
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.Sender: Sending DataTransferOp  <*> :  proto 
INFO org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove: Successfully moved  this 
DEBUG org.apache.hadoop.hdfs.server.datanode.DataXceiver: Checking block access token for block \' <*> \' with mode \' mode \' 
DEBUG org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager: Checking access for user= userId , block= block , access mode= mode  using  <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.DataXceiver: opWriteBlock: stage= stage , clientname= clientname \n  block  = block , newGs= latestGenerationStamp , bytesRcvd=[ minBytesRcvd ,  maxBytesRcvd ] \n  targets= <*> ; pipelineSize= pipelineSize , srcDataNode= srcDataNode 
DEBUG org.apache.hadoop.hdfs.server.datanode.DataXceiver: isDatanode= isDatanode , isClient= isClient , isTransfer= isTransfer 
DEBUG org.apache.hadoop.hdfs.server.datanode.DataXceiver: writeBlock receive buf size  <*>  tcp no delay  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.DataXceiver: Receiving  block  src:  <*>  dest:  <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.BlockReceiver: <*> :  block \n  isClient  = <*> , clientname= clientname \n  isDatanode= <*> , srcDataNode= srcDataNode \n  inAddr= inAddr , myAddr= myAddr \n  cachingStrategy =  cachingStrategy 
WARN org.apache.hadoop.hdfs.server.datanode.BlockReceiver: Could not get file descriptor for outputstream of class  <*> 
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver: readNextPacket: dataPlusChecksumLen =  dataPlusChecksumLen  headerLen =  $i 
DEBUG org.apache.hadoop.hdfs.server.datanode.BlockReceiver: Receiving one packet for block  <*> :  <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder: <*> : enqueue  <*> 
WARN org.apache.hadoop.hdfs.server.datanode.BlockReceiver: Slow BlockReceiver write packet to mirror took  duration_ ms (threshold= <*> ms) 
WARN org.apache.hadoop.hdfs.server.datanode.BlockReceiver: Slow BlockReceiver write data to disk cost: <*> ms (threshold= <*> ms) 
WARN org.apache.hadoop.hdfs.server.datanode.BlockReceiver: Slow flushOrSync took  <*> ms (threshold= <*> ms), isSync: isSync , flushTotalNanos= flushTotalNanos ns 
WARN org.apache.hadoop.hdfs.server.datanode.BlockReceiver: Slow manageWriterOsCache took  <*> ms (threshold= <*> ms) 
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver: readNextPacket: dataPlusChecksumLen =  dataPlusChecksumLen  headerLen =  $i 
DEBUG org.apache.hadoop.hdfs.server.datanode.BlockReceiver: Receiving one packet for block  <*> :  <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder: <*> : enqueue  <*> 
WARN org.apache.hadoop.hdfs.server.datanode.BlockReceiver: Slow BlockReceiver write packet to mirror took  duration_ ms (threshold= <*> ms) 
DEBUG org.apache.hadoop.hdfs.server.datanode.BlockReceiver: receivePacket for  <*> : previous write did not end at the chunk boundary.  onDiskLen= <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.BlockReceiver: computePartialChunkCrc for  <*> : sizePartialChunk= sizePartialChunk , block offset= blkoff# , metafile offset= ckoff 
WARN org.apache.hadoop.hdfs.server.datanode.BlockReceiver: Slow BlockReceiver write data to disk cost: <*> ms (threshold= <*> ms) 
DEBUG org.apache.hadoop.hdfs.server.datanode.BlockReceiver: Writing out partial crc for data len  <*> , skip=  
WARN org.apache.hadoop.hdfs.server.datanode.BlockReceiver: Slow flushOrSync took  <*> ms (threshold= <*> ms), isSync: isSync , flushTotalNanos= flushTotalNanos ns 
WARN org.apache.hadoop.hdfs.server.datanode.BlockReceiver: Slow manageWriterOsCache took  <*> ms (threshold= <*> ms) 
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver: readNextPacket: dataPlusChecksumLen =  dataPlusChecksumLen  headerLen =  $i 
DEBUG org.apache.hadoop.hdfs.server.datanode.BlockReceiver: Receiving one packet for block  <*> :  <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder: <*> : enqueue  <*> 
WARN org.apache.hadoop.hdfs.server.datanode.BlockReceiver: Slow BlockReceiver write packet to mirror took  duration_ ms (threshold= <*> ms) 
DEBUG org.apache.hadoop.hdfs.server.datanode.BlockReceiver: computePartialChunkCrc for  <*> : sizePartialChunk= sizePartialChunk , block offset= blkoff# , metafile offset= ckoff 
DEBUG org.apache.hadoop.hdfs.server.datanode.BlockReceiver: Read in partial CRC chunk from disk for  <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.BlockReceiver: Writing out partial crc for data len  <*> , skip=  
WARN org.apache.hadoop.hdfs.server.datanode.BlockReceiver: Slow flushOrSync took  <*> ms (threshold= <*> ms), isSync: isSync , flushTotalNanos= flushTotalNanos ns 
WARN org.apache.hadoop.hdfs.server.datanode.BlockReceiver: Slow manageWriterOsCache took  <*> ms (threshold= <*> ms) 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Failed to delete restart meta file:  <*> 
WARN org.apache.hadoop.hdfs.server.datanode.BlockReceiver: <*> \n <*> 
TRACE org.apache.hadoop.hdfs.server.datanode.DataXceiver: TRANSFER: send close-ack
INFO org.apache.hadoop.hdfs.qjournal.server.JournalNode: Initializing journal in directory  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: <*>  does not exist. Creating ... 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Locking is disabled for  <*> 
ERROR org.apache.hadoop.hdfs.server.common.Storage: *********** Upgrade is not supported from this  older version  oldVersion  of storage to the current version.  Please upgrade to  Hadoop-.  or a later version and then upgrade to current  version. Old layout version is  <*>  and latest layout version this software version can  upgrade from is  $i . ************ 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Recovering storage directory  <*>  from previous upgrade 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Scanning storage  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: No files in  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Updating lastPromisedEpoch from  <*>  to  newEpoch  for client  <*> 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
DEBUG org.apache.hadoop.hdfs.server.namenode.NameNode: Setting fs.defaultFS to  <*> 
DEBUG org.apache.hadoop.hdfs.DFSInotifyEventInputStream: poll(): lastReadTxid is -, reading current txid from NN
DEBUG org.apache.hadoop.hdfs.DFSInotifyEventInputStream: poll(): lastReadTxid is -, reading current txid from NN
DEBUG org.apache.hadoop.hdfs.DFSInotifyEventInputStream: poll(): lastReadTxid is -, reading current txid from NN
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* fsync:  src  for  clientName 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: Resolved path is  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSNamesystem: persistBlocks:  path  with  <*>  blocks is persisted to  the file system 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList$1: Adding replicas to map for block pool  <*>  on volume  <*> ... 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil: Block  blockFile  does not have a metafile! 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil: Block  blockFile  does not have a metafile! 
DEBUG org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: resolveDuplicateReplicas decide to keep  replicaToKeep .  Will try to delete  <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: resolveDuplicateReplicas decide to keep  replicaToKeep .  Will try to delete  <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: resolveDuplicateReplicas decide to keep  replicaToKeep .  Will try to delete  <*> 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil: Block  blockFile  does not have a metafile! 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil: Block  blockFile  does not have a metafile! 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil: Block  blockFile  does not have a metafile! 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Failed to delete restart meta file:  <*> 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Failed to delete restart meta file:  <*> 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Failed to delete restart meta file:  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList$1: Time to add replicas to map for block pool  <*>  on volume  <*> :  timeTaken ms 
WARN org.apache.hadoop.hdfs.DFSUtil: hadoop.ssl.enabled is deprecated. Please use dfs.http.policy.
INFO org.apache.hadoop.hdfs.DFSUtil: Starting web server as:  <*> 
WARN org.apache.hadoop.hdfs.DFSUtil: hadoop.ssl.enabled is deprecated. Please use dfs.http.policy.
INFO org.apache.hadoop.hdfs.server.namenode.ha.StandbyCheckpointer$CheckpointerThread: Triggering a rollback fsimage for rolling upgrade.
INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Save namespace ...
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: NNStorage.attemptRestoreRemovedStorage: check removed(failed) storarge. removedStorages size =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: currently disabled dir  <*> ; type= <*> ;canwrite= <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: restoring dir  <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Save namespace ...
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: NNStorage.attemptRestoreRemovedStorage: check removed(failed) storarge. removedStorages size =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: currently disabled dir  <*> ; type= <*> ;canwrite= <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at  segmentTxId 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: NNStorage.attemptRestoreRemovedStorage: check removed(failed) storarge. removedStorages size =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: currently disabled dir  <*> ; type= <*> ;canwrite= <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: restoring dir  <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.JournalSet: Disabling journal  j 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Saver: Saving image file  newFile  using  compression 
WARN org.apache.hadoop.hdfs.server.namenode.LeaseManager: Ignore the lease of file  p  for checkpoint since the file is not under construction 
INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Saver: Image file  newFile  of size  <*>  bytes saved in  <*>  seconds. 
INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage: Deleting  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Failed to delete image file:  $u 
INFO org.apache.hadoop.hdfs.qjournal.server.JournalNode: Initializing journal in directory  <*> 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: <*> is not a directory 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Cannot access storage directory  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Completing previous checkpoint for storage directory  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Scanning storage  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Latest log is  latestLog 
INFO org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil: Rollback of  <*>  is complete. 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* fsync:  src  for  clientName 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
DEBUG org.apache.hadoop.hdfs.server.namenode.FSNamesystem: persistBlocks:  path  with  <*>  blocks is persisted to  the file system 
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Waiting until the NameNode rolls its edit logs in order to freeze the BackupNode namespace.
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: BackupNode namespace frozen.
DEBUG org.apache.hadoop.hdfs.server.namenode.Checkpointer: Doing checkpoint. Last applied:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to  <*> 
DEBUG org.apache.hadoop.hdfs.web.URLConnectionFactory: open AuthenticatedURL connection url 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to  <*>  milliseconds 
WARN org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Overwriting existing file  f  with file downloaded from  url 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file  <*>  size  <*>  bytes. 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to  <*> 
DEBUG org.apache.hadoop.hdfs.web.URLConnectionFactory: open AuthenticatedURL connection url 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to  <*>  milliseconds 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file  <*>  size  <*>  bytes. 
DEBUG org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Renaming  <*>  to  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to  <*> 
DEBUG org.apache.hadoop.hdfs.web.URLConnectionFactory: open URL connection
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to  <*>  milliseconds 
WARN org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Overwriting existing file  f  with file downloaded from  url 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file  <*>  size  <*>  bytes. 
DEBUG org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Renaming  <*>  to  <*> 
CLOSEALLSTREAMS org.apache.hadoop.hdfs.server.namenode.BackupImage:  <*> <*>
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from  <*>  stream(s). 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: About to load edits:\n   <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading  editIn  expecting start txid # <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: BUG: Namespace quota violation in image for  <*>  quota =  <*>  < consumed =  child# 
INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with  <*>  entries  <*>  lookups 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: msg  \n <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
DEBUG org.apache.hadoop.hdfs.web.URLConnectionFactory: open AuthenticatedURL connection url 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to  <*>  milliseconds 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Connection closed by client
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid  txid  to namenode at  fsName  in  <*>  seconds 
INFO org.apache.hadoop.hdfs.server.namenode.JournalSet: Skipping jas  jas  since it\'s disabled 
INFO org.apache.hadoop.hdfs.server.namenode.JournalSet: Skipping jas  jas  since it\'s disabled 
INFO org.apache.hadoop.hdfs.server.namenode.JournalSet: Skipping jas  jas  since it\'s disabled 
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Going to finish converging with remaining  editStreamsAll  txns from in-progress stream  stream 
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Loading edits into backupnode to try to catch up from txid  <*>  to  <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.JournalSet: Disabling journal  j 
ERROR org.apache.hadoop.hdfs.server.namenode.JournalSet: Disabling journal  j 
ERROR org.apache.hadoop.hdfs.server.namenode.JournalSet: Error:  <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.JournalSet: Disabling journal  j 
ERROR org.apache.hadoop.hdfs.server.namenode.JournalSet: Error:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: About to load edits:\n   <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: BUG: Namespace quota violation in image for  <*>  quota =  <*>  < consumed =  child# 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: About to load edits:\n   <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading  editIn  expecting start txid # <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: BUG: Namespace quota violation in image for  <*>  quota =  <*>  < consumed =  child# 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: BUG: Diskspace quota violation in image for  <*>  quota =  <*>  < consumed =  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: About to load edits:\n   <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading  editIn  expecting start txid # <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: BUG: Namespace quota violation in image for  <*>  quota =  <*>  < consumed =  child# 
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Going to finish converging with remaining  editStreamsAll  txns from in-progress stream  stream 
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Going to finish converging with remaining  editStreamsAll  txns from in-progress stream  stream 
CLOSEALLSTREAMS org.apache.hadoop.hdfs.server.namenode.BackupImage:  <*> <*>
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Successfully synced BackupNode with NameNode at txnid  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.BackupImage: State transition  <*>  ->  newState 
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpoint completed in  <*>  seconds.  New Image Size:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Waiting until the NameNode rolls its edit logs in order to freeze the BackupNode namespace.
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: BackupNode namespace frozen.
DEBUG org.apache.hadoop.hdfs.server.namenode.Checkpointer: Doing checkpoint. Last applied:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Unable to roll forward using only logs. Downloading image with txid  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file  <*>  size  <*>  bytes. 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: renaming   <*>  to  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: renaming   <*>  to  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Loading image with txid  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: Reloading namespace from  file 
INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode$Loader: Loading  <*>  INodes. 
INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode$Loader: Loading  <*>  INodes. 
INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Loader: Loaded FSImage in  <*>  seconds. 
INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid  <*>  from  curFile 
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from  <*>  stream(s). 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: About to load edits:\n   <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading  editIn  expecting start txid # <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with  <*>  entries  <*>  lookups 
WARN org.apache.hadoop.hdfs.server.namenode.FSImage: Unable to delete cancelled checkpoint in  sd 
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpoint completed in  <*>  seconds.  New Image Size:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Waiting until the NameNode rolls its edit logs in order to freeze the BackupNode namespace.
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: BackupNode namespace frozen.
DEBUG org.apache.hadoop.hdfs.server.namenode.Checkpointer: Doing checkpoint. Last applied:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Loading image with txid  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: Reloading namespace from  file 
CLOSEALLSTREAMS org.apache.hadoop.hdfs.server.namenode.BackupImage:  <*> <*>
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from  <*>  stream(s). 
INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with  <*>  entries  <*>  lookups 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: STATE* Leaving safe mode after  <*>  secs 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: STATE* Network topology has  <*>  racks and  <*>  datanodes 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: STATE* UnderReplicatedBlocks has  <*>  blocks 
WARN org.apache.hadoop.hdfs.server.namenode.FSImage: Unable to delete cancelled checkpoint in  sd 
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpoint completed in  <*>  seconds.  New Image Size:  <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: getDatanodeListForReport with includedNodes =  <*> , excludedNodes =  <*> , foundNodes =  <*> , nodes =  <*> 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from  <*>  to  volFailures 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: storageInfo  failed. 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from  <*>  to  volFailures 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: storageInfo  failed. 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of storages reported in heartbeat= <*> ; Number of storages in storageMap= <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: getDatanodeListForReport with includedNodes =  <*> , excludedNodes =  <*> , foundNodes =  <*> , nodes =  <*> 
WARN org.apache.hadoop.hdfs.DFSUtil: hadoop.ssl.enabled is deprecated. Please use dfs.http.policy.
WARN org.apache.hadoop.hdfs.DFSUtil: dfs.https.enable is deprecated. Please use dfs.http.policy.
INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for  name  at:  <*> 
INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for  name  at:  <*> 
WARN org.apache.hadoop.hdfs.DFSUtil: dfs.https.enable is deprecated. Please use dfs.http.policy.
DEBUG org.apache.hadoop.hdfs.server.common.Storage: Failed to preserve last modified date from\' srcFile \' to \' destFile \' 
DEBUG org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Copied  srcMeta  to  <*>  and calculated checksum 
DEBUG org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Copied  srcFile  to  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: Registration IDs mismatched: the  <*>  ID is  <*>  but the expected ID is  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: *BLOCK* NameNode.blockReport: from  nodeReg , reports.length= <*> 
FATAL org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: BLOCK* NameSystem.getDatanode:  <*> 
WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Unexpected replica state  reportedState  for block:  storedBlock  on  dn  size  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Received an RBW replica for  storedBlock  on  dn : ignoring it, since it is  complete with the same genstamp 
WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Unexpected replica state  reportedState  for block:  storedBlock  on  dn  size  <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  priLevel 
TRACE org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* processOverReplicatedBlock: Postponing  block  since storage  storage  does not yet have up-to-date information. 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  i 
WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Inconsistent number of corrupt replicas for  storedBlock blockMap has  <*>  but corrupt replicas map has  <*> 
WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* addStoredBlock: Redundant addStoredBlock request received for  storedBlock  on  <*>  size  <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  i 
WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Inconsistent number of corrupt replicas for  storedBlock blockMap has  <*>  but corrupt replicas map has  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* invalidateBlock:  b  on  dn 
FATAL org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: BLOCK* NameSystem.getDatanode:  <*> 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: BLOCK*  <*> : add  block  to  datanode 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* removeStoredBlock:  block  is removed from excessBlocks 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* invalidateBlock:  b  on  dn 
FATAL org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: BLOCK* NameSystem.getDatanode:  <*> 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: BLOCK*  <*> : add  block  to  datanode 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* removeStoredBlock:  block  from  node 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: UnderReplicationBlocks.update  block  curReplicas  curReplicas  curExpectedReplicas  curExpectedReplicas  oldReplicas  oldReplicas  oldExpectedReplicas   oldExpectedReplicas  curPri   <*>  oldPri   <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  priLevel 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.update: block  has only  curReplicas  replicas and needs  curExpectedReplicas  replicas so is added to neededReplications  at priority level  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* processReport: Received first block report from  storage  after starting up or becoming active. Its block  contents are no longer considered stale 
TRACE org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* processOverReplicatedBlock: Postponing  block  since storage  storage  does not yet have up-to-date information. 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.add: block  has only  curReplicas  replicas and need  expectedReplicas  replicas so is added to neededReplications  at priority level  <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* rescanPostponedMisreplicatedBlocks: Re-scanned block  b , result is  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: <*>  had lastBlockReportId x <*> , but curBlockReportId = x <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* processReport: from storage  <*>  node  nodeID , blocks:  <*> , hasStaleStorages:  <*> , processing time:  <*>  msecs 
FATAL org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: BLOCK* NameSystem.getDatanode:  <*> 
WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Unexpected replica state  reportedState  for block:  storedBlock  on  dn  size  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Received an RBW replica for  storedBlock  on  dn : ignoring it, since it is  complete with the same genstamp 
WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Unexpected replica state  reportedState  for block:  storedBlock  on  dn  size  <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Queueing reported block  block  in state  reportedState  from datanode  <*>  for later processing because  reason . 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Queueing reported block  block  in state  reportedState  from datanode  <*>  for later processing because  reason . 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Queueing reported block  block  in state  reportedState  from datanode  <*>  for later processing because  reason . 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* processReport: Received first block report from  storage  after starting up or becoming active. Its block  contents are no longer considered stale 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: <*>  had lastBlockReportId x <*> , but curBlockReportId = x <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* processReport:  staleBefore  on  <*>  size  <*>  does not belong to any file 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* processReport: from storage  <*>  node  nodeID , blocks:  <*> , hasStaleStorages:  <*> , processing time:  <*>  msecs 
FATAL org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: BLOCK* NameSystem.getDatanode:  <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Queueing reported block  block  in state  reportedState  from datanode  <*>  for later processing because  reason . 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Reported block  block  on  <*>  size  <*>  replicaState =  reportedState 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: In memory blockUCState =  <*> 
WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* addStoredBlock: Redundant addStoredBlock request received for  storedBlock  on  <*>  size  <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  i 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* processReport: Received first block report from  storage  after starting up or becoming active. Its block  contents are no longer considered stale 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* rescanPostponedMisreplicatedBlocks: Re-scanned block  b , result is  <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: processReport x <*> :  <*>  more RPCs remaining in this report. 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* processReport:  staleBefore  on  <*>  size  <*>  does not belong to any file 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* processReport: from storage  <*>  node  nodeID , blocks:  <*> , hasStaleStorages:  <*> , processing time:  <*>  msecs 
INFO org.apache.hadoop.hdfs.server.datanode.BPServiceActor: this  starting to offer service 
INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Generated and persisted new Datanode UUID  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Periodic Block Verification scan disabled because  reason#_ 
INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Periodic Directory Tree Verification scan is disabled because  reason#_ 
INFO org.apache.hadoop.hdfs.server.datanode.BPServiceActor: this  beginning handshake with NN 
INFO org.apache.hadoop.hdfs.server.datanode.BPServiceActor: Block pool  this  successfully registered with NN 
INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block token params received from NN: for block pool  blockPoolId  keyUpdateInterval= <*>  min(s), tokenLifetime= <*>  min(s) 
WARN org.apache.hadoop.hdfs.server.datanode.BPServiceActor: Ending block pool service for:  this 
INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolManager: Removed  bpos 
INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Removed bpid= blockPoolId  from blockPoolScannerMap 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica: this : could not add no-checksum anchor to slot  <*> 
TRACE org.apache.hadoop.hdfs.BlockReaderLocal: <*> : starting 
TRACE org.apache.hadoop.hdfs.BlockReaderLocal: loaded  <*>  bytes into bounce  buffer from offset  oldDataPos  of  <*> 
TRACE org.apache.hadoop.hdfs.BlockReaderLocal: loaded  <*>  bytes into bounce  buffer from offset  oldDataPos  of  <*> 
DEBUG org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$CacheCleaner: this : cache cleaner running at  <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: demoteOldEvictable: demoting  <*> :  <*> :  <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$CacheCleaner: CacheCleaner: purging  replica :  <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica: this  is stale because it\'s  stale#  ms old, and staleThresholdMs =  <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: demoteOldEvictable: demoting  <*> :  <*> :  <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : trimEvictionMaps is purging  replica <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica: this  is not stale because it\'s only  stale#  ms old, and staleThresholdMs =  <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : unref replica  replica :  purgeReason  refCount  <*>  ->  <*> <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: demoteOldEvictable: demoting  <*> :  <*> :  <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : trimEvictionMaps is purging  replica <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica: this  is stale because it\'s  stale#  ms old, and staleThresholdMs =  <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: demoteOldEvictable: demoting  <*> :  <*> :  <*> 
DEBUG org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$CacheCleaner: this : finishing cache cleaner run started at  <*> .  Demoted  <*>  mmapped replicas;  purged  numPurged  replicas. 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
DEBUG org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker: ACCESS CHECK:  this , doCheckOwner= doCheckOwner , ancestorAccess= ancestorAccess , parentAccess= parentAccess , access= access , subAccess= subAccess , ignoreEmptyDir= ignoreEmptyDir , resolveLink= resolveLink 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: Resolved path is  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker: ACCESS CHECK:  this , doCheckOwner= doCheckOwner , ancestorAccess= ancestorAccess , parentAccess= parentAccess , access= access , subAccess= subAccess , ignoreEmptyDir= ignoreEmptyDir , resolveLink= resolveLink 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Removing block pool  bpid 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Failed to delete old dfsUsed file in  <*> 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Failed to delete old dfsUsed file in  <*> 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Failed to delete old dfsUsed file in  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.JournalNode: Initializing journal in directory  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: <*>  does not exist. Creating ... 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Lock on  <*>  acquired by nodename  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Cannot lock storage  <*> . The directory is already locked 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Completing previous upgrade for storage directory  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Scanning storage  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: No files in  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Discard the EditLog files, the given start txid is  startTxId 
INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Trash the EditLog file  elf 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from  <*>  to  volFailures 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: storageInfo  failed. 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Removed storage  len$  from DataNode this 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from  <*>  to  volFailures 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: storageInfo  failed. 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Deferring removal of stale storage  len$  with  <*>  blocks 
INFO org.apache.hadoop.hdfs.qjournal.server.JournalNode: Initializing journal in directory  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Lock on  <*>  acquired by nodename  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Cannot lock storage  <*> . The directory is already locked 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Scanning storage  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: No files in  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NameNode: Setting fs.defaultFS to  <*> 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Edits URI  dir  listed multiple times in  dfs.namenode.shared.edits.dir . Ignoring duplicates. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Edits URI  i$  listed multiple times in  dfs.namenode.shared.edits.dir  and  dfs.namenode.edits.dir . Ignoring duplicates. 
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: !!! WARNING !!!\n\tThe NameNode currently runs without persistent storage.\n\tAny changes to the file system meta-data may be lost.\n\tRecommended actions:\n\t\t- shutdown and restart NameNode with configured \ propertyName \ in hdfs-site.xml; \n\t\t- use Backup Node as a persistent and up-to-date storage  of the file system meta-data. 
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: !!! WARNING !!!\n\tThe NameNode currently runs without persistent storage.\n\tAny changes to the file system meta-data may be lost.\n\tRecommended actions:\n\t\t- shutdown and restart NameNode with configured \ propertyName \ in hdfs-site.xml; \n\t\t- use Backup Node as a persistent and up-to-date storage  of the file system meta-data. 
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: <*> 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.HeartbeatManager: Setting heartbeat recheck interval to  <*>  since  dfs.namenode.stale.datanode.interval  is less than  dfs.namenode.heartbeat.recheck-interval 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit= <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check= <*> 
WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: The given interval for marking stale datanode =  staleInterval , which is larger than heartbeat expire interval  heartbeatExpireInterval . 
INFO org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: dfs.namenode.startup.delay.block.deletion.sec is set to  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: The block deletion will start around  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable= <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.key.update.interval= <*>  min(s),  dfs.block.access.token.lifetime = <*>  min(s),  dfs.encrypt.data.transfer.algorithm = <*> 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled =  <*> 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Determined nameservice ID:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Caching file names occuring more than  <*>  times 
INFO org.apache.hadoop.hdfs.server.namenode.CacheManager: Using minimum value {} for {}
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: dfs.namenode.safemode.threshold-pct =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: dfs.namenode.safemode.min.datanodes =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: dfs.namenode.safemode.extension     =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use  <*>  of total heap and retry cache entry expiry time is  <*>  millis 
INFO org.apache.hadoop.hdfs.server.namenode.NNConf: ACLs enabled?  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNConf: XAttrs enabled?  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNConf: Maximum size of an xattr:  <*> <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: STATE* Safe mode is ON <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Lock on  <*>  acquired by nodename  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: <*>  does not exist. Creating ... 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Lock on  <*>  acquired by nodename  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Recovering storage directory  <*>  from previous rollback 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Completing previous rollback for storage directory  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Completing previous checkpoint for storage directory  <*> 
ERROR org.apache.hadoop.hdfs.server.common.Storage: *********** Upgrade is not supported from this  older version  oldVersion  of storage to the current version.  Please upgrade to  Hadoop-.  or a later version and then upgrade to current  version. Old layout version is  <*>  and latest layout version this software version can  upgrade from is  $i . ************ 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: Using clusterid:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image?  <*>#_  (staleImage= <*> , haEnabled= <*> , isRollingUpgrade= <*> ) 
INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Save namespace ...
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: NNStorage.attemptRestoreRemovedStorage: check removed(failed) storarge. removedStorages size =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: currently disabled dir  <*> ; type= <*> ;canwrite= <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment  <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.JournalSet: Disabling journal  j 
ERROR org.apache.hadoop.hdfs.server.namenode.JournalSet: Error:  <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
DEBUG org.apache.hadoop.hdfs.util.MD5FileUtils: Saved MD  digestString  to  <*> 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
WARN org.apache.hadoop.hdfs.util.MD5FileUtils: deleting   <*>  FAILED 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain  <*>  images with txid >=  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at  segmentTxId 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: NNStorage.attemptRestoreRemovedStorage: check removed(failed) storarge. removedStorages size =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: currently disabled dir  <*> ; type= <*> ;canwrite= <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.JournalSet: Disabling journal  j 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.JournalSet: Skipping jas  jas  since it\'s disabled 
INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at  segmentTxId 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: NNStorage.attemptRestoreRemovedStorage: check removed(failed) storarge. removedStorages size =  <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.JournalSet: Disabling journal  j 
ERROR org.apache.hadoop.hdfs.server.namenode.JournalSet: Error:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with  <*>  entries  <*>  lookups 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in  ioe  msecs 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: Error converting file to URI
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: Error converting file to URI
ERROR org.apache.hadoop.hdfs.server.namenode.FSEditLog: No edits directories configured!
ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: Could not initialize shared edits dir
DEBUG org.apache.hadoop.hdfs.DFSClient: src : masked= <*> 
DEBUG org.apache.hadoop.hdfs.DFSOutputStream: Set non-null progress callback on DFSOutputStream  src 
DEBUG org.apache.hadoop.hdfs.DFSOutputStream: computePacketChunkSize: src= <*> , chunkSize= chunkSize , chunksPerPacket= <*> , packetSize= <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.JournalNode: Initializing journal in directory  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: <*>  does not exist. Creating ... 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Cannot access storage directory  <*> 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Cannot access storage directory  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Completing previous finalize for storage directory  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Scanning storage  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Latest log is  latestLog 
WARN org.apache.hadoop.hdfs.qjournal.server.Journal: Latest log  latestLog  has no transactions.  moving it aside and looking for previous log 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: No files in  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Updating lastPromisedEpoch from  <*>  to  newEpoch  for client  <*> 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Edit log file  <*>  appears to be empty.  Moving it aside... 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Synchronizing log  <*> : old segment  <*>  is not the right length 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSNamesystem: DIR* NameSystem.createSymlink: target= target  link= linkArg 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: Resolved path is  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSNamesystem: addSymlink:  path  is added 
DEBUG org.apache.hadoop.hdfs.server.namenode.NameNode: Setting fs.defaultFS to  <*> 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Found KeyProvider:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: <*> 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.HeartbeatManager: Setting heartbeat recheck interval to  <*>  since  dfs.namenode.stale.datanode.interval  is less than  dfs.namenode.heartbeat.recheck-interval 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit= <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check= <*> 
WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: The given interval for marking stale datanode =  staleInterval , which is larger than heartbeat expire interval  heartbeatExpireInterval . 
INFO org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: dfs.namenode.startup.delay.block.deletion.sec is set to  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: The block deletion will start around  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable= <*> 
ERROR org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Security is enabled but block access tokens (via dfs.block.access.token.enable) aren\'t enabled. This may cause issues when clients attempt to talk to a DataNode.
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         =  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             =  $i 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             =  $i 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      =  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval =  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        =  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled =  <*> 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled:  <*> 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Configured NNs:\n <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.FSEditLog: No edits directories configured!
INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Allocated new BlockPoolId:  <*> 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: Storage directory  <*>  has been successfully formatted. 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: Storage directory  <*>  has been successfully formatted. 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain  <*>  images with txid >=  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.JournalNode: Initializing journal in directory  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Locking is disabled for  <*> 
ERROR org.apache.hadoop.hdfs.server.common.Storage: *********** Upgrade is not supported from this  older version  oldVersion  of storage to the current version.  Please upgrade to  Hadoop-.  or a later version and then upgrade to current  version. Old layout version is  <*>  and latest layout version this software version can  upgrade from is  $i . ************ 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Completing previous upgrade for storage directory  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Scanning storage  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: No files in  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil: Rollback of  <*>  is complete. 
WARN org.apache.hadoop.hdfs.DFSInputStream: Found Checksum error for  <*>  from  <*>  at  <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer: Connecting to datanode  <*> 
WARN org.apache.hadoop.hdfs.DFSInputStream: <*> <*> . Throwing a BlockMissingException 
DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer: Connecting to datanode  <*> 
WARN org.apache.hadoop.hdfs.DFSInputStream: <*> <*> . Throwing a BlockMissingException 
DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer: Connecting to datanode  <*> 
WARN org.apache.hadoop.hdfs.DFSInputStream: <*> <*> . Throwing a BlockMissingException 
TRACE org.apache.hadoop.hdfs.BlockReaderFactory: this : trying to construct BlockReaderLocalLegacy 
TRACE org.apache.hadoop.hdfs.DFSClient: Address  targetAddr <*> 
TRACE org.apache.hadoop.hdfs.BlockReaderFactory: this : can\'t construct BlockReaderLocalLegacy because  the address  <*>  is not local 
DEBUG org.apache.hadoop.hdfs.BlockReaderFactory: this : not trying to create a  remote block reader because the UNIX domain socket at  <*>  is not usable. 
TRACE org.apache.hadoop.hdfs.DFSClient: Address  targetAddr <*> 
TRACE org.apache.hadoop.hdfs.BlockReaderFactory: this : trying to create a remote block reader from the  UNIX domain socket at  <*> 
TRACE org.apache.hadoop.hdfs.BlockReaderFactory: nextDomainPeer: reusing existing peer  <*> 
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.Sender: Sending DataTransferOp  <*> :  proto 
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.Sender: Sending DataTransferOp  <*> :  proto 
TRACE org.apache.hadoop.hdfs.DFSClient: Address  targetAddr <*> 
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.Sender: Sending DataTransferOp  <*> :  proto 
TRACE org.apache.hadoop.hdfs.DFSClient: Address  targetAddr <*> 
TRACE org.apache.hadoop.hdfs.BlockReaderFactory: this : got security exception while constructing  a remote block reader from the unix domain socket at  <*> 
WARN org.apache.hadoop.hdfs.DFSInputStream: Failed to connect to  <*>  for block , add to deadNodes and continue.  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
ERROR org.apache.hadoop.hdfs.server.namenode.FSDirectory: ERROR in FSDirectory.verifyINodeName
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: STATE* Leaving safe mode after  <*>  secs 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: STATE* Network topology has  <*>  racks and  <*>  datanodes 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: STATE* UnderReplicatedBlocks has  <*>  blocks 
INFO org.apache.hadoop.hdfs.qjournal.server.JournalNode: Initializing journal in directory  <*> 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Cannot access storage directory  <*> 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Cannot access storage directory  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Scanning storage  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Latest log is  latestLog 
WARN org.apache.hadoop.hdfs.qjournal.server.Journal: Latest log  latestLog  has no transactions.  moving it aside and looking for previous log 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: No files in  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: getSegmentInfo( segmentTxId ):  <*>  ->  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Synchronizing log  <*> : no current segment in place 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Synchronizing log  <*>  from  url 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Accepted recovery for segment  <*> :  <*> 
DEBUG org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager: Checking access for user= userId , block= block , access mode= mode  using  <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer: <*> :  b  (numBytes= <*> ) , stage= stage , clientname= clientname , targets= <*> , target storage types= <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer: Connecting to datanode  <*> 
DEBUG org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient: SASL client doing encrypted handshake for addr = {}, datanodeId = {}
DEBUG org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient: Client using encryption algorithm {}
DEBUG org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil: Verifying QOP, requested QOP = {}, negotiated QOP = {}
DEBUG org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil: Creating IOStreamPair of CryptoInputStream and CryptoOutputStream.
DEBUG org.apache.hadoop.hdfs.server.datanode.BlockSender: Bumping up the client provided block\'s genstamp to latest  <*>  for block  block 
DEBUG org.apache.hadoop.hdfs.server.datanode.BlockSender: block= block , replica= <*> 
WARN org.apache.hadoop.hdfs.server.datanode.BlockSender: <*> :sendBlock() :  <*> 
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.Sender: Sending DataTransferOp  <*> :  proto 
INFO org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer: <*> : Transmitted  <*>  (numBytes= <*> ) to  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode  <*> 
FATAL org.apache.hadoop.hdfs.server.namenode.NameNode: Must specify a valid cluster ID after the  <*>  flag 
DEBUG org.apache.hadoop.hdfs.server.namenode.NameNode: Setting fs.defaultFS to  <*> 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Edits URI  dir  listed multiple times in  dfs.namenode.shared.edits.dir . Ignoring duplicates. 
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: !!! WARNING !!!\n\tThe NameNode currently runs without persistent storage.\n\tAny changes to the file system meta-data may be lost.\n\tRecommended actions:\n\t\t- shutdown and restart NameNode with configured \ propertyName \ in hdfs-site.xml; \n\t\t- use Backup Node as a persistent and up-to-date storage  of the file system meta-data. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: !!! WARNING !!!\n\tThe NameNode currently runs without persistent storage.\n\tAny changes to the file system meta-data may be lost.\n\tRecommended actions:\n\t\t- shutdown and restart NameNode with configured \ propertyName \ in hdfs-site.xml; \n\t\t- use Backup Node as a persistent and up-to-date storage  of the file system meta-data. 
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Edits URI  dir  listed multiple times in  dfs.namenode.shared.edits.dir . Ignoring duplicates. 
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: !!! WARNING !!!\n\tThe NameNode currently runs without persistent storage.\n\tAny changes to the file system meta-data may be lost.\n\tRecommended actions:\n\t\t- shutdown and restart NameNode with configured \ propertyName \ in hdfs-site.xml; \n\t\t- use Backup Node as a persistent and up-to-date storage  of the file system meta-data. 
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Edits URI  i$  listed multiple times in  dfs.namenode.shared.edits.dir  and  dfs.namenode.edits.dir . Ignoring duplicates. 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Found KeyProvider:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Enabling async auditlog
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Logj is required to enable async auditlog
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: <*> 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.HeartbeatManager: Setting heartbeat recheck interval to  <*>  since  dfs.namenode.stale.datanode.interval  is less than  dfs.namenode.heartbeat.recheck-interval 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit= <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check= <*> 
WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: The given interval for marking stale datanode =  <*> , which is less than    heartbeat intervals. This may cause too frequent changes of  stale states of DataNodes since a heartbeat msg may be missing  due to temporary short-term failures. Reset stale interval to  <*> . 
INFO org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: dfs.namenode.startup.delay.block.deletion.sec is set to  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: The block deletion will start around  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable= <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled =  <*> 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled:  <*> 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Configured NNs:\n <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: STATE* Leaving safe mode after  <*>  secs 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: STATE* Safe mode is OFF
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: STATE* Network topology has  <*>  racks and  <*>  datanodes 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: STATE* UnderReplicatedBlocks has  <*>  blocks 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: couldn\'t find any VERSION file containing valid ClusterId
INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Allocated new BlockPoolId:  <*> 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: Storage directory  <*>  has been successfully formatted. 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: Storage directory  <*>  has been successfully formatted. 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: Storage directory  <*>  has been successfully formatted. 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSImage: Unable to delete cancelled checkpoint in  sd 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Storage directory  <*>  does not exist 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Cannot access storage directory  <*> 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Storage directory  <*>  does not exist 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Cannot access storage directory  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: <*>  does not exist. Creating ... 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Cannot access storage directory  <*> 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Cannot access storage directory  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Recovering storage directory  <*>  from previous upgrade 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Completing previous rollback for storage directory  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Completing previous checkpoint for storage directory  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: Clusterid mismatch - current clusterid:  <*> , Ignoring given clusterid:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: Using clusterid:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Storage directory  <*>  is not formatted. 
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Formatting ...
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: Storage directory  sd  contains no VERSION file. Skipping... 
ERROR org.apache.hadoop.hdfs.server.namenode.FSEditLog: No edits directories configured!
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImagePreTransactionalStorageInspector: Name checkpoint time is newer than edits, not loading edits.
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load edit log stream:  elis 
INFO org.apache.hadoop.hdfs.server.namenode.FSImage: No edit log streams selected.
CLOSEALLSTREAMS org.apache.hadoop.hdfs.server.namenode.FSImage: <*> toAtLeastTxId recovery 
INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Starting upgrade of local storage directories.\n   old LV =  <*> ; old CTime =  <*> .\n   new LV =  <*> ; new CTime =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil: Starting upgrade of storage directory  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil: Starting upgrade of storage directory  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil: Starting upgrade of storage directory  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil: Performing upgrade of storage directory  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil: Performing upgrade of storage directory  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image?  <*>#_  (staleImage= <*> , haEnabled= <*> , isRollingUpgrade= <*> ) 
INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Save namespace ...
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: NNStorage.attemptRestoreRemovedStorage: check removed(failed) storarge. removedStorages size =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: currently disabled dir  <*> ; type= <*> ;canwrite= <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain  <*>  images with txid >=  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with  <*>  entries  <*>  lookups 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in  ioe  msecs 
WARN org.apache.hadoop.hdfs.server.namenode.FSEditLog: No class configured for  uriScheme ,  <*>  is empty 
WARN org.apache.hadoop.hdfs.server.namenode.FSEditLog: No class configured for  uriScheme ,  <*>  is empty 
INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment  <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.JournalSet: Disabling journal  j 
ERROR org.apache.hadoop.hdfs.server.namenode.JournalSet: Error:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: Resolved path is  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker: ACCESS CHECK:  this , doCheckOwner= doCheckOwner , ancestorAccess= ancestorAccess , parentAccess= parentAccess , access= access , subAccess= subAccess , ignoreEmptyDir= ignoreEmptyDir , resolveLink= resolveLink 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
INFO org.apache.hadoop.hdfs.qjournal.server.JournalNode: Initializing journal in directory  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: <*>  does not exist. Creating ... 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Cannot access storage directory  <*> 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Cannot access storage directory  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Scanning storage  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Latest log is  latestLog 
INFO org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil: Rollback of  <*>  is complete. 
DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode: Got:  <*> 
DEBUG org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager: Checking access for user= userId , block= block , access mode= mode  using  <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode: Got:  <*> 
DEBUG org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager: Checking access for user= userId , block= block , access mode= mode  using  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: Resolved path is  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker: ACCESS CHECK:  this , doCheckOwner= doCheckOwner , ancestorAccess= ancestorAccess , parentAccess= parentAccess , access= access , subAccess= subAccess , ignoreEmptyDir= ignoreEmptyDir , resolveLink= resolveLink 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
DEBUG org.apache.hadoop.hdfs.web.TokenAspect: Created new DT for  <*> 
TRACE org.apache.hadoop.hdfs.web.WebHdfsFileSystem: url= <*> 
DEBUG org.apache.hadoop.hdfs.web.URLConnectionFactory: open AuthenticatedURL connection url 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: storageInfo  failed. 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Deferring removal of stale storage  len$  with  <*>  blocks 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: storageInfo  failed. 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of storages reported in heartbeat= <*> ; Number of storages in storageMap= <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Removed storage  len$  from DataNode this 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: getDatanodeListForReport with includedNodes =  <*> , excludedNodes =  <*> , foundNodes =  <*> , nodes =  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.JournalNode: Initializing journal in directory  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: <*>  does not exist. Creating ... 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Scanning storage  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Latest log is  latestLog 
DEBUG org.apache.hadoop.hdfs.qjournal.server.GetJournalEditServlet: Validating request made by  <*>  /  <*> . This user is:  <*> 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
DEBUG org.apache.hadoop.hdfs.qjournal.server.GetJournalEditServlet: isValidRequestor is allowing other JN principal:  <*> 
WARN org.apache.hadoop.hdfs.qjournal.server.GetJournalEditServlet: Received an invalid request file transfer request from  <*> :  <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.DataXceiver: Number of active connections is:  <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.DataXceiver: Number of active connections is:  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.DataXceiverServer: Shutting down DataXceiverServer before restart
INFO org.apache.hadoop.hdfs.server.datanode.DataXceiverServer: Closing all peers.
INFO org.apache.hadoop.hdfs.qjournal.server.JournalNode: Initializing journal in directory  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: <*>  does not exist. Creating ... 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Lock on  <*>  acquired by nodename  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Recovering storage directory  <*>  from failed checkpoint 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Scanning storage  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Latest log is  latestLog 
INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Discard the EditLog files, the given start txid is  startTxId 
INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Trash the EditLog file  elf 
INFO org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager: Setting block keys
INFO org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager: Setting block keys
INFO org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager: Setting block keys
INFO org.apache.hadoop.hdfs.server.namenode.CacheManager: removeDirective of  id  successful. 
REMOVE org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: <*>
REMOVE org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: <*>
DEBUG org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: *DIR* NameNode.append: file  src  for  clientName  at  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: Resolved path is  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: recoverLease:  <*> , src= src  from client  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Recovering  lease , src= src 
WARN org.apache.hadoop.hdfs.server.namenode.LeaseManager: Removing non-existent lease! holder= holder  src= src 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSNamesystem: closeFile:  path  with  <*>  blocks is persisted to the file system 
TRACE org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* processOverReplicatedBlock: Postponing  block  since storage  storage  does not yet have up-to-date information. 
TRACE org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* processOverReplicatedBlock: Postponing  block  since storage  storage  does not yet have up-to-date information. 
TRACE org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* processOverReplicatedBlock: Postponing  block  since storage  storage  does not yet have up-to-date information. 
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* internalReleaseLease: Removed empty last block and closed file.
DEBUG org.apache.hadoop.hdfs.server.namenode.FSNamesystem: DIR* NameSystem.appendFile: file  <*>  for  holder  at  clientMachine  block  <*>  block size  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: rollingUpgrade  action 
LOGFINALIZEROLLINGUPGRADE org.apache.hadoop.hdfs.server.namenode.FSNamesystem: <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: renaming   <*>  to  <*> 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
DEBUG org.apache.hadoop.hdfs.util.MD5FileUtils: Saved MD  digestString  to  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: renaming   <*>  to  <*> 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
DEBUG org.apache.hadoop.hdfs.util.MD5FileUtils: Saved MD  digestString  to  <*> 
WARN org.apache.hadoop.hdfs.util.MD5FileUtils: deleting   <*>  FAILED 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: Storage directory  <*>  has been successfully formatted. 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: Storage directory  <*>  has been successfully formatted. 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: Storage directory  <*>  has been successfully formatted. 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: Resolved path is  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: Resolved path is  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker: ACCESS CHECK:  this , doCheckOwner= doCheckOwner , ancestorAccess= ancestorAccess , parentAccess= parentAccess , access= access , subAccess= subAccess , ignoreEmptyDir= ignoreEmptyDir , resolveLink= resolveLink 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.DFSClient$Conf: Bad checksum type:  <*> . Using default  CRCC 
DEBUG org.apache.hadoop.hdfs.DFSClient$Conf: dfs.client.use.legacy.blockreader.local =  <*> 
DEBUG org.apache.hadoop.hdfs.DFSClient$Conf: dfs.client.read.shortcircuit =  <*> 
DEBUG org.apache.hadoop.hdfs.DFSClient$Conf: dfs.client.domain.socket.data.traffic =  <*> 
DEBUG org.apache.hadoop.hdfs.DFSClient$Conf: dfs.domain.socket.path =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
WARN org.apache.hadoop.hdfs.DFSClient: dfs.client.test.drop.namenode.response.number is set to  <*> , this hacked client will proactively drop responses 
DEBUG org.apache.hadoop.hdfs.NameNodeProxies: Couldn\'t create proxy provider  failoverProxyProviderClass 
WARN org.apache.hadoop.hdfs.NameNodeProxies: Currently creating proxy using LossyRetryInvocationHandler requires NN HA setup
DEBUG org.apache.hadoop.hdfs.NameNodeProxies: Couldn\'t create proxy provider  failoverProxyProviderClass 
ERROR org.apache.hadoop.hdfs.NameNodeProxies: Unsupported protocol found when creating the proxy connection to NameNode:  <*> 
DEBUG org.apache.hadoop.hdfs.DFSClient: Using local interfaces [ <*> ] with addresses [ <*> ] 
WARN org.apache.hadoop.hdfs.ClientContext: Existing client context \' <*> \' does not match  requested configuration.  Existing:  <*> , Requested:  <*> 
DEBUG org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil: DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for {}
DEBUG org.apache.hadoop.hdfs.DFSOutputStream: Queued packet  <*> 
DEBUG org.apache.hadoop.hdfs.DFSOutputStream: Waiting for ack for:  seqno 
WARN org.apache.hadoop.hdfs.DFSOutputStream: Slow waitForAckedSeqno took  <*> ms (threshold= <*> ms) 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: initReplicaRecovery:  block , recoveryId= recoveryId , replica= <*> 
WARN org.apache.hadoop.hdfs.server.datanode.BlockReceiver: <*> \n <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: initReplicaRecovery: changing replica state for  block  from  <*>  to  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.JournalNode: Initializing journal in directory  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: <*>  does not exist. Creating ... 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Lock on  <*>  acquired by nodename  <*> 
ERROR org.apache.hadoop.hdfs.server.common.Storage: *********** Upgrade is not supported from this  older version  oldVersion  of storage to the current version.  Please upgrade to  Hadoop-.  or a later version and then upgrade to current  version. Old layout version is  <*>  and latest layout version this software version can  upgrade from is  $i . ************ 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Completing previous finalize for storage directory  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Scanning storage  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: No files in  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Formatting  this  with namespace info:  nsInfo 
INFO org.apache.hadoop.hdfs.qjournal.server.JNStorage: Formatting journal  <*>  with nsid:  <*> 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: <*> is not a directory 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Cannot access storage directory  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.JournalNode: Initializing journal in directory  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: <*>  does not exist. Creating ... 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Locking is disabled for  <*> 
ERROR org.apache.hadoop.hdfs.server.common.Storage: *********** Upgrade is not supported from this  older version  oldVersion  of storage to the current version.  Please upgrade to  Hadoop-.  or a later version and then upgrade to current  version. Old layout version is  <*>  and latest layout version this software version can  upgrade from is  $i . ************ 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Completing previous checkpoint for storage directory  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Scanning storage  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: No files in  <*> 
DEBUG org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager: Checking access for user= userId , block= block , access mode= mode  using  <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer: <*> :  b  (numBytes= <*> ) , stage= stage , clientname= clientname , targets= <*> , target storage types= <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer: Connecting to datanode  <*> 
DEBUG org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient: SASL client skipping handshake on trusted connection for addr = {}, datanodeId = {}
DEBUG org.apache.hadoop.hdfs.server.datanode.BlockSender: Bumping up the client provided block\'s genstamp to latest  <*>  for block  block 
DEBUG org.apache.hadoop.hdfs.server.datanode.BlockSender: block= block , replica= <*> 
WARN org.apache.hadoop.hdfs.server.datanode.BlockSender: Could not find metadata file for  block 
WARN org.apache.hadoop.hdfs.server.datanode.BlockSender: <*> :sendBlock() :  <*> 
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.Sender: Sending DataTransferOp  <*> :  proto 
INFO org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer: <*> : Transmitted  <*>  (numBytes= <*> ) to  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: startFile: recover  <*> , src= src  client  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Recovering  lease , src= src 
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: DIR* NameSystem.internalReleaseLease: attempt to release a create lock on  src  but file is already closed. 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: Resolved path is  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: Resolved path is  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: DIR* FSDirectory.unprotectedDelete: failed to remove  src  because the root is not allowed to be deleted 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.FSDirectory: ERROR in FSDirectory.verifyINodeName
ERROR org.apache.hadoop.hdfs.server.namenode.FSDirectory: FSDirectory.verifyMaxDirItems:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSNamesystem: mkdirs: created directory  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: DIR* addFile:  path  is added 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* addToInvalidates:  b   <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  priLevel 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: BLOCK*  <*> : add  block  to  datanode 
INFO org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: BLOCK*  <*> : add  block  to  datanode 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  priLevel 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  i 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker: ACCESS CHECK:  this , doCheckOwner= doCheckOwner , ancestorAccess= ancestorAccess , parentAccess= parentAccess , access= access , subAccess= subAccess , ignoreEmptyDir= ignoreEmptyDir , resolveLink= resolveLink 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: BLOCK*  <*> : add  block  to  datanode 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: BLOCK*  <*> : add  block  to  datanode 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  i 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* addToInvalidates:  b   <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  priLevel 
INFO org.apache.hadoop.hdfs.qjournal.server.JournalNode: Initializing journal in directory  <*> 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Storage directory  <*>  does not exist 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Cannot access storage directory  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Scanning storage  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: No files in  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Rolling forward previously half-completed synchronization:  <*>  ->  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Prepared recovery for segment  segmentTxId :  <*> 
DEBUG org.apache.hadoop.hdfs.DFSInotifyEventInputStream: poll(): lastReadTxid is -, reading current txid from NN
DEBUG org.apache.hadoop.hdfs.DFSInotifyEventInputStream: poll(): read no edits from the NN when requesting edits after txid {}
DEBUG org.apache.hadoop.hdfs.DFSInotifyEventInputStream: poll(): read no edits from the NN when requesting edits after txid {}
DEBUG org.apache.hadoop.hdfs.DFSInotifyEventInputStream: timed poll(): poll() returned null, sleeping for {} ms
DEBUG org.apache.hadoop.hdfs.DFSInotifyEventInputStream: poll(): lastReadTxid is -, reading current txid from NN
DEBUG org.apache.hadoop.hdfs.DFSInotifyEventInputStream: poll(): read no edits from the NN when requesting edits after txid {}
DEBUG org.apache.hadoop.hdfs.DFSInotifyEventInputStream: poll(): read no edits from the NN when requesting edits after txid {}
WARN org.apache.hadoop.hdfs.DFSClient$Conf: Bad checksum type:  <*> . Using default  CRCC 
DEBUG org.apache.hadoop.hdfs.DFSClient$Conf: dfs.client.use.legacy.blockreader.local =  <*> 
DEBUG org.apache.hadoop.hdfs.DFSClient$Conf: dfs.client.read.shortcircuit =  <*> 
DEBUG org.apache.hadoop.hdfs.DFSClient$Conf: dfs.client.domain.socket.data.traffic =  <*> 
DEBUG org.apache.hadoop.hdfs.DFSClient$Conf: dfs.domain.socket.path =  <*> 
DEBUG org.apache.hadoop.hdfs.DFSClient: Using legacy short-circuit local reads.
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
DEBUG org.apache.hadoop.hdfs.DFSClient: Using local interfaces [ <*> ] with addresses [ <*> ] 
WARN org.apache.hadoop.hdfs.ClientContext: Existing client context \' <*> \' does not match  requested configuration.  Existing:  <*> , Requested:  <*> 
DEBUG org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil: DataTransferProtocol using SaslPropertiesResolver, configured QOP {} = {}, configured class {} = {}
INFO org.apache.hadoop.hdfs.qjournal.server.JournalNode: Initializing journal in directory  <*> 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Storage directory  <*>  does not exist 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Cannot access storage directory  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Completing previous checkpoint for storage directory  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Scanning storage  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Latest log is  latestLog 
WARN org.apache.hadoop.hdfs.qjournal.server.Journal: Latest log  latestLog  has no transactions.  moving it aside and looking for previous log 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: No files in  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Updating lastPromisedEpoch from  <*>  to  newEpoch  for client  <*> 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Edit log file  <*>  appears to be empty.  Moving it aside... 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Skipping download of log  <*> : already have up-to-date logs 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Accepted recovery for segment  <*> :  <*> 

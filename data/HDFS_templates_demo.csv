Method,LoggingLevel,Log
<org.apache.hadoop.hdfs.web.WebHdfsFileSystem: void close()>,debug,Token cancel failed:  <*> 
<org.apache.hadoop.hdfs.web.WebHdfsFileSystem: org.apache.hadoop.security.token.Token getDelegationToken()>,debug,Using UGI token:  <*> 
<org.apache.hadoop.hdfs.web.WebHdfsFileSystem: org.apache.hadoop.security.token.Token getDelegationToken()>,debug,Fetched new token:  <*> 
"<org.apache.hadoop.hdfs.web.WebHdfsFileSystem: java.net.URL toUrl(org.apache.hadoop.hdfs.web.resources.HttpOpParam$Op,org.apache.hadoop.fs.Path,org.apache.hadoop.hdfs.web.resources.Param[])>",trace,url= <*> 
"<org.apache.hadoop.hdfs.web.WebHdfsFileSystem: java.net.URL getNamenodeURL(java.lang.String,java.lang.String)>",trace,url= <*> 
<org.apache.hadoop.hdfs.web.WebHdfsFileSystem: boolean replaceExpiredDelegationToken()>,debug,Replaced expired token:  <*> 
"<org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner: void shouldRetry(java.io.IOException,int)>",info,"Retrying connect to namenode:  <*> . Already tried  retry  time(s); retry policy is  <*> , delay  <*> ms. "
"<org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner: void shouldRetry(java.io.IOException,int)>",warn,Original exception is 
<org.apache.hadoop.hdfs.web.URLConnectionFactory: org.apache.hadoop.hdfs.web.URLConnectionFactory newDefaultURLConnectionFactory(org.apache.hadoop.conf.Configuration)>,debug,Cannot load customized ssl related configuration. Fallback to system-generic settings.
"<org.apache.hadoop.hdfs.web.URLConnectionFactory: java.net.URLConnection openConnection(java.net.URL,boolean)>",debug,open AuthenticatedURL connection url 
"<org.apache.hadoop.hdfs.web.URLConnectionFactory: java.net.URLConnection openConnection(java.net.URL,boolean)>",debug,open URL connection
<org.apache.hadoop.hdfs.web.TokenAspect: void initDelegationToken(org.apache.hadoop.security.UserGroupInformation)>,debug,Found existing DT for  <*> 
<org.apache.hadoop.hdfs.web.TokenAspect: void ensureTokenInitialized()>,debug,Created new DT for  <*> 
<org.apache.hadoop.hdfs.web.resources.ExceptionHandler: javax.ws.rs.core.Response toResponse(java.lang.Exception)>,trace,GOT EXCEPITION
<org.apache.hadoop.hdfs.web.resources.ExceptionHandler: javax.ws.rs.core.Response toResponse(java.lang.Exception)>,warn,INTERNAL_SERVER_ERROR
"<org.apache.hadoop.hdfs.web.HftpFileSystem: java.net.URL getNamenodeURL(java.lang.String,java.lang.String)>",trace,url= <*> 
<org.apache.hadoop.hdfs.web.HftpFileSystem$2: org.apache.hadoop.security.token.Token run()>,debug,Exception getting delegation token
<org.apache.hadoop.hdfs.web.HftpFileSystem$2: org.apache.hadoop.security.token.Token run()>,debug,Got dt for  <*> ;t.service= <*> 
<org.apache.hadoop.hdfs.web.HftpFileSystem$2: org.apache.hadoop.security.token.Token run()>,warn,"Couldn\'t connect to  <*> , assuming security is disabled "
"<org.apache.hadoop.hdfs.util.MD5FileUtils: void saveMD5File(java.io.File,java.lang.String)>",debug,Saved MD  digestString  to  <*> 
"<org.apache.hadoop.hdfs.util.MD5FileUtils: void renameMD5File(java.io.File,java.io.File)>",warn,deleting   <*>  FAILED 
"<org.apache.hadoop.hdfs.util.LightWeightHashSet: void <init>(int,float,float)>",debug,"initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor "
<org.apache.hadoop.hdfs.util.AtomicFileOutputStream: void close()>,warn,Unable to delete tmp file  <*> 
<org.apache.hadoop.hdfs.util.AtomicFileOutputStream: void abort()>,warn,Unable to abort file  <*> 
<org.apache.hadoop.hdfs.util.AtomicFileOutputStream: void abort()>,warn,Unable to delete tmp file during abort  <*> 
<org.apache.hadoop.hdfs.tools.offlineImageViewer.WebImageViewer: void initServerAndWait(java.lang.String)>,info,Interrupted. Stopping the WebImageViewer.
<org.apache.hadoop.hdfs.tools.offlineImageViewer.WebImageViewer: void initServer(java.lang.String)>,info,WebImageViewer started. Listening on  <*> . Press Ctrl+C to stop the viewer. 
<org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageViewer: void go()>,error,image loading failed at offset  <*> 
<org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageLoader: org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageLoader load(java.lang.String)>,debug,Loading section  <*>  length:  <*> 
"<org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageLoader: java.util.Map loadINodeDirectorySection(java.io.InputStream,java.util.List)>",info,Loading inode directory section
"<org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageLoader: java.util.Map loadINodeDirectorySection(java.io.InputStream,java.util.List)>",info,Loaded  counter  directories 
<org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageLoader: java.lang.String[] loadStringTable(java.io.InputStream)>,info,Loading  <*>  strings 
<org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageLoader: com.google.common.collect.ImmutableList loadINodeReferenceSection(java.io.InputStream)>,info,Loading inode references
<org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageLoader: com.google.common.collect.ImmutableList loadINodeReferenceSection(java.io.InputStream)>,info,Loaded  counter  inode references 
<org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageLoader: byte[][] loadINodeSection(java.io.InputStream)>,debug,Sorting inodes
<org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageLoader: byte[][] loadINodeSection(java.io.InputStream)>,debug,Finished sorting inodes
<org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageLoader: byte[][] loadINodeSection(java.io.InputStream)>,info,Loading  <*>  inodes. 
<org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageHandler: org.jboss.netty.channel.ChannelFuture handleOperation(org.jboss.netty.channel.MessageEvent)>,info,<*>  method= <*>  op= <*>  target= path 
<org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsBinaryLoader: void loadEdits()>,error,Got IOException at position  <*> 
<org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsBinaryLoader: void loadEdits()>,error,Got IOException while reading stream!  Resyncing.
<org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsBinaryLoader: void loadEdits()>,error,Got RuntimeException at position  <*> 
<org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsBinaryLoader: void loadEdits()>,error,Got RuntimeException while reading stream!  Resyncing.
<org.apache.hadoop.hdfs.tools.GetGroups: void setConf(org.apache.hadoop.conf.Configuration)>,debug,Using NN principal:  <*> 
<org.apache.hadoop.hdfs.tools.DFSZKFailoverController: void main(java.lang.String[])>,fatal,"Got a fatal error, exiting now"
<org.apache.hadoop.hdfs.tools.DFSZKFailoverController: void checkRpcAdminAccess()>,info,Allowed RPC access from  <*>  at  <*> 
<org.apache.hadoop.hdfs.tools.DFSZKFailoverController: void checkRpcAdminAccess()>,warn,Disallowed RPC access from  <*>  at  <*> . Not listed in  dfs.cluster.administrators 
"<org.apache.hadoop.hdfs.tools.DFSZKFailoverController: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.tools.NNHAServiceTarget)>",info,Failover controller configured for NameNode  localTarget 
<org.apache.hadoop.hdfs.tools.DFSHAAdmin: org.apache.hadoop.conf.Configuration addSecurityConfiguration(org.apache.hadoop.conf.Configuration)>,debug,Using NN principal:  <*> 
<org.apache.hadoop.hdfs.tools.DFSAdmin: int run(java.lang.String[])>,debug,Exception encountered:
"<org.apache.hadoop.hdfs.tools.DelegationTokenFetcher: org.apache.hadoop.security.Credentials getDTfromRemote(org.apache.hadoop.hdfs.web.URLConnectionFactory,java.net.URI,java.lang.String,java.lang.String)>",debug,Retrieving token from:  <*> 
"<org.apache.hadoop.hdfs.tools.DelegationTokenFetcher: long renewDelegationToken(org.apache.hadoop.hdfs.web.URLConnectionFactory,java.net.URI,org.apache.hadoop.security.token.Token)>",info,error in renew over HTTP
"<org.apache.hadoop.hdfs.tools.DelegationTokenFetcher: long renewDelegationToken(org.apache.hadoop.hdfs.web.URLConnectionFactory,java.net.URI,org.apache.hadoop.security.token.Token)>",info,rethrowing exception from HTTP request:  <*> 
"<org.apache.hadoop.hdfs.tools.DelegationTokenFetcher: java.net.HttpURLConnection run(org.apache.hadoop.hdfs.web.URLConnectionFactory,java.net.URL)>",info,Error when dealing remote token:
"<org.apache.hadoop.hdfs.tools.DelegationTokenFetcher: java.net.HttpURLConnection run(org.apache.hadoop.hdfs.web.URLConnectionFactory,java.net.URL)>",info,rethrowing exception from HTTP request:  <*> 
<org.apache.hadoop.hdfs.tools.DelegationTokenFetcher: java.io.IOException getExceptionFromResponse(java.net.HttpURLConnection)>,info,Error response from HTTP request= <*> ;ec= ie ;em= exceptionMsg 
<org.apache.hadoop.hdfs.tools.DelegationTokenFetcher: java.io.IOException getExceptionFromResponse(java.net.HttpURLConnection)>,info,Exception from HTTP response= <*> 
<org.apache.hadoop.hdfs.tools.DelegationTokenFetcher: java.io.IOException getExceptionFromResponse(java.net.HttpURLConnection)>,warn,failed to create object of this class
<org.apache.hadoop.hdfs.tools.DelegationTokenFetcher$1: java.lang.Object run()>,debug,Renewed token for  <*>  until:  <*> 
<org.apache.hadoop.hdfs.tools.DelegationTokenFetcher$1: java.lang.Object run()>,debug,Cancelled token for  <*> 
<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm: void unregisterSlot(int)>,trace,this : unregisterSlot  slotIdx 
<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm: void free()>,trace,this : freed 
<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm: void free()>,warn,this : failed to munmap 
"<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm: void <init>(org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$ShmId,java.io.FileInputStream)>",trace,"creating  <*> (shmId= shmId , mmappedLength= <*> , baseAddress= <*> , slots.length= <*> ) "
<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm: sun.misc.Unsafe safetyDance()>,error,failed to load misc.Unsafe
"<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$Slot registerSlot(int,org.apache.hadoop.hdfs.ExtendedBlockId)>",trace,this : registerSlot  slotIdx : allocatedSlots= <*> <*> 
<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$Slot allocAndRegisterSlot(org.apache.hadoop.hdfs.ExtendedBlockId)>,trace,this : allocAndRegisterSlot  <*> : allocatedSlots= <*> <*> 
<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica: void close()>,trace,closed  this suffix 
<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica: java.nio.MappedByteBuffer loadMmapInternal()>,trace,this : created mmap of size  <*> 
<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica: java.nio.MappedByteBuffer loadMmapInternal()>,warn,this : mmap error 
<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica: java.nio.MappedByteBuffer loadMmapInternal()>,warn,this : mmap error 
<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica: boolean isStale()>,trace,this : checked shared memory segment.  isStale= <*> 
<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica: boolean isStale()>,trace,"this  is stale because it\'s  stale#  ms old, and staleThresholdMs =  <*> "
<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica: boolean isStale()>,trace,"this  is not stale because it\'s only  stale#  ms old, and staleThresholdMs =  <*> "
<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica: boolean addNoChecksumAnchor()>,trace,this : added no-checksum anchor to slot  <*> 
<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica: boolean addNoChecksumAnchor()>,trace,this : could not add no-checksum anchor to slot  <*> 
<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: void unref(org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica)>,debug,this :  purgeReason#_ 
<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: void unref(org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica)>,trace,this : unref replica  replica :  purgeReason  refCount  <*>  ->  <*> <*> 
<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: void trimEvictionMaps()>,trace,this : trimEvictionMaps is purging  replica <*> 
<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: void startCacheCleanerThreadIfNeeded()>,debug,this : starting cache cleaner thread which will run  every  <*>  ms 
<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: void ref(org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica)>,trace,this :  <*>  no longer contains  replica .  refCount  <*>  ->  <*> <*> 
<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: void ref(org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica)>,trace,this : replica  refCount  <*>  ->  <*> <*> 
<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: void close()>,info,this : closing 
"<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: void <init>(int,long,int,long,long,long,int)>",error,failed to create ShortCircuitShmManager
"<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplicaInfo fetchOrCreate(org.apache.hadoop.hdfs.ExtendedBlockId,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$ShortCircuitReplicaCreator)>",debug,this : retrying  <*> 
"<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplicaInfo fetchOrCreate(org.apache.hadoop.hdfs.ExtendedBlockId,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$ShortCircuitReplicaCreator)>",trace,this : can\'t fetchOrCreate  key  because the cache is closed. 
"<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplicaInfo fetch(org.apache.hadoop.hdfs.ExtendedBlockId,org.apache.hadoop.util.Waitable)>",info,this : interrupted while waiting for  key 
"<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplicaInfo fetch(org.apache.hadoop.hdfs.ExtendedBlockId,org.apache.hadoop.util.Waitable)>",info,this : got stale replica  <*> .  Removing  this replica from the replicaInfoMap and retrying. 
"<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplicaInfo fetch(org.apache.hadoop.hdfs.ExtendedBlockId,org.apache.hadoop.util.Waitable)>",trace,this : found waitable for  key 
"<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplicaInfo fetch(org.apache.hadoop.hdfs.ExtendedBlockId,org.apache.hadoop.util.Waitable)>",warn,this : could not get  key  due to InvalidToken  exception. 
"<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplicaInfo fetch(org.apache.hadoop.hdfs.ExtendedBlockId,org.apache.hadoop.util.Waitable)>",warn,this : failed to get  key 
"<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplicaInfo create(org.apache.hadoop.hdfs.ExtendedBlockId,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$ShortCircuitReplicaCreator,org.apache.hadoop.util.Waitable)>",trace,this : loading  key 
"<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplicaInfo create(org.apache.hadoop.hdfs.ExtendedBlockId,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$ShortCircuitReplicaCreator,org.apache.hadoop.util.Waitable)>",trace,this : successfully loaded  <*> 
"<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplicaInfo create(org.apache.hadoop.hdfs.ExtendedBlockId,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$ShortCircuitReplicaCreator,org.apache.hadoop.util.Waitable)>",warn,this : failed to load  key 
"<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplicaInfo create(org.apache.hadoop.hdfs.ExtendedBlockId,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$ShortCircuitReplicaCreator,org.apache.hadoop.util.Waitable)>",warn,this : could not load  key  due to InvalidToken  exception. 
"<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplicaInfo create(org.apache.hadoop.hdfs.ExtendedBlockId,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$ShortCircuitReplicaCreator,org.apache.hadoop.util.Waitable)>",warn,this : failed to load  key 
<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: int demoteOldEvictableMmaped(long)>,trace,demoteOldEvictable: demoting  <*> :  <*> :  <*> 
<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$SlotReleaser: void run()>,error,<*> : failed to release  short-circuit shared memory slot  <*>  by sending  ReleaseShortCircuitAccessRequestProto to  <*> .  Closing shared memory segment. 
<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$SlotReleaser: void run()>,trace,<*> : about to release  <*> 
<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$SlotReleaser: void run()>,trace,<*> : released  <*> 
<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$CacheCleaner: void run()>,debug,this : cache cleaner running at  <*> 
<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$CacheCleaner: void run()>,debug,this : finishing cache cleaner run started at  <*> .  Demoted  <*>  mmapped replicas;  purged  numPurged  replicas. 
<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$CacheCleaner: void run()>,trace,CacheCleaner: purging  replica :  <*> 
<org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory: void <init>(org.apache.hadoop.hdfs.DFSClient$Conf)>,debug,Both short-circuit local reads and UNIX domain socket are disabled.
<org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory: void <init>(org.apache.hadoop.hdfs.DFSClient$Conf)>,debug,feature  is enabled. 
<org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory: void <init>(org.apache.hadoop.hdfs.DFSClient$Conf)>,warn,feature#_  cannot be used because  <*> 
"<org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory: org.apache.hadoop.net.unix.DomainSocket createSocket(org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory$PathInfo,int)>",warn,error creating DomainSocket
<org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager$EndpointShmManager: void shutdown(org.apache.hadoop.hdfs.shortcircuit.DfsClientShm)>,warn,this : error shutting down shm: got IOException calling  shutdown(SHUT_RDWR) 
<org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager$EndpointShmManager: void freeSlot(org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$Slot)>,trace,this : freeing empty stale  shm 
<org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager$EndpointShmManager: void freeSlot(org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$Slot)>,trace,this : shutting down UNIX domain socket for  empty  shm 
<org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager$EndpointShmManager: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$Slot allocSlotFromExistingShm(org.apache.hadoop.hdfs.ExtendedBlockId)>,trace,this : pulled the last slot  <*>  out of  shm 
<org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager$EndpointShmManager: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$Slot allocSlotFromExistingShm(org.apache.hadoop.hdfs.ExtendedBlockId)>,trace,this : pulled slot  <*>  out of  shm 
"<org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager$EndpointShmManager: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$Slot allocSlot(org.apache.hadoop.hdfs.net.DomainPeer,org.apache.commons.lang.mutable.MutableBoolean,java.lang.String,org.apache.hadoop.hdfs.ExtendedBlockId)>",debug,this : the UNIX domain socket associated with  this short-circuit memory closed before we could make  use of the shm. 
"<org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager$EndpointShmManager: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$Slot allocSlot(org.apache.hadoop.hdfs.net.DomainPeer,org.apache.commons.lang.mutable.MutableBoolean,java.lang.String,org.apache.hadoop.hdfs.ExtendedBlockId)>",trace,this : the DfsClientShmManager has been closed. 
"<org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager$EndpointShmManager: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$Slot allocSlot(org.apache.hadoop.hdfs.net.DomainPeer,org.apache.commons.lang.mutable.MutableBoolean,java.lang.String,org.apache.hadoop.hdfs.ExtendedBlockId)>",trace,this : shared memory segment access is disabled. 
"<org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager$EndpointShmManager: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$Slot allocSlot(org.apache.hadoop.hdfs.net.DomainPeer,org.apache.commons.lang.mutable.MutableBoolean,java.lang.String,org.apache.hadoop.hdfs.ExtendedBlockId)>",trace,this : waiting for loading to finish... 
"<org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager$EndpointShmManager: org.apache.hadoop.hdfs.shortcircuit.DfsClientShm requestNewShm(java.lang.String,org.apache.hadoop.hdfs.net.DomainPeer)>",info,this : datanode does not support short-circuit  shared memory access:  <*> 
"<org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager$EndpointShmManager: org.apache.hadoop.hdfs.shortcircuit.DfsClientShm requestNewShm(java.lang.String,org.apache.hadoop.hdfs.net.DomainPeer)>",trace,this : createNewShm: created  <*> 
"<org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager$EndpointShmManager: org.apache.hadoop.hdfs.shortcircuit.DfsClientShm requestNewShm(java.lang.String,org.apache.hadoop.hdfs.net.DomainPeer)>",warn,this : error requesting short-circuit shared memory  access:  <*> 
"<org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods: void init(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.web.resources.DelegationParam,org.apache.hadoop.hdfs.web.resources.UserParam,org.apache.hadoop.hdfs.web.resources.DoAsParam,org.apache.hadoop.hdfs.web.resources.UriFsPathParam,org.apache.hadoop.hdfs.web.resources.HttpOpParam,org.apache.hadoop.hdfs.web.resources.Param[])>",trace,"HTTP  <*> :  op ,  path , ugi= ugi ,  username ,  doAsUser <*> "
"<org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods: java.net.URI redirectURI(org.apache.hadoop.hdfs.server.namenode.NameNode,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.web.resources.DelegationParam,org.apache.hadoop.hdfs.web.resources.UserParam,org.apache.hadoop.hdfs.web.resources.DoAsParam,java.lang.String,org.apache.hadoop.hdfs.web.resources.HttpOpParam$Op,long,long,java.lang.String,org.apache.hadoop.hdfs.web.resources.Param[])>",trace,redirectURI= <*> 
"<org.apache.hadoop.hdfs.server.namenode.TransferFsImage: void uploadImageFromStorage(java.net.URL,org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.namenode.NNStorage,org.apache.hadoop.hdfs.server.namenode.NNStorage$NameNodeFile,long,org.apache.hadoop.hdfs.util.Canceler)>",info,Image upload with txid  txid  conflicted with a previous image upload to the  same NameNode. Continuing... 
"<org.apache.hadoop.hdfs.server.namenode.TransferFsImage: void uploadImageFromStorage(java.net.URL,org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.namenode.NNStorage,org.apache.hadoop.hdfs.server.namenode.NNStorage$NameNodeFile,long,org.apache.hadoop.hdfs.util.Canceler)>",info,Uploaded image with txid  txid  to namenode at  fsName  in  <*>  seconds 
<org.apache.hadoop.hdfs.server.namenode.TransferFsImage: void setTimeout(java.net.HttpURLConnection)>,info,Image Transfer timeout configured to  <*>  milliseconds 
"<org.apache.hadoop.hdfs.server.namenode.TransferFsImage: void downloadEditsToStorage(java.net.URL,org.apache.hadoop.hdfs.server.protocol.RemoteEditLog,org.apache.hadoop.hdfs.server.namenode.NNStorage)>",debug,Dest file:  f 
"<org.apache.hadoop.hdfs.server.namenode.TransferFsImage: void downloadEditsToStorage(java.net.URL,org.apache.hadoop.hdfs.server.protocol.RemoteEditLog,org.apache.hadoop.hdfs.server.namenode.NNStorage)>",debug,Renaming  <*>  to  <*> 
"<org.apache.hadoop.hdfs.server.namenode.TransferFsImage: void downloadEditsToStorage(java.net.URL,org.apache.hadoop.hdfs.server.protocol.RemoteEditLog,org.apache.hadoop.hdfs.server.namenode.NNStorage)>",info,Skipping download of remote edit log  log  since it already is stored locally at  f 
"<org.apache.hadoop.hdfs.server.namenode.TransferFsImage: void downloadEditsToStorage(java.net.URL,org.apache.hadoop.hdfs.server.protocol.RemoteEditLog,org.apache.hadoop.hdfs.server.namenode.NNStorage)>",info,Downloaded file  <*>  size  <*>  bytes. 
"<org.apache.hadoop.hdfs.server.namenode.TransferFsImage: void downloadEditsToStorage(java.net.URL,org.apache.hadoop.hdfs.server.protocol.RemoteEditLog,org.apache.hadoop.hdfs.server.namenode.NNStorage)>",warn,Unable to rename edits file from  <*>  to  <*> 
"<org.apache.hadoop.hdfs.server.namenode.TransferFsImage: void copyFileToStream(java.io.OutputStream,java.io.File,java.io.FileInputStream,org.apache.hadoop.hdfs.util.DataTransferThrottler,org.apache.hadoop.hdfs.util.Canceler)>",info,Connection closed by client
"<org.apache.hadoop.hdfs.server.namenode.TransferFsImage: void copyFileToStream(java.io.OutputStream,java.io.File,java.io.FileInputStream,org.apache.hadoop.hdfs.util.DataTransferThrottler,org.apache.hadoop.hdfs.util.Canceler)>",warn,SIMULATING A CORRUPT BYTE IN IMAGE TRANSFER!
"<org.apache.hadoop.hdfs.server.namenode.TransferFsImage: org.apache.hadoop.io.MD5Hash receiveFile(java.lang.String,java.util.List,org.apache.hadoop.hdfs.server.common.Storage,boolean,long,org.apache.hadoop.io.MD5Hash,java.lang.String,java.io.InputStream,org.apache.hadoop.hdfs.util.DataTransferThrottler)>",warn,Overwriting existing file  f  with file downloaded from  url 
"<org.apache.hadoop.hdfs.server.namenode.TransferFsImage: org.apache.hadoop.io.MD5Hash receiveFile(java.lang.String,java.util.List,org.apache.hadoop.hdfs.server.common.Storage,boolean,long,org.apache.hadoop.io.MD5Hash,java.lang.String,java.io.InputStream,org.apache.hadoop.hdfs.util.DataTransferThrottler)>",warn,Unable to download file  f 
"<org.apache.hadoop.hdfs.server.namenode.TransferFsImage: org.apache.hadoop.io.MD5Hash handleUploadImageRequest(javax.servlet.http.HttpServletRequest,long,org.apache.hadoop.hdfs.server.common.Storage,java.io.InputStream,long,org.apache.hadoop.hdfs.util.DataTransferThrottler)>",info,Downloaded file  <*>  size  <*>  bytes. 
"<org.apache.hadoop.hdfs.server.namenode.TransferFsImage: org.apache.hadoop.io.MD5Hash getFileClient(java.net.URL,java.lang.String,java.util.List,org.apache.hadoop.hdfs.server.common.Storage,boolean)>",info,Opening connection to  <*> 
"<org.apache.hadoop.hdfs.server.namenode.TransferFsImage: org.apache.hadoop.io.MD5Hash downloadImageToStorage(java.net.URL,long,org.apache.hadoop.hdfs.server.common.Storage,boolean)>",info,Downloaded file  <*>  size  <*>  bytes. 
"<org.apache.hadoop.hdfs.server.namenode.StreamFile: void doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)>",debug,response.isCommitted()= <*> 
"<org.apache.hadoop.hdfs.server.namenode.snapshot.DirectorySnapshottableFeature: org.apache.hadoop.hdfs.server.namenode.snapshot.Snapshot removeSnapshot(org.apache.hadoop.hdfs.server.namenode.INodeDirectory,java.lang.String,org.apache.hadoop.hdfs.server.namenode.INode$BlocksMapUpdateInfo,java.util.List)>",error,BUG: removeSnapshot increases namespace usage.
<org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: void shutdown()>,info,Interrupted waiting to join on checkpointer thread
<org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: void shutdown()>,warn,Exception shutting down SecondaryNameNode
<org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: void shutdown()>,warn,Exception while closing CheckpointStorage
<org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: void main(java.lang.String[])>,fatal,Failed to parse options
<org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: void main(java.lang.String[])>,fatal,Failed to start secondary namenode
<org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: void join()>,debug,Exception 
"<org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CommandLineOpts)>",info,Web server init done
"<org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CommandLineOpts)>",info,Checkpoint Period   : <*>  secs  ( <*>  min) 
"<org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CommandLineOpts)>",info,Log Size Trigger    : <*>  txns 
<org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: void doWork()>,error,Exception in doCheckpoint
<org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: void doWork()>,fatal,Merging failed  <*>  times. 
<org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: void doWork()>,fatal,Throwable Exception in doCheckpoint
<org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: java.net.URL getInfoServer()>,debug,Will connect to NameNode at  <*> 
<org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: int processStartupCommand(org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CommandLineOpts)>,error,<*> :  <*> 
<org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: int processStartupCommand(org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CommandLineOpts)>,error,<*> :  <*> 
<org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: int processStartupCommand(org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CommandLineOpts)>,error,<*> :  <*> 
<org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: boolean doCheckpoint()>,warn,Checkpoint done. New Image Size:  <*> 
<org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: boolean doCheckpoint()>,warn,Failed to write legacy OIV image: 
<org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage: void recoverCreate(boolean)>,info,Formatting storage directory  sd 
<org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage: void deleteTempEdits()>,warn,Failed to delete temporary edits file:  <*> 
<org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$2: java.lang.Boolean run()>,info,Image has not changed. Will not download image.
<org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$2: java.lang.Boolean run()>,info,Image has changed. Downloading updated image from NN.
"<org.apache.hadoop.hdfs.server.namenode.RenewDelegationTokenServlet: void doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)>",info,Request for token received with no authentication from  <*> 
"<org.apache.hadoop.hdfs.server.namenode.RenewDelegationTokenServlet: void doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)>",info,Exception while renewing token. Re-throwing. s= <*> 
<org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: org.apache.hadoop.hdfs.server.namenode.FSEditLogOp nextOp()>,<init>,got premature end-of-file at txid  <*> ; expected file to go up to  <*> 
<org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: org.apache.hadoop.hdfs.server.namenode.FSEditLogOp nextOp()>,error,Got error reading edit log input stream  <*> ; failing over to edit log  <*> 
<org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: org.apache.hadoop.hdfs.server.namenode.FSEditLogOp nextOp()>,error,failing over to edit log  <*> 
<org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: org.apache.hadoop.hdfs.server.namenode.FSEditLogOp nextOp()>,info,Fast-forwarding stream \' <*> \' to transaction ID  <*> 
"<org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil: void doUpgrade(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.hdfs.server.common.Storage)>",error,Unable to rename temp to previous for  <*> 
"<org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil: void doUpgrade(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.hdfs.server.common.Storage)>",info,Performing upgrade of storage directory  <*> 
<org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil: void doRollBack(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory)>,info,Rollback of  <*>  is complete. 
"<org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil: void doPreUpgrade(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory)>",info,Starting upgrade of storage directory  <*> 
<org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil: void doFinalize(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory)>,info,Directory  <*>  does not exist. 
<org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil: void doFinalize(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory)>,info,Finalize upgrade for  <*>  is not required. 
<org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil: void doFinalize(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory)>,info,Finalizing upgrade of storage directory  <*> 
<org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil: void doFinalize(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory)>,info,Finalize upgrade for  <*>  is complete. 
"<org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil: boolean canRollBack(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.hdfs.server.common.StorageInfo,org.apache.hadoop.hdfs.server.common.StorageInfo,int)>",info,Storage directory  <*>  does not contain previous fs state. 
"<org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: void purgeOldLegacyOIVImages(java.lang.String,long)>",info,Deleting  <*> 
"<org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: void purgeOldLegacyOIVImages(java.lang.String,long)>",warn,Invalid file name. Skipping  fName 
"<org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: void purgeOldLegacyOIVImages(java.lang.String,long)>",warn,Failed to delete image file:  $u 
<org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: long getImageTxIdToRetain(org.apache.hadoop.hdfs.server.namenode.FSImageTransactionalStorageInspector)>,info,Going to retain  <*>  images with txid >=  <*> 
<org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager$DeletionStoragePurger: void purgeLog(org.apache.hadoop.hdfs.server.namenode.FileJournalManager$EditLogFile)>,info,Purging old edit log  log 
<org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager$DeletionStoragePurger: void purgeImage(org.apache.hadoop.hdfs.server.namenode.FSImageStorageInspector$FSImageFile)>,info,Purging old image  image 
<org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager$DeletionStoragePurger: void deleteOrWarn(java.io.File)>,warn,Could not delete  file 
<org.apache.hadoop.hdfs.server.namenode.NNStorage: void writeTransactionIdFileToStorage(long)>,warn,writeTransactionIdToStorage failed on  sd 
<org.apache.hadoop.hdfs.server.namenode.NNStorage: void setRestoreFailedStorage(boolean)>,warn,set restore failed storage to  val 
<org.apache.hadoop.hdfs.server.namenode.NNStorage: void reportErrorsOnDirectory(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory)>,debug,current list of storage dirs: <*> 
<org.apache.hadoop.hdfs.server.namenode.NNStorage: void reportErrorsOnDirectory(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory)>,debug,at the end current list of storage dirs: <*> 
<org.apache.hadoop.hdfs.server.namenode.NNStorage: void reportErrorsOnDirectory(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory)>,error,Error reported on storage directory  sd 
<org.apache.hadoop.hdfs.server.namenode.NNStorage: void reportErrorsOnDirectory(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory)>,warn,About to remove corresponding storage:  <*> 
<org.apache.hadoop.hdfs.server.namenode.NNStorage: void reportErrorsOnDirectory(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory)>,warn,Unable to unlock bad storage directory:  <*> 
"<org.apache.hadoop.hdfs.server.namenode.NNStorage: void processStartupOptionsForUpgrade(org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,int)>",info,Using clusterid:  <*> 
"<org.apache.hadoop.hdfs.server.namenode.NNStorage: void processStartupOptionsForUpgrade(org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,int)>",warn,"Clusterid mismatch - current clusterid:  <*> , Ignoring given clusterid:  <*> "
<org.apache.hadoop.hdfs.server.namenode.NNStorage: void format(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory)>,info,Storage directory  <*>  has been successfully formatted. 
<org.apache.hadoop.hdfs.server.namenode.NNStorage: void attemptRestoreRemovedStorage()>,info,NNStorage.attemptRestoreRemovedStorage: check removed(failed) storarge. removedStorages size =  <*> 
<org.apache.hadoop.hdfs.server.namenode.NNStorage: void attemptRestoreRemovedStorage()>,info,currently disabled dir  <*> ; type= <*> ;canwrite= <*> 
<org.apache.hadoop.hdfs.server.namenode.NNStorage: void attemptRestoreRemovedStorage()>,info,restoring dir  <*> 
"<org.apache.hadoop.hdfs.server.namenode.NNStorage: org.apache.hadoop.hdfs.server.namenode.FSImageStorageInspector readAndInspectDirs(java.util.EnumSet,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption)>",warn,Storage directory  sd  contains no VERSION file. Skipping... 
<org.apache.hadoop.hdfs.server.namenode.NNStorage: org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory getStorageDirectory(java.net.URI)>,warn,Error converting file to URI
<org.apache.hadoop.hdfs.server.namenode.NNStorage: java.lang.String newBlockPoolID()>,warn,Could not find ip address of \default\ inteface.
<org.apache.hadoop.hdfs.server.namenode.NNStorage: java.lang.String determineClusterId()>,info,current cluster id for sd= <*> ;lv= <*> ;cid= <*> 
<org.apache.hadoop.hdfs.server.namenode.NNStorage: java.lang.String determineClusterId()>,warn,this sd not available:  <*> 
<org.apache.hadoop.hdfs.server.namenode.NNStorage: java.lang.String determineClusterId()>,warn,couldn\'t find any VERSION file containing valid ClusterId
<org.apache.hadoop.hdfs.server.namenode.NNConf: void <init>(org.apache.hadoop.conf.Configuration)>,info,ACLs enabled?  <*> 
<org.apache.hadoop.hdfs.server.namenode.NNConf: void <init>(org.apache.hadoop.conf.Configuration)>,info,XAttrs enabled?  <*> 
<org.apache.hadoop.hdfs.server.namenode.NNConf: void <init>(org.apache.hadoop.conf.Configuration)>,info,Maximum size of an xattr:  <*> <*> 
<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: void verifySoftwareVersion(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration)>,info,<*> . Note: This is normal during a rolling upgrade. 
<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: void verifySoftwareVersion(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration)>,warn,<*>  DN:  dnReg 
<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: void verifyRequest(org.apache.hadoop.hdfs.server.protocol.NodeRegistration)>,warn,Registration IDs mismatched: the  <*>  ID is  <*>  but the expected ID is  <*> 
"<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: void rename2(java.lang.String,java.lang.String,org.apache.hadoop.fs.Options$Rename[])>",debug,*DIR* NameNode.rename:  src  to  dst 
<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: void refreshUserToGroupsMappings()>,info,Refreshing all user-to-groups mappings. Requested by user:  <*> 
<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: void refreshSuperUserGroupsConfiguration()>,info,Refreshing SuperUser proxy group mapping list 
<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: void refreshCallQueue()>,info,Refreshing call queue.
"<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: void errorReport(org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration,int,java.lang.String)>",info,Error report from  registration :  msg 
"<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: void errorReport(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration,int,java.lang.String)>",info,Error report from  <*> :  msg 
"<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: void errorReport(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration,int,java.lang.String)>",info,Error report from  <*> :  msg 
"<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: void errorReport(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration,int,java.lang.String)>",warn,Disk error on  <*> :  msg 
"<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: void errorReport(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration,int,java.lang.String)>",warn,Fatal disk error on  <*> :  msg 
"<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: void blockReceivedAndDeleted(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration,java.lang.String,org.apache.hadoop.hdfs.server.protocol.StorageReceivedDeletedBlocks[])>",debug,*BLOCK* NameNode.blockReceivedAndDeleted: from  nodeReg   <*>  blocks. 
"<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: void abandonBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,java.lang.String,java.lang.String)>",debug,*BLOCK* NameNode.abandonBlock:  b  of file  src 
"<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.namenode.NameNode)>",info,Service RPC server is binding to  bindHost : <*> 
"<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.namenode.NameNode)>",info,RPC server is binding to  serviceHandlerCount : <*> 
"<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: org.apache.hadoop.hdfs.server.protocol.DatanodeCommand cacheReport(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration,java.lang.String,java.util.List)>",debug,*BLOCK* NameNode.cacheReport: from  nodeReg   <*>  blocks 
"<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: org.apache.hadoop.hdfs.server.protocol.DatanodeCommand blockReport(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration,java.lang.String,org.apache.hadoop.hdfs.server.protocol.StorageBlockReport[],org.apache.hadoop.hdfs.server.protocol.BlockReportContext)>",debug,"*BLOCK* NameNode.blockReport: from  nodeReg , reports.length= <*> "
<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: org.apache.hadoop.hdfs.server.namenode.FSEditLogOp readOp(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream)>,debug,Tried to read from deleted or moved edit log segment
<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: org.apache.hadoop.hdfs.server.namenode.FSEditLogOp readOp(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream)>,debug,Tried to read from deleted edit log segment
<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: org.apache.hadoop.hdfs.protocol.RollingUpgradeInfo rollingUpgrade(org.apache.hadoop.hdfs.protocol.HdfsConstants$RollingUpgradeAction)>,info,rollingUpgrade  action 
"<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: org.apache.hadoop.hdfs.protocol.LocatedBlock getAdditionalDatanode(java.lang.String,long,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],java.lang.String[],org.apache.hadoop.hdfs.protocol.DatanodeInfo[],int,java.lang.String)>",debug,"getAdditionalDatanode: src= src , fileId= fileId , blk= blk , existings= <*> , excludes= <*> , numAdditionalNodes= numAdditionalNodes , clientName= clientName "
"<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: org.apache.hadoop.hdfs.protocol.LocatedBlock append(java.lang.String,java.lang.String)>",debug,*DIR* NameNode.append: file  src  for  clientName  at  <*> 
"<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: org.apache.hadoop.hdfs.protocol.LocatedBlock addBlock(java.lang.String,java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],long,java.lang.String[])>",debug,*BLOCK* NameNode.addBlock: file  src  fileId= fileId  for  clientName 
"<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: org.apache.hadoop.hdfs.protocol.HdfsFileStatus create(java.lang.String,org.apache.hadoop.fs.permission.FsPermission,java.lang.String,org.apache.hadoop.io.EnumSetWritable,boolean,short,long,org.apache.hadoop.crypto.CryptoProtocolVersion[])>",debug,*DIR* NameNode.create: file  src  for  clientName  at  <*> 
<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: org.apache.hadoop.hdfs.inotify.EventBatchList getEditsFromTxid(long)>,info,NN is transitioning from active to standby and FSEditLog is closed -- could not read edits
<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: java.lang.String[] getGroupsForUser(java.lang.String)>,debug,Getting groups for user  user 
"<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: boolean rename(java.lang.String,java.lang.String)>",debug,*DIR* NameNode.rename:  src  to  dst 
"<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: boolean mkdirs(java.lang.String,org.apache.hadoop.fs.permission.FsPermission,boolean)>",debug,*DIR* NameNode.mkdirs:  src 
"<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: boolean delete(java.lang.String,boolean)>",debug,"*DIR* Namenode.delete: src= src , recursive= recursive "
"<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: boolean complete(java.lang.String,java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,long)>",debug,*DIR* NameNode.complete:  src  fileId= fileId  for  clientName 
<org.apache.hadoop.hdfs.server.namenode.NameNodeResourceChecker: java.util.Collection getVolumesLowOnSpace()>,debug,Going to check the following volumes disk space:  <*> 
<org.apache.hadoop.hdfs.server.namenode.NameNodeResourceChecker$CheckedVolume: boolean isResourceAvailable()>,debug,Space available on volume \' <*> \' is  <*> 
<org.apache.hadoop.hdfs.server.namenode.NameNodeResourceChecker$CheckedVolume: boolean isResourceAvailable()>,warn,"Space available on volume \' <*> \' is  <*> , which is below the configured reserved amount  <*> "
<org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer: void initWebHdfs(org.apache.hadoop.conf.Configuration)>,info,Added filter \' <*> \' (class= <*> ) 
<org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer: java.util.Map getAuthFilterParams(org.apache.hadoop.conf.Configuration)>,error,"WebHDFS and security are enabled, but configuration property \'dfs.web.authentication.kerberos.principal\' is not set."
<org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer: java.util.Map getAuthFilterParams(org.apache.hadoop.conf.Configuration)>,error,"WebHDFS and security are enabled, but configuration property \'dfs.web.authentication.kerberos.keytab\' is not set."
<org.apache.hadoop.hdfs.server.namenode.NamenodeFsck: void lostFoundInit(org.apache.hadoop.hdfs.DFSClient)>,warn,Cannot use /lost+found : a regular file with this name exists.
<org.apache.hadoop.hdfs.server.namenode.NamenodeFsck: void lostFoundInit(org.apache.hadoop.hdfs.DFSClient)>,warn,Cannot initialize /lost+found .
<org.apache.hadoop.hdfs.server.namenode.NamenodeFsck: void fsck()>,info,FSCK started by  <*>  from  <*>  for path  <*>  at  <*> 
<org.apache.hadoop.hdfs.server.namenode.NamenodeFsck: void fsck()>,warn,Fsck on path \' <*> \'  FAILED 
<org.apache.hadoop.hdfs.server.namenode.NamenodeFsck: void deleteCorruptedFile(java.lang.String)>,error,Fsck: error deleting corrupted file  path 
<org.apache.hadoop.hdfs.server.namenode.NamenodeFsck: void deleteCorruptedFile(java.lang.String)>,info,Fsck: deleted corrupt file  path 
"<org.apache.hadoop.hdfs.server.namenode.NamenodeFsck: void copyBlocksToLostFound(java.lang.String,org.apache.hadoop.hdfs.protocol.HdfsFileStatus,org.apache.hadoop.hdfs.protocol.LocatedBlocks)>",error,Fsck: could not copy block  <*>  to  <*> 
"<org.apache.hadoop.hdfs.server.namenode.NamenodeFsck: void copyBlocksToLostFound(java.lang.String,org.apache.hadoop.hdfs.protocol.HdfsFileStatus,org.apache.hadoop.hdfs.protocol.LocatedBlocks)>",error,copyBlocksToLostFound: error processing  <*> 
"<org.apache.hadoop.hdfs.server.namenode.NamenodeFsck: void copyBlocksToLostFound(java.lang.String,org.apache.hadoop.hdfs.protocol.HdfsFileStatus,org.apache.hadoop.hdfs.protocol.LocatedBlocks)>",info,Fsck: copied the remains of the corrupted file  <*>  to /lost+found 
"<org.apache.hadoop.hdfs.server.namenode.NamenodeFsck: void copyBlocksToLostFound(java.lang.String,org.apache.hadoop.hdfs.protocol.HdfsFileStatus,org.apache.hadoop.hdfs.protocol.LocatedBlocks)>",warn,"Fsck: can\'t copy the remains of  <*>  to  lost+found, because  <*>  already exists. "
"<org.apache.hadoop.hdfs.server.namenode.NamenodeFsck: void copyBlocksToLostFound(java.lang.String,org.apache.hadoop.hdfs.protocol.HdfsFileStatus,org.apache.hadoop.hdfs.protocol.LocatedBlocks)>",warn,Fsck: there were errors copying the remains of the corrupted file  <*>  to /lost+found 
"<org.apache.hadoop.hdfs.server.namenode.NamenodeFsck: void copyBlock(org.apache.hadoop.hdfs.DFSClient,org.apache.hadoop.hdfs.protocol.LocatedBlock,java.io.OutputStream)>",error,Error reading block
"<org.apache.hadoop.hdfs.server.namenode.NamenodeFsck: void copyBlock(org.apache.hadoop.hdfs.DFSClient,org.apache.hadoop.hdfs.protocol.LocatedBlock,java.io.OutputStream)>",info,Could not obtain block from any node:   <*> 
"<org.apache.hadoop.hdfs.server.namenode.NamenodeFsck: void copyBlock(org.apache.hadoop.hdfs.DFSClient,org.apache.hadoop.hdfs.protocol.LocatedBlock,java.io.OutputStream)>",info,Failed to connect to  <*> : <*> 
"<org.apache.hadoop.hdfs.server.namenode.NamenodeFsck: void check(java.lang.String,org.apache.hadoop.hdfs.protocol.HdfsFileStatus,org.apache.hadoop.hdfs.server.namenode.NamenodeFsck$Result)>",info,Fsck: ignoring open file  <*> 
<org.apache.hadoop.hdfs.server.namenode.NameNode: void stopHttpServer()>,error,Exception while stopping httpserver
<org.apache.hadoop.hdfs.server.namenode.NameNode: void stopCommonServices()>,warn,ServicePlugin  p  could not be stopped 
<org.apache.hadoop.hdfs.server.namenode.NameNode: void stop()>,warn,Encountered exception while exiting state 
<org.apache.hadoop.hdfs.server.namenode.NameNode: void startCommonServices(org.apache.hadoop.conf.Configuration)>,info,<*>  RPC up at:  <*> 
<org.apache.hadoop.hdfs.server.namenode.NameNode: void startCommonServices(org.apache.hadoop.conf.Configuration)>,info,<*>  service RPC up at:  <*> 
<org.apache.hadoop.hdfs.server.namenode.NameNode: void startCommonServices(org.apache.hadoop.conf.Configuration)>,warn,ServicePlugin  p  could not be started 
"<org.apache.hadoop.hdfs.server.namenode.NameNode: void setServiceAddress(org.apache.hadoop.conf.Configuration,java.lang.String)>",info,Setting ADDRESS  address 
<org.apache.hadoop.hdfs.server.namenode.NameNode: void setClientNamenodeAddress(org.apache.hadoop.conf.Configuration)>,info,fs.defaultFS is  <*> 
<org.apache.hadoop.hdfs.server.namenode.NameNode: void setClientNamenodeAddress(org.apache.hadoop.conf.Configuration)>,info,Clients are to use  <*>  to access  this namenode/service. 
<org.apache.hadoop.hdfs.server.namenode.NameNode: void main(java.lang.String[])>,fatal,Failed to start namenode.
<org.apache.hadoop.hdfs.server.namenode.NameNode: void join()>,info,Caught interrupted exception 
"<org.apache.hadoop.hdfs.server.namenode.NameNode: void initializeGenericKeys(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)>",debug,Setting fs.defaultFS to  <*> 
<org.apache.hadoop.hdfs.server.namenode.NameNode: void initialize(org.apache.hadoop.conf.Configuration)>,info,Clients are to use  <*>  to access  this namenode/service. 
"<org.apache.hadoop.hdfs.server.namenode.NameNode: void doRecovery(org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.conf.Configuration)>",info,starting recovery...
"<org.apache.hadoop.hdfs.server.namenode.NameNode: void doRecovery(org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.conf.Configuration)>",info,RECOVERY COMPLETE
"<org.apache.hadoop.hdfs.server.namenode.NameNode: void doRecovery(org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.conf.Configuration)>",info,RECOVERY FAILED: caught exception
"<org.apache.hadoop.hdfs.server.namenode.NameNode: void doRecovery(org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.conf.Configuration)>",info,RECOVERY FAILED: caught exception
<org.apache.hadoop.hdfs.server.namenode.NameNode: void doImmediateShutdown(java.lang.Throwable)>,fatal,Error encountered requiring NN shutdown. Shutting down immediately.
"<org.apache.hadoop.hdfs.server.namenode.NameNode: void copyEditLogSegmentsToSharedDir(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,java.util.Collection,org.apache.hadoop.hdfs.server.namenode.NNStorage,org.apache.hadoop.conf.Configuration)>",closeAllStreams,<*> L
"<org.apache.hadoop.hdfs.server.namenode.NameNode: void copyEditLogSegmentsToSharedDir(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,java.util.Collection,org.apache.hadoop.hdfs.server.namenode.NNStorage,org.apache.hadoop.conf.Configuration)>",debug,Beginning to copy stream  stream  to shared edits 
"<org.apache.hadoop.hdfs.server.namenode.NameNode: void copyEditLogSegmentsToSharedDir(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,java.util.Collection,org.apache.hadoop.hdfs.server.namenode.NNStorage,org.apache.hadoop.conf.Configuration)>",debug,ending log segment because of END_LOG_SEGMENT op in  stream 
"<org.apache.hadoop.hdfs.server.namenode.NameNode: void copyEditLogSegmentsToSharedDir(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,java.util.Collection,org.apache.hadoop.hdfs.server.namenode.NNStorage,org.apache.hadoop.conf.Configuration)>",debug,ending log segment because of end of stream in  stream 
"<org.apache.hadoop.hdfs.server.namenode.NameNode: void copyEditLogSegmentsToSharedDir(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,java.util.Collection,org.apache.hadoop.hdfs.server.namenode.NNStorage,org.apache.hadoop.conf.Configuration)>",trace,copying op:  <*> 
<org.apache.hadoop.hdfs.server.namenode.NameNode: void checkHaStateChange(org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo)>,warn,"Allowing manual HA control from  <*>  even though automatic HA is enabled, because the user  specified the force flag "
"<org.apache.hadoop.hdfs.server.namenode.NameNode: org.apache.hadoop.hdfs.server.namenode.NameNode createNameNode(java.lang.String[],org.apache.hadoop.conf.Configuration)>",info,createNameNode  <*> 
<org.apache.hadoop.hdfs.server.namenode.NameNode: org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption parseArguments(java.lang.String[])>,fatal,Must specify a valid cluster ID after the  <*>  flag 
<org.apache.hadoop.hdfs.server.namenode.NameNode: org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption parseArguments(java.lang.String[])>,fatal,Must specify a valid cluster ID after the  <*>  flag 
<org.apache.hadoop.hdfs.server.namenode.NameNode: org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption parseArguments(java.lang.String[])>,fatal,Must specify a valid cluster ID after the  <*>  flag 
<org.apache.hadoop.hdfs.server.namenode.NameNode: org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption parseArguments(java.lang.String[])>,fatal,Unknown upgrade flag  clusterId 
<org.apache.hadoop.hdfs.server.namenode.NameNode: org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption parseArguments(java.lang.String[])>,fatal,Must specify a rolling upgrade startup option  <*> 
<org.apache.hadoop.hdfs.server.namenode.NameNode: org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption parseArguments(java.lang.String[])>,fatal,Invalid argument:  <*> 
"<org.apache.hadoop.hdfs.server.namenode.NameNode: boolean initializeSharedEdits(org.apache.hadoop.conf.Configuration,boolean,boolean)>",error,Could not initialize shared edits dir
"<org.apache.hadoop.hdfs.server.namenode.NameNode: boolean initializeSharedEdits(org.apache.hadoop.conf.Configuration,boolean,boolean)>",fatal,No shared edits directory configured for namespace  <*>  namenode  <*> 
"<org.apache.hadoop.hdfs.server.namenode.NameNode: boolean initializeSharedEdits(org.apache.hadoop.conf.Configuration,boolean,boolean)>",warn,Could not close sharedEditsImage
"<org.apache.hadoop.hdfs.server.namenode.NameNode: boolean initializeSharedEdits(org.apache.hadoop.conf.Configuration,boolean,boolean)>",warn,Could not unlock storage directories
"<org.apache.hadoop.hdfs.server.namenode.NameNode: boolean initializeSharedEdits(org.apache.hadoop.conf.Configuration,boolean,boolean)>",warn,Could not close sharedEditsImage
"<org.apache.hadoop.hdfs.server.namenode.NameNode: boolean initializeSharedEdits(org.apache.hadoop.conf.Configuration,boolean,boolean)>",warn,Could not unlock storage directories
"<org.apache.hadoop.hdfs.server.namenode.NameNode: boolean initializeSharedEdits(org.apache.hadoop.conf.Configuration,boolean,boolean)>",warn,Could not close sharedEditsImage
"<org.apache.hadoop.hdfs.server.namenode.NameNode: boolean initializeSharedEdits(org.apache.hadoop.conf.Configuration,boolean,boolean)>",warn,Could not unlock storage directories
"<org.apache.hadoop.hdfs.server.namenode.NameNode: boolean initializeSharedEdits(org.apache.hadoop.conf.Configuration,boolean,boolean)>",warn,Could not close sharedEditsImage
"<org.apache.hadoop.hdfs.server.namenode.NameNode: boolean initializeSharedEdits(org.apache.hadoop.conf.Configuration,boolean,boolean)>",warn,Could not unlock storage directories
"<org.apache.hadoop.hdfs.server.namenode.NameNode: boolean format(org.apache.hadoop.conf.Configuration,boolean,boolean)>",warn,Encountered exception during format: 
<org.apache.hadoop.hdfs.server.namenode.NameCache: void initialized()>,info,initialized with  <*>  entries  <*>  lookups 
<org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext: void quit()>,error,Exiting on user request.
"<org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext: void editLogLoaderPrompt(java.lang.String,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext,java.lang.String)>",info,Continuing
"<org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext: java.lang.String ask(java.lang.String,java.lang.String,java.lang.String[])>",error,"I\'m sorry, I cannot understand your response.\n"
"<org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext: java.lang.String ask(java.lang.String,java.lang.String,java.lang.String[])>",info,automatically choosing  firstChoice 
"<org.apache.hadoop.hdfs.server.namenode.ListPathsServlet: void doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)>",warn,ListPathServlet encountered InterruptedException
<org.apache.hadoop.hdfs.server.namenode.ListPathsServlet$2: java.lang.Void run()>,warn,ListPathsServlet - Path  p  does not exist 
<org.apache.hadoop.hdfs.server.namenode.LeaseManager: void stopMonitor()>,warn,Encountered exception 
<org.apache.hadoop.hdfs.server.namenode.LeaseManager: void removeLeaseWithPrefixPath(java.lang.String)>,debug,<*> .removeLeaseWithPrefixPath: entry= entry 
"<org.apache.hadoop.hdfs.server.namenode.LeaseManager: void removeLease(org.apache.hadoop.hdfs.server.namenode.LeaseManager$Lease,java.lang.String)>",debug,src  not found in lease.paths (= <*> ) 
"<org.apache.hadoop.hdfs.server.namenode.LeaseManager: void removeLease(org.apache.hadoop.hdfs.server.namenode.LeaseManager$Lease,java.lang.String)>",error,lease  not found in sortedLeases 
"<org.apache.hadoop.hdfs.server.namenode.LeaseManager: void removeLease(java.lang.String,java.lang.String)>",warn,Removing non-existent lease! holder= holder  src= src 
"<org.apache.hadoop.hdfs.server.namenode.LeaseManager: void changeLease(java.lang.String,java.lang.String)>",debug,"<*> .changelease:   src= src , dest= dst "
"<org.apache.hadoop.hdfs.server.namenode.LeaseManager: void changeLease(java.lang.String,java.lang.String)>",debug,changeLease: replacing  oldpath  with  <*> 
<org.apache.hadoop.hdfs.server.namenode.LeaseManager: long getNumUnderConstructionBlocks()>,info,Number of blocks under construction:  numUCBlocks 
<org.apache.hadoop.hdfs.server.namenode.LeaseManager: long getNumUnderConstructionBlocks()>,warn,The file  <*>  is not under construction but has lease. 
<org.apache.hadoop.hdfs.server.namenode.LeaseManager: java.util.Map getINodesUnderConstruction()>,warn,Ignore the lease of file  p  for checkpoint since the file is not under construction 
"<org.apache.hadoop.hdfs.server.namenode.LeaseManager: java.util.Map findLeaseWithPrefixPath(java.lang.String,java.util.SortedMap)>",debug,<*> .findLease: prefix= prefix 
<org.apache.hadoop.hdfs.server.namenode.LeaseManager: boolean checkLeases()>,debug,Lease recovery for  p  is complete. File closed. 
<org.apache.hadoop.hdfs.server.namenode.LeaseManager: boolean checkLeases()>,debug,Started block recovery  p  lease  leaseToCheck 
<org.apache.hadoop.hdfs.server.namenode.LeaseManager: boolean checkLeases()>,error,Cannot release the path  p  in the lease  leaseToCheck 
<org.apache.hadoop.hdfs.server.namenode.LeaseManager: boolean checkLeases()>,info,leaseToCheck  has expired hard limit 
<org.apache.hadoop.hdfs.server.namenode.LeaseManager: boolean checkLeases()>,warn,Unable to release hard-limit expired lease:  <*> 
<org.apache.hadoop.hdfs.server.namenode.LeaseManager$Monitor: void run()>,debug,<*>  is interrupted 
<org.apache.hadoop.hdfs.server.namenode.JournalSet: void setOutputBufferCapacity(int)>,error,Error in setting outputbuffer capacity
"<org.apache.hadoop.hdfs.server.namenode.JournalSet: void selectInputStreams(java.util.Collection,long,boolean)>",info,Skipping jas  jas  since it\'s disabled 
"<org.apache.hadoop.hdfs.server.namenode.JournalSet: void selectInputStreams(java.util.Collection,long,boolean)>",warn,Unable to determine input streams from  <*> . Skipping. 
"<org.apache.hadoop.hdfs.server.namenode.JournalSet: void mapJournalsAndReportErrors(org.apache.hadoop.hdfs.server.namenode.JournalSet$JournalClosure,java.lang.String)>",error,Error:  status  failed for (journal  jas ) 
"<org.apache.hadoop.hdfs.server.namenode.JournalSet: void mapJournalsAndReportErrors(org.apache.hadoop.hdfs.server.namenode.JournalSet$JournalClosure,java.lang.String)>",error,Error:  <*> 
"<org.apache.hadoop.hdfs.server.namenode.JournalSet: void mapJournalsAndReportErrors(org.apache.hadoop.hdfs.server.namenode.JournalSet$JournalClosure,java.lang.String)>",fatal,Error:  status  failed for required journal ( jas ) 
<org.apache.hadoop.hdfs.server.namenode.JournalSet: void disableAndReportErrorOnJournals(java.util.List)>,error,Disabling journal  j 
<org.apache.hadoop.hdfs.server.namenode.JournalSet: org.apache.hadoop.hdfs.server.protocol.RemoteEditLogManifest getEditLogManifest(long)>,debug,Found gap in logs at  j# :  not returning previous logs in manifest. 
<org.apache.hadoop.hdfs.server.namenode.JournalSet: org.apache.hadoop.hdfs.server.protocol.RemoteEditLogManifest getEditLogManifest(long)>,debug,Generated manifest for logs since  fromTxId : $u 
<org.apache.hadoop.hdfs.server.namenode.JournalSet: org.apache.hadoop.hdfs.server.protocol.RemoteEditLogManifest getEditLogManifest(long)>,warn,Cannot list edit logs in  fjm 
<org.apache.hadoop.hdfs.server.namenode.JournalSet$JournalAndStream: void abort()>,error,Unable to abort stream  <*> 
"<org.apache.hadoop.hdfs.server.namenode.INodesInPath: org.apache.hadoop.hdfs.server.namenode.INodesInPath resolve(org.apache.hadoop.hdfs.server.namenode.INodeDirectory,byte[][],int,boolean)>",debug,UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
"<org.apache.hadoop.hdfs.server.namenode.INodeReference$WithName: void destroyAndCollectBlocks(org.apache.hadoop.hdfs.server.namenode.INode$BlocksMapUpdateInfo,java.util.List)>",error,should not exceed quota while snapshot deletion
"<org.apache.hadoop.hdfs.server.namenode.INodeReference$DstReference: void destroyAndCollectBlocks(org.apache.hadoop.hdfs.server.namenode.INode$BlocksMapUpdateInfo,java.util.List)>",error,should not exceed quota while snapshot deletion
"<org.apache.hadoop.hdfs.server.namenode.INodeReference$DstReference: void destroyAndCollectBlocks(org.apache.hadoop.hdfs.server.namenode.INode$BlocksMapUpdateInfo,java.util.List)>",error,should not exceed quota while snapshot deletion
"<org.apache.hadoop.hdfs.server.namenode.ImageServlet: void validateRequest(javax.servlet.ServletContext,org.apache.hadoop.conf.Configuration,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,org.apache.hadoop.hdfs.server.namenode.FSImage,java.lang.String)>",warn,Received non-NN/SNN/administrator request for image or edits from  <*>  at  <*> 
"<org.apache.hadoop.hdfs.server.namenode.ImageServlet: void validateRequest(javax.servlet.ServletContext,org.apache.hadoop.conf.Configuration,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,org.apache.hadoop.hdfs.server.namenode.FSImage,java.lang.String)>",warn,Received an invalid request file transfer request from a secondary with storage info  theirStorageInfoString 
"<org.apache.hadoop.hdfs.server.namenode.ImageServlet: boolean isValidRequestor(javax.servlet.ServletContext,java.lang.String,org.apache.hadoop.conf.Configuration)>",debug,SecondaryNameNode principal could not be added
"<org.apache.hadoop.hdfs.server.namenode.ImageServlet: boolean isValidRequestor(javax.servlet.ServletContext,java.lang.String,org.apache.hadoop.conf.Configuration)>",info,ImageServlet allowing checkpointer:  remoteUser 
"<org.apache.hadoop.hdfs.server.namenode.ImageServlet: boolean isValidRequestor(javax.servlet.ServletContext,java.lang.String,org.apache.hadoop.conf.Configuration)>",info,ImageServlet allowing administrator:  remoteUser 
"<org.apache.hadoop.hdfs.server.namenode.ImageServlet: boolean isValidRequestor(javax.servlet.ServletContext,java.lang.String,org.apache.hadoop.conf.Configuration)>",info,ImageServlet rejecting:  remoteUser 
"<org.apache.hadoop.hdfs.server.namenode.ImageServlet: boolean isValidRequestor(javax.servlet.ServletContext,java.lang.String,org.apache.hadoop.conf.Configuration)>",warn,Received null remoteUser while authorizing access to getImage servlet
<org.apache.hadoop.hdfs.server.namenode.ha.StandbyCheckpointer: void stop()>,warn,Edit log tailer thread exited with an exception
<org.apache.hadoop.hdfs.server.namenode.ha.StandbyCheckpointer: void start()>,info,Starting standby checkpoint thread...\nCheckpointing active NN at  <*> \n Serving checkpoints at  <*> 
<org.apache.hadoop.hdfs.server.namenode.ha.StandbyCheckpointer: void doCheckpoint()>,<init>,"Standby Checkpointer should only attempt a checkpoint when NN is in standby mode, but the edit logs are in an unexpected state"
<org.apache.hadoop.hdfs.server.namenode.ha.StandbyCheckpointer: void doCheckpoint()>,info,A checkpoint was triggered but the Standby Node has not received any transactions since the last checkpoint at txid  <*> . Skipping... 
<org.apache.hadoop.hdfs.server.namenode.ha.StandbyCheckpointer$CheckpointerThread: void doWork()>,error,Exception in doCheckpoint
<org.apache.hadoop.hdfs.server.namenode.ha.StandbyCheckpointer$CheckpointerThread: void doWork()>,info,Triggering a rollback fsimage for rolling upgrade.
<org.apache.hadoop.hdfs.server.namenode.ha.StandbyCheckpointer$CheckpointerThread: void doWork()>,info,"Triggering checkpoint because there have been  <*>  txns since the last checkpoint, which  exceeds the configured threshold  <*> "
<org.apache.hadoop.hdfs.server.namenode.ha.StandbyCheckpointer$CheckpointerThread: void doWork()>,info,"Triggering checkpoint because it has been  <*>  seconds since the last checkpoint, which  exceeds the configured interval  <*> "
<org.apache.hadoop.hdfs.server.namenode.ha.StandbyCheckpointer$CheckpointerThread: void doWork()>,info,But skipping this checkpoint since we are about to failover!
<org.apache.hadoop.hdfs.server.namenode.ha.StandbyCheckpointer$CheckpointerThread: void doWork()>,info,Checkpoint was cancelled:  <*> 
<org.apache.hadoop.hdfs.server.namenode.ha.StandbyCheckpointer$CheckpointerThread: void doWork()>,info,Interrupted during checkpointing
<org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: void triggerActiveLogRoll()>,info,Triggering log roll on remote NameNode  <*> 
<org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: void triggerActiveLogRoll()>,warn,Unable to trigger a roll of the active NN
<org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: void stop()>,warn,Edit log tailer thread exited with an exception
<org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: void doTailEdits()>,debug,lastTxnId:  <*> 
<org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: void doTailEdits()>,debug,edit streams to load from:  <*> 
<org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: void doTailEdits()>,warn,Edits tailer failed to find any streams. Will try again later.
"<org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: void <init>(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.conf.Configuration)>",debug,logRollPeriodMs= <*>  sleepTime= <*> 
"<org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: void <init>(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.conf.Configuration)>",info,Will roll logs on active node at  <*>  every  <*>  seconds. 
"<org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: void <init>(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.conf.Configuration)>",info,Not going to trigger log rolls on active node because dfs.ha.log-roll.period is negative.
<org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread: void doWork()>,fatal,Unknown error encountered while tailing edits. Shutting down standby NN.
<org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread: void doWork()>,warn,Error while reading edits from disk. Will try again.
<org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread: void doWork()>,warn,Edit log tailer interrupted
<org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider: org.apache.hadoop.io.retry.FailoverProxyProvider$ProxyInfo getProxy()>,error,Failed to create RPC proxy to NameNode
<org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby: int doRun()>,debug,Full exception trace
<org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby: int doRun()>,fatal,Unable to fetch namespace information from active NN at  <*> :  <*> 
<org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby: int doRun()>,fatal,Layout version on remote node ( <*> ) does not match  this node\'s layout version ( <*> ) 
<org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby: int doRun()>,info,The active NameNode is in Upgrade. Prepare the upgrade for the standby NameNode as well.
"<org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby: boolean doPreUpgrade(org.apache.hadoop.hdfs.server.namenode.NNStorage,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo)>",error,Failed to move aside pre-upgrade storage in image directory  <*> 
"<org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby: boolean doPreUpgrade(org.apache.hadoop.hdfs.server.namenode.NNStorage,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo)>",warn,The storage directory is in an inconsistent state
"<org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby: boolean checkLogsAvailableForRead(org.apache.hadoop.hdfs.server.namenode.FSImage,long,long)>",fatal,Unable to read transaction ids  firstTxIdInLogs - curTxIdOnOtherNode  from the configured shared edits storage  <*> .  Please copy these logs into the shared edits storage  or call saveNamespace on the active node.\n Error:  <*> 
"<org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby: boolean checkLogsAvailableForRead(org.apache.hadoop.hdfs.server.namenode.FSImage,long,long)>",fatal,Unable to read transaction ids  firstTxIdInLogs - curTxIdOnOtherNode  from the configured shared edits storage  <*> .  Please copy these logs into the shared edits storage  or call saveNamespace on the active node.\n Error:  <*> 
"<org.apache.hadoop.hdfs.server.namenode.GetDelegationTokenServlet: void doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)>",info,Request for token received with no authentication from  <*> 
"<org.apache.hadoop.hdfs.server.namenode.GetDelegationTokenServlet: void doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)>",info,"Sending token: { <*> , <*> } "
"<org.apache.hadoop.hdfs.server.namenode.GetDelegationTokenServlet: void doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)>",info,Exception while sending token. Re-throwing 
"<org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker: void checkPermission(java.lang.String,org.apache.hadoop.hdfs.server.namenode.FSDirectory,boolean,org.apache.hadoop.fs.permission.FsAction,org.apache.hadoop.fs.permission.FsAction,org.apache.hadoop.fs.permission.FsAction,org.apache.hadoop.fs.permission.FsAction,boolean,boolean)>",debug,"ACCESS CHECK:  this , doCheckOwner= doCheckOwner , ancestorAccess= ancestorAccess , parentAccess= parentAccess , access= access , subAccess= subAccess , ignoreEmptyDir= ignoreEmptyDir , resolveLink= resolveLink "
"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void updatePipelineInternal(java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.DatanodeID[],java.lang.String[],boolean)>",warn,Update  oldBlock  (len =  <*> ) to an older state:  newBlock  (len =  <*> ) 
"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void updatePipeline(java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.DatanodeID[],java.lang.String[])>",info,"updatePipeline(block= oldBlock , newGenerationStamp= <*> , newLength= <*> , newNodes= <*> , clientName= clientName ) "
"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void updatePipeline(java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.DatanodeID[],java.lang.String[])>",info,updatePipeline( oldBlock ) successfully to  newBlock 
<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void stopStandbyServices()>,info,Stopping services started for standby state
<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void stopActiveServices()>,info,Stopping services started for active state
<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void startStandbyServices(org.apache.hadoop.conf.Configuration)>,info,Starting services required for standby state
<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void startRollingUpgradeInternalForNonHA(long)>,info,Successfully saved namespace for preparing rolling upgrade.
<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void startActiveServices()>,debug,NameNode metadata after re-processing replication and invalidation queues during failover:\n <*> 
<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void startActiveServices()>,info,Starting services required for active state
<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void startActiveServices()>,info,Catching up to latest edits from old active before taking over writer role in edits logs
<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void startActiveServices()>,info,Reprocessing replication and invalidation queues
<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void startActiveServices()>,info,Will take over writing edit logs at txnid  <*> 
<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void saveNamespace()>,info,New namespace image has been created
<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void reportBadBlocks(org.apache.hadoop.hdfs.protocol.LocatedBlock[])>,info,*DIR* reportBadBlocks
"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void renameTo(java.lang.String,java.lang.String,org.apache.hadoop.fs.Options$Rename[])>",debug,DIR* NameSystem.renameTo: with options -  srcArg  to  dstArg 
<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void removeBlocksAndUpdateSafemodeTotal(org.apache.hadoop.hdfs.server.namenode.INode$BlocksMapUpdateInfo)>,debug,"Adjusting safe-mode totals for deletion.decreasing safeBlocks by  numRemovedSafe , totalBlocks by  numRemovedComplete "
<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void registerMBean()>,info,Registered FSNamesystemState MBean
"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void recoverLeaseInternal(org.apache.hadoop.hdfs.server.namenode.INodeFile,java.lang.String,java.lang.String,java.lang.String,boolean)>",info,"recoverLease:  <*> , src= src  from client  <*> "
"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void recoverLeaseInternal(org.apache.hadoop.hdfs.server.namenode.INodeFile,java.lang.String,java.lang.String,java.lang.String,boolean)>",info,"startFile: recover  <*> , src= src  client  <*> "
"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void persistNewBlock(java.lang.String,org.apache.hadoop.hdfs.server.namenode.INodeFile)>",debug,"persistNewBlock:  path  with new block  <*> , current total block count is  <*> "
"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void persistBlocks(java.lang.String,org.apache.hadoop.hdfs.server.namenode.INodeFile,boolean)>",debug,persistBlocks:  path  with  <*>  blocks is persisted to  the file system 
<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void loadFSImage(org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption)>,info,"Need to save fs image?  <*>#_  (staleImage= <*> , haEnabled= <*> , isRollingUpgrade= <*> ) "
<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void leaveSafeMode()>,info,STATE* Safe mode is already OFF
<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void initializeReplQueues()>,info,initializing replication queues
"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void fsync(java.lang.String,long,java.lang.String,long)>",info,BLOCK* fsync:  src  for  clientName 
<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void finalizeRollingUpgrade()>,logFinalizeRollingUpgrade,<*> 
<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void enterSafeMode(boolean)>,info,STATE* Safe mode is ON <*> 
"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void endCheckpoint(org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration,org.apache.hadoop.hdfs.server.namenode.CheckpointSignature)>",info,End checkpoint for  <*> 
<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void enableAsyncAuditLog()>,warn,Logj is required to enable async auditlog
"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void createSymlinkInt(java.lang.String,java.lang.String,org.apache.hadoop.fs.permission.PermissionStatus,boolean,boolean)>",debug,DIR* NameSystem.createSymlink: target= target  link= linkArg 
"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void concatInternal(org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker,java.lang.String,java.lang.String[],boolean)>",debug,DIR* NameSystem.concat:  <*>  to  target 
"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void concat(java.lang.String,java.lang.String[])>",debug,concat  <*>  to  target 
"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void commitOrCompleteLastBlock(org.apache.hadoop.hdfs.server.namenode.INodeFile,org.apache.hadoop.hdfs.protocol.Block)>",warn,Unexpected exception while updating disk space.
"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void commitBlockSynchronization(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,long,boolean,boolean,org.apache.hadoop.hdfs.protocol.DatanodeID[],java.lang.String[])>",debug,Block (= lastblock ) not found 
"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void commitBlockSynchronization(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,long,boolean,boolean,org.apache.hadoop.hdfs.protocol.DatanodeID[],java.lang.String[])>",debug,Unexpected block (= lastblock ) since the file (= <*> ) is not under construction 
"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void commitBlockSynchronization(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,long,boolean,boolean,org.apache.hadoop.hdfs.protocol.DatanodeID[],java.lang.String[])>",debug,DatanodeDescriptor (= <*> ) not found 
"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void commitBlockSynchronization(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,long,boolean,boolean,org.apache.hadoop.hdfs.protocol.DatanodeID[],java.lang.String[])>",info,"commitBlockSynchronization(lastblock= lastblock , newgenerationstamp= newgenerationstamp , newlength= newlength , newtargets= <*> , closeFile= closeFile , deleteBlock= deleteblock ) "
"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void commitBlockSynchronization(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,long,boolean,boolean,org.apache.hadoop.hdfs.protocol.DatanodeID[],java.lang.String[])>",info,"commitBlockSynchronization(newblock= lastblock , file= src , newgenerationstamp= newgenerationstamp , newlength= newlength , newtargets= <*> ) successful "
"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void commitBlockSynchronization(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,long,boolean,boolean,org.apache.hadoop.hdfs.protocol.DatanodeID[],java.lang.String[])>",info,commitBlockSynchronization( lastblock ) successful 
"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void closeFile(java.lang.String,org.apache.hadoop.hdfs.server.namenode.INodeFile)>",debug,closeFile:  path  with  <*>  blocks is persisted to the file system 
<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void checkConfiguration(org.apache.hadoop.conf.Configuration)>,warn,Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void checkConfiguration(org.apache.hadoop.conf.Configuration)>,warn,Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.namenode.FSImage,boolean)>",error,<*>  initialization failed. 
"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.namenode.FSImage,boolean)>",error,<*>  initialization failed. 
"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.namenode.FSImage,boolean)>",info,No KeyProvider found.
"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.namenode.FSImage,boolean)>",info,Found KeyProvider:  <*> 
"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.namenode.FSImage,boolean)>",info,Enabling async auditlog
"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.namenode.FSImage,boolean)>",info,fsLock is fair: <*> 
"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.namenode.FSImage,boolean)>",info,fsOwner             =  <*> 
"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.namenode.FSImage,boolean)>",info,supergroup          =  <*> 
"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.namenode.FSImage,boolean)>",info,isPermissionEnabled =  <*> 
"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.namenode.FSImage,boolean)>",info,Determined nameservice ID:  <*> 
"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.namenode.FSImage,boolean)>",info,HA Enabled:  <*> 
"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.namenode.FSImage,boolean)>",info,Append Enabled:  <*> 
"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.namenode.FSImage,boolean)>",warn,Configured NNs:\n <*> 
<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: org.apache.hadoop.security.token.Token getDelegationToken(org.apache.hadoop.io.Text)>,warn,trying to get DT with no secret manager running
<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: org.apache.hadoop.ipc.RetryCache initRetryCache(org.apache.hadoop.conf.Configuration)>,info,Retry cache on namenode is  <*> 
<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: org.apache.hadoop.ipc.RetryCache initRetryCache(org.apache.hadoop.conf.Configuration)>,info,Retry cache will use  <*>  of total heap and retry cache entry expiry time is  <*>  millis 
"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: org.apache.hadoop.hdfs.server.protocol.NamenodeCommand startCheckpoint(org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration,org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration)>",info,Start checkpoint for  <*> 
"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: org.apache.hadoop.hdfs.server.namenode.INodeSymlink addSymlink(java.lang.String,java.lang.String,org.apache.hadoop.fs.permission.PermissionStatus,boolean,boolean)>",debug,addSymlink:  path  is added 
"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: org.apache.hadoop.hdfs.server.namenode.INodeSymlink addSymlink(java.lang.String,java.lang.String,org.apache.hadoop.fs.permission.PermissionStatus,boolean,boolean)>",info,addSymlink: failed to add  path 
"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: org.apache.hadoop.hdfs.server.namenode.INode$BlocksMapUpdateInfo startFileInternal(org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker,java.lang.String,org.apache.hadoop.fs.permission.PermissionStatus,java.lang.String,java.lang.String,boolean,boolean,boolean,short,long,boolean,org.apache.hadoop.crypto.CipherSuite,org.apache.hadoop.crypto.CryptoProtocolVersion,org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion,boolean)>",debug,DIR* NameSystem.startFile: added  src  inode  <*>   holder 
"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: org.apache.hadoop.hdfs.server.namenode.INode$BlocksMapUpdateInfo startFileInternal(org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker,java.lang.String,org.apache.hadoop.fs.permission.PermissionStatus,java.lang.String,java.lang.String,boolean,boolean,boolean,short,long,boolean,org.apache.hadoop.crypto.CipherSuite,org.apache.hadoop.crypto.CryptoProtocolVersion,org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion,boolean)>",warn,DIR* NameSystem.startFile:  src   <*> 
"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: org.apache.hadoop.hdfs.server.namenode.FSNamesystem$FileState analyzeFileState(java.lang.String,long,java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.LocatedBlock[])>",debug,BLOCK* NameSystem.allocateBlock: handling block allocation writing to a file with a complete previous block: src= src  lastBlock= <*> 
"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: org.apache.hadoop.hdfs.server.namenode.FSNamesystem$FileState analyzeFileState(java.lang.String,long,java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.LocatedBlock[])>",info,BLOCK* allocateBlock: caught retry for allocation of a new block in  src . Returning previously allocated block  <*> 
<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: org.apache.hadoop.hdfs.server.namenode.FSNamesystem loadFromDisk(org.apache.hadoop.conf.Configuration)>,info,Finished loading FSImage in  ioe  msecs 
<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: org.apache.hadoop.hdfs.server.namenode.FSNamesystem loadFromDisk(org.apache.hadoop.conf.Configuration)>,warn,Encountered exception loading fsimage
"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo saveAllocatedBlock(java.lang.String,org.apache.hadoop.hdfs.server.namenode.INodesInPath,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo[])>",info,BLOCK* allocateBlock:  src .  <*>   <*> 
<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: org.apache.hadoop.hdfs.protocol.RollingUpgradeInfo$Bean getRollingUpgradeStatus()>,warn,Encountered exception setting Rollback Image
"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: org.apache.hadoop.hdfs.protocol.LocatedBlock getAdditionalBlock(java.lang.String,long,java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,java.util.Set,java.util.List)>",debug,BLOCK* NameSystem.getAdditionalBlock:  src  inodeId  fileId  for  clientName 
"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: org.apache.hadoop.hdfs.protocol.LocatedBlock appendFileInternal(org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker,java.lang.String,java.lang.String,java.lang.String,boolean)>",warn,DIR* NameSystem.append:  <*> 
"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: org.apache.hadoop.hdfs.protocol.LocatedBlock appendFileInt(java.lang.String,java.lang.String,java.lang.String,boolean)>",debug,"DIR* NameSystem.appendFile: src= srcArg , holder= holder , clientMachine= clientMachine "
"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: org.apache.hadoop.hdfs.protocol.LocatedBlock appendFileInt(java.lang.String,java.lang.String,java.lang.String,boolean)>",debug,DIR* NameSystem.appendFile: file  <*>  for  holder  at  clientMachine  block  <*>  block size  <*> 
"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: org.apache.hadoop.crypto.CryptoProtocolVersion chooseProtocolVersion(org.apache.hadoop.hdfs.protocol.EncryptionZone,org.apache.hadoop.crypto.CryptoProtocolVersion[])>",debug,Ignoring unknown CryptoProtocolVersion provided by client:  <*> 
"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: java.util.List getNamespaceEditsDirs(org.apache.hadoop.conf.Configuration,boolean)>",warn,Edits URI  dir  listed multiple times in  dfs.namenode.shared.edits.dir . Ignoring duplicates. 
"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: java.util.List getNamespaceEditsDirs(org.apache.hadoop.conf.Configuration,boolean)>",warn,Edits URI  i$  listed multiple times in  dfs.namenode.shared.edits.dir  and  dfs.namenode.edits.dir . Ignoring duplicates. 
"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: java.util.Collection listCorruptFileBlocks(java.lang.String,java.lang.String[])>",info,there are no corrupt file blocks.
"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: java.util.Collection listCorruptFileBlocks(java.lang.String,java.lang.String[])>",info,list corrupt file blocks returned:  count 
"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: java.util.Collection getStorageDirs(org.apache.hadoop.conf.Configuration,java.lang.String)>",warn,!!! WARNING !!!\n\tThe NameNode currently runs without persistent storage.\n\tAny changes to the file system meta-data may be lost.\n\tRecommended actions:\n\t\t- shutdown and restart NameNode with configured \ propertyName \ in hdfs-site.xml; \n\t\t- use Backup Node as a persistent and up-to-date storage  of the file system meta-data. 
<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: java.lang.String getCorruptFiles()>,warn,Get corrupt file blocks returned error:  <*> 
<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: boolean setSafeMode(org.apache.hadoop.hdfs.protocol.HdfsConstants$SafeModeAction)>,error,Unexpected safe mode action
"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: boolean renameToInt(java.lang.String,java.lang.String,boolean)>",debug,DIR* NameSystem.renameTo:  srcArg  to  dstArg 
"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: boolean mkdirsRecursively(java.lang.String,org.apache.hadoop.fs.permission.PermissionStatus,boolean,long)>",debug,mkdirs: created directory  <*> 
"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: boolean mkdirsInt(java.lang.String,org.apache.hadoop.fs.permission.PermissionStatus,boolean)>",debug,DIR* NameSystem.mkdirs:  srcArg 
<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: boolean isInSnapshot(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoUnderConstruction)>,error,Error while resolving the link :  <*> 
"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: boolean internalReleaseLease(org.apache.hadoop.hdfs.server.namenode.LeaseManager$Lease,java.lang.String,java.lang.String)>",info,"Recovering  lease , src= src "
"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: boolean internalReleaseLease(org.apache.hadoop.hdfs.server.namenode.LeaseManager$Lease,java.lang.String,java.lang.String)>",warn,"BLOCK* internalReleaseLease: All existing blocks are COMPLETE, lease removed, file closed."
"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: boolean internalReleaseLease(org.apache.hadoop.hdfs.server.namenode.LeaseManager$Lease,java.lang.String,java.lang.String)>",warn,DIR* NameSystem.internalReleaseLease: attempt to release a create lock on  src  but file is already closed. 
"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: boolean internalReleaseLease(org.apache.hadoop.hdfs.server.namenode.LeaseManager$Lease,java.lang.String,java.lang.String)>",warn,"BLOCK* internalReleaseLease: Committed blocks are minimally replicated, lease removed, file closed."
"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: boolean internalReleaseLease(org.apache.hadoop.hdfs.server.namenode.LeaseManager$Lease,java.lang.String,java.lang.String)>",warn,DIR* NameSystem.internalReleaseLease: Failed to release lease for file  src . Committed blocks are waiting to be minimally replicated.  Try again later. 
"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: boolean internalReleaseLease(org.apache.hadoop.hdfs.server.namenode.LeaseManager$Lease,java.lang.String,java.lang.String)>",warn,BLOCK* internalReleaseLease: Removed empty last block and closed file.
"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: boolean internalReleaseLease(org.apache.hadoop.hdfs.server.namenode.LeaseManager$Lease,java.lang.String,java.lang.String)>",warn,DIR* NameSystem.internalReleaseLease: File  src  has not been closed.  Lease recovery is in progress.  RecoveryId =  <*>  for block  <*> 
"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: boolean deleteInternal(java.lang.String,boolean,boolean,boolean)>",debug,DIR* Namesystem.delete:  <*>  is removed 
"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: boolean deleteInt(java.lang.String,boolean,boolean)>",debug,DIR* NameSystem.delete:  src 
"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: boolean completeFileInternal(java.lang.String,java.lang.String,org.apache.hadoop.hdfs.protocol.Block,long)>",info,"DIR* completeFile: request from  holder  to complete inode  fileId ( src ) which is already closed. But, it appears to be  an RPC retry. Returning success "
"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: boolean completeFile(java.lang.String,java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,long)>",debug,DIR* NameSystem.completeFile:  srcArg  for  holder 
"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: boolean completeFile(java.lang.String,java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,long)>",info,DIR* completeFile:  srcArg  is closed by  holder 
"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: boolean checkFileProgress(org.apache.hadoop.hdfs.server.namenode.INodeFile,boolean)>",info,BLOCK* checkFileProgress:  block  has not reached minimal replication  $i 
"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: boolean checkFileProgress(org.apache.hadoop.hdfs.server.namenode.INodeFile,boolean)>",warn,BLOCK* checkFileProgress:  <*>  has not reached minimal replication  $i 
"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: boolean abandonBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,java.lang.String,java.lang.String)>",debug,BLOCK* NameSystem.abandonBlock:  b of file  src 
"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: boolean abandonBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,java.lang.String,java.lang.String)>",debug,BLOCK* NameSystem.abandonBlock:  b  is removed from pendingCreates 
<org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeMonitor: void run()>,info,"NameNode is being shutdown, exit SafeModeMonitor thread"
"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: void reportStatus(java.lang.String,boolean)>",info,msg  \n <*> 
<org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: void leave()>,info,STATE* Leaving safe mode after  <*>  secs 
<org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: void leave()>,info,STATE* Safe mode is OFF
<org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: void leave()>,info,STATE* Network topology has  <*>  racks and  <*>  datanodes 
<org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: void leave()>,info,STATE* UnderReplicatedBlocks has  <*>  blocks 
"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: void adjustBlockTotals(int,int)>",debug,Adjusting block totals from  <*> / <*>  to  <*> / <*> 
"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: void <init>(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.conf.Configuration)>",info,dfs.namenode.safemode.threshold-pct =  <*> 
"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: void <init>(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.conf.Configuration)>",info,dfs.namenode.safemode.min.datanodes =  <*> 
"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: void <init>(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.conf.Configuration)>",info,dfs.namenode.safemode.extension     =  <*> 
"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: void <init>(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.conf.Configuration)>",warn,"The threshold value should\'t be greater than , threshold:  <*> "
<org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeResourceMonitor: void run()>,error,Exception in NameNodeResourceMonitor: 
<org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeResourceMonitor: void run()>,warn,NameNode low on available disk space.  Entering safe mode. 
<org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeResourceMonitor: void run()>,warn,NameNode low on available disk space.  Already in safe mode. 
<org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller: void run()>,error,Swallowing exception in  <*> : 
<org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller: void run()>,info,NameNode rolling its own edit log because number of edits in open segment exceeds threshold of  <*> 
<org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller: void run()>,info,"<*>  was interrupted, exiting "
<org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber: void run()>,error,Ignoring exception in LazyPersistFileScrubber:
<org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber: void run()>,info,"LazyPersistFileScrubber was interrupted, exiting"
<org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber: void clearCorruptLazyPersistFiles()>,warn,Removing lazyPersist file  <*>  with no replicas. 
<org.apache.hadoop.hdfs.server.namenode.FSImageTransactionalStorageInspector: void inspectDirectory(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory)>,debug,Checking file  f 
<org.apache.hadoop.hdfs.server.namenode.FSImageTransactionalStorageInspector: void inspectDirectory(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory)>,error,Image file  f  has improperly formatted  transaction ID 
<org.apache.hadoop.hdfs.server.namenode.FSImageTransactionalStorageInspector: void inspectDirectory(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory)>,info,No version file in  <*> 
<org.apache.hadoop.hdfs.server.namenode.FSImageTransactionalStorageInspector: void inspectDirectory(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory)>,warn,Unable to determine the max transaction ID seen by  sd 
<org.apache.hadoop.hdfs.server.namenode.FSImageTransactionalStorageInspector: void inspectDirectory(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory)>,warn,Unable to inspect storage directory  <*> 
<org.apache.hadoop.hdfs.server.namenode.FSImageTransactionalStorageInspector: void inspectDirectory(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory)>,warn,Found image file at  f  but storage directory is  not configured to contain images. 
<org.apache.hadoop.hdfs.server.namenode.FSImagePreTransactionalStorageInspector: java.util.List getLatestImages()>,error,This is a rare failure scenario!!!
<org.apache.hadoop.hdfs.server.namenode.FSImagePreTransactionalStorageInspector: java.util.List getLatestImages()>,error,Image checkpoint time  <*>  > edits checkpoint time  <*> 
<org.apache.hadoop.hdfs.server.namenode.FSImagePreTransactionalStorageInspector: java.util.List getLatestImages()>,error,Name-node will treat the image as the latest state of the namespace. Old edits will be discarded.
<org.apache.hadoop.hdfs.server.namenode.FSImagePreTransactionalStorageInspector: java.util.List getLatestEditsFiles()>,debug,"Name checkpoint time is newer than edits, not loading edits."
<org.apache.hadoop.hdfs.server.namenode.FSImagePreTransactionalStorageInspector: boolean doRecovery()>,debug,Performing recovery in  <*>  and  <*> 
<org.apache.hadoop.hdfs.server.namenode.FSImagePreTransactionalStorageInspector: boolean doRecovery()>,warn,Unable to delete dir  <*>  before rename 
"<org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Loader: void loadInternal(java.io.RandomAccessFile,java.io.FileInputStream)>",warn,Unrecognized section  <*> 
<org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Loader: void load(java.io.File)>,info,Loaded FSImage in  <*>  seconds. 
<org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode$Loader: void loadINodeSection(java.io.InputStream)>,info,Loading  <*>  INodes. 
<org.apache.hadoop.hdfs.server.namenode.FSImageFormat: void setRenameReservedMapInternal(java.lang.String)>,info,Will rename reserved path  <*>  to  <*> 
"<org.apache.hadoop.hdfs.server.namenode.FSImageFormat: java.lang.String renameReservedPathsOnUpgrade(java.lang.String,int)>",info,Upgrade process renamed reserved path  oldPath  to  path 
"<org.apache.hadoop.hdfs.server.namenode.FSImageFormat: byte[] renameReservedRootComponentOnUpgrade(byte[],int)>",info,Renamed root path .reserved to  renameString 
"<org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Saver: void save(java.io.File,org.apache.hadoop.hdfs.server.namenode.FSImageCompression)>",info,Saving image file  newFile  using  compression 
"<org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Saver: void save(java.io.File,org.apache.hadoop.hdfs.server.namenode.FSImageCompression)>",info,Image file  newFile  of size  <*>  bytes saved in  <*>  seconds. 
"<org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Loader: void loadFullNameINodes(long,java.io.DataInput,org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgress$Counter)>",info,Renaming reserved path  <*>  to  <*> 
"<org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Loader: void loadFilesUnderConstruction(java.io.DataInput,boolean,org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgress$Counter)>",info,Number of files under construction =  <*> 
<org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Loader: void load(java.io.File)>,debug,load last allocated InodeId from fsimage: <*> 
<org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Loader: void load(java.io.File)>,debug,Old layout version doesn\'t have inode id. Will assign new id for each inode.
<org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Loader: void load(java.io.File)>,info,Upgrading to sequential block IDs. Generation stamp for new blocks set to  <*> 
<org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Loader: void load(java.io.File)>,info,Loading image file  curFile  using  stampAtIdSwitch 
<org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Loader: void load(java.io.File)>,info,Number of files =  <*> 
<org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Loader: void load(java.io.File)>,info,Image file  curFile  of size  <*>  bytes loaded in  <*>  seconds. 
<org.apache.hadoop.hdfs.server.namenode.FSImage: void waitForThreads(java.util.List)>,error,Caught interrupted exception while waiting for thread  <*>  to finish. Retrying join 
"<org.apache.hadoop.hdfs.server.namenode.FSImage: void updateCountForQuotaRecursively(org.apache.hadoop.hdfs.server.namenode.INodeDirectory,org.apache.hadoop.hdfs.server.namenode.Quota$Counts)>",error,BUG: Namespace quota violation in image for  <*>  quota =  <*>  < consumed =  child# 
"<org.apache.hadoop.hdfs.server.namenode.FSImage: void updateCountForQuotaRecursively(org.apache.hadoop.hdfs.server.namenode.INodeDirectory,org.apache.hadoop.hdfs.server.namenode.Quota$Counts)>",error,BUG: Diskspace quota violation in image for  <*>  quota =  <*>  < consumed =  <*> 
"<org.apache.hadoop.hdfs.server.namenode.FSImage: void saveNamespace(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.namenode.NNStorage$NameNodeFile,org.apache.hadoop.hdfs.util.Canceler)>",<init>,editLog must be initialized
"<org.apache.hadoop.hdfs.server.namenode.FSImage: void saveNamespace(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.namenode.NNStorage$NameNodeFile,org.apache.hadoop.hdfs.util.Canceler)>",info,Save namespace ...
"<org.apache.hadoop.hdfs.server.namenode.FSImage: void renameImageFileInDir(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.hdfs.server.namenode.NNStorage$NameNodeFile,org.apache.hadoop.hdfs.server.namenode.NNStorage$NameNodeFile,long,boolean)>",debug,renaming   <*>  to  <*> 
"<org.apache.hadoop.hdfs.server.namenode.FSImage: void renameCheckpoint(org.apache.hadoop.hdfs.server.namenode.NNStorage$NameNodeFile,org.apache.hadoop.hdfs.server.namenode.NNStorage$NameNodeFile)>",warn,Unable to rename checkpoint in  <*> 
"<org.apache.hadoop.hdfs.server.namenode.FSImage: void renameCheckpoint(long,org.apache.hadoop.hdfs.server.namenode.NNStorage$NameNodeFile,org.apache.hadoop.hdfs.server.namenode.NNStorage$NameNodeFile,boolean)>",warn,Unable to rename checkpoint in  sd 
"<org.apache.hadoop.hdfs.server.namenode.FSImage: void reloadFromImageFile(java.io.File,org.apache.hadoop.hdfs.server.namenode.FSNamesystem)>",debug,Reloading namespace from  file 
<org.apache.hadoop.hdfs.server.namenode.FSImage: void purgeOldStorage(org.apache.hadoop.hdfs.server.namenode.NNStorage$NameNodeFile)>,warn,Unable to purge old storage  <*> 
"<org.apache.hadoop.hdfs.server.namenode.FSImage: void loadFSImageFile(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext,org.apache.hadoop.hdfs.server.namenode.FSImageStorageInspector$FSImageFile,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption)>",debug,Planning to load image :\n imageFile 
"<org.apache.hadoop.hdfs.server.namenode.FSImage: void loadFSImage(java.io.File,org.apache.hadoop.io.MD5Hash,org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext,boolean)>",info,Loaded image for txid  <*>  from  curFile 
"<org.apache.hadoop.hdfs.server.namenode.FSImage: void format(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,java.lang.String)>",info,Allocated new BlockPoolId:  <*> 
<org.apache.hadoop.hdfs.server.namenode.FSImage: void finalizeUpgrade(boolean)>,info,Finalizing upgrade for local dirs.  <*> 
<org.apache.hadoop.hdfs.server.namenode.FSImage: void endCheckpoint(org.apache.hadoop.hdfs.server.namenode.CheckpointSignature)>,info,End checkpoint at txid  <*> 
<org.apache.hadoop.hdfs.server.namenode.FSImage: void doUpgrade(org.apache.hadoop.hdfs.server.namenode.FSNamesystem)>,<init>,Edits log must not be open.
<org.apache.hadoop.hdfs.server.namenode.FSImage: void doUpgrade(org.apache.hadoop.hdfs.server.namenode.FSNamesystem)>,error,Failed to move aside pre-upgrade storage in image directory  <*> 
<org.apache.hadoop.hdfs.server.namenode.FSImage: void doUpgrade(org.apache.hadoop.hdfs.server.namenode.FSNamesystem)>,info,Starting upgrade of local storage directories.\n   old LV =  <*> ; old CTime =  <*> .\n   new LV =  <*> ; new CTime =  <*> 
<org.apache.hadoop.hdfs.server.namenode.FSImage: void doRollback(org.apache.hadoop.hdfs.server.namenode.FSNamesystem)>,info,Can perform rollback for  sd 
<org.apache.hadoop.hdfs.server.namenode.FSImage: void doRollback(org.apache.hadoop.hdfs.server.namenode.FSNamesystem)>,info,Can perform rollback for shared edit log.
<org.apache.hadoop.hdfs.server.namenode.FSImage: void doRollback(org.apache.hadoop.hdfs.server.namenode.FSNamesystem)>,info,Rolling back storage directory  <*> .\n   new LV =  <*> ; new CTime =  <*> 
<org.apache.hadoop.hdfs.server.namenode.FSImage: void deleteCancelledCheckpoint(long)>,warn,Unable to delete cancelled checkpoint in  sd 
"<org.apache.hadoop.hdfs.server.namenode.FSImage: org.apache.hadoop.hdfs.server.protocol.NamenodeCommand startCheckpoint(org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration,org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration)>",error,Name node  <*>  has newer image layout version: LV =  <*>  cTime =  <*> . Current version: LV =  <*>  cTime =  <*> 
"<org.apache.hadoop.hdfs.server.namenode.FSImage: org.apache.hadoop.hdfs.server.protocol.NamenodeCommand startCheckpoint(org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration,org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration)>",info,Start checkpoint at txid  <*> 
"<org.apache.hadoop.hdfs.server.namenode.FSImage: long loadEdits(java.lang.Iterable,org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)>",debug,About to load edits:\n   <*> 
"<org.apache.hadoop.hdfs.server.namenode.FSImage: long loadEdits(java.lang.Iterable,org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)>",info,Reading  editIn  expecting start txid # <*> 
"<org.apache.hadoop.hdfs.server.namenode.FSImage: boolean recoverTransitionRead(org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)>",info,Storage directory  <*>  is not formatted. 
"<org.apache.hadoop.hdfs.server.namenode.FSImage: boolean recoverTransitionRead(org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)>",info,Formatting ...
"<org.apache.hadoop.hdfs.server.namenode.FSImage: boolean recoverTransitionRead(org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)>",trace,Data dir states:\n   <*> 
"<org.apache.hadoop.hdfs.server.namenode.FSImage: boolean loadFSImage(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)>",closeAllStreams,<*> toAtLeastTxId recovery 
"<org.apache.hadoop.hdfs.server.namenode.FSImage: boolean loadFSImage(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)>",debug,Planning to load edit log stream:  elis 
"<org.apache.hadoop.hdfs.server.namenode.FSImage: boolean loadFSImage(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)>",error,Failed to load image from  i$ 
"<org.apache.hadoop.hdfs.server.namenode.FSImage: boolean loadFSImage(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)>",info,No edit log streams selected.
"<org.apache.hadoop.hdfs.server.namenode.FSImage: boolean loadFSImage(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)>",setMaxOpSize,dfs.namenode.max.op.size 
<org.apache.hadoop.hdfs.server.namenode.FSImage$FSImageSaver: void run()>,error,Unable to save image for  <*> 
<org.apache.hadoop.hdfs.server.namenode.FSImage$FSImageSaver: void run()>,info,Cancelled image saving for  <*> :  <*> 
<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: void dumpOpCounts(java.util.EnumMap)>,append,Summary of operations loaded from edit log:\n  
<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader$EditLogValidation validateEditLog(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream)>,warn,Caught exception after reading  numValid  ops from  in  while determining its valid length. Position was  <*> 
<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader$EditLogValidation validateEditLog(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream)>,warn,"After resync, position is  <*> "
<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader$EditLogValidation scanEditLog(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream)>,warn,Caught exception after reading  numValid  ops from  in  while determining its valid length. Position was  <*> 
<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader$EditLogValidation scanEditLog(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream)>,warn,"After resync, position is  <*> "
"<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: long loadEditRecords(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream,boolean,long,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)>",editLogLoaderPrompt,We failed to read txId  expectedTxId 
"<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: long loadEditRecords(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream,boolean,long,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)>",editLogLoaderPrompt,"There appears to be a gap in the edit log.  We expected txid  expectedTxId , but got txid  <*> . "
"<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: long loadEditRecords(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream,boolean,long,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)>",editLogLoaderPrompt,"There appears to be an out-of-order edit in the edit log.  We expected txid  expectedTxId , but got txid  <*> . "
"<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: long loadEditRecords(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream,boolean,long,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)>",editLogLoaderPrompt,Failed to apply edit log operation  <*> : error  <*> 
"<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: long loadEditRecords(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream,boolean,long,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)>",error,Encountered exception on operation  <*> 
"<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: long loadEditRecords(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream,boolean,long,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)>",info,replaying edit log:  deltaTxId / <*>  transactions completed. ( <*> %) 
"<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: long loadEditRecords(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream,boolean,long,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)>",info,Stopped at OP_START_ROLLING_UPGRADE for rollback.
"<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: long loadEditRecords(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream,boolean,long,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)>",trace,Acquiring write lock to replay edit log
"<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: long loadEditRecords(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream,boolean,long,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)>",trace,"op= <*> , startOpt= startOpt , numEdits= numEdits , totalEdits= <*> "
"<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: long loadEditRecords(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream,boolean,long,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)>",trace,replaying edit log finished
"<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: long loadEditRecords(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream,boolean,long,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)>",trace,replaying edit log finished
"<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: long loadEditRecords(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream,boolean,long,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)>",warn,Stopped reading edit log at  <*> / <*> 
"<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: long applyEditLogOp(org.apache.hadoop.hdfs.server.namenode.FSEditLogOp,org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,int,long)>",debug,<*> :  <*>  numblocks :  <*>  clientHolder  <*>  clientMachine  <*> 
"<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: long applyEditLogOp(org.apache.hadoop.hdfs.server.namenode.FSEditLogOp,org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,int,long)>",debug,Reopening an already-closed file for append
"<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: long applyEditLogOp(org.apache.hadoop.hdfs.server.namenode.FSEditLogOp,org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,int,long)>",debug,<*> :  <*>  numblocks :  <*>  clientHolder  <*>  clientMachine  <*> 
"<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: long applyEditLogOp(org.apache.hadoop.hdfs.server.namenode.FSEditLogOp,org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,int,long)>",debug,<*> :  <*>  numblocks :  <*> 
"<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: long applyEditLogOp(org.apache.hadoop.hdfs.server.namenode.FSEditLogOp,org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,int,long)>",debug,<*> :  <*>  new block id :  <*> 
"<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: long applyEditLogOp(org.apache.hadoop.hdfs.server.namenode.FSEditLogOp,org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,int,long)>",trace,replaying edit log:  op 
"<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void startLogSegment(long,boolean)>",info,Starting log segment at  segmentTxId 
<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void releaseBackupStream(org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration)>,info,Removing backup journal  <*> 
"<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void registerBackupNode(org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration,org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration)>",info,Backup node  bnReg  re-registers 
"<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void registerBackupNode(org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration,org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration)>",info,Registering new backup node:  bnReg 
<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void logSync()>,fatal,Could not sync enough journals to persistent storage due to  <*> .  Unsynced transactions:  <*> 
<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void logSync()>,fatal,Could not sync enough journals to persistent storage. Unsynced transactions:  <*> 
<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void initSharedJournalsForRead()>,warn,"Initializing shared journals for READ, already open for READ"
<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void initJournals(java.util.List)>,error,No edits directories configured!
<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void endCurrentLogSegment(boolean)>,info,Ending log segment  <*> 
<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void close()>,debug,Closing log when already closed
<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void close()>,warn,Error closing journalSet
<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void close()>,warn,Error closing journalSet
<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void abortCurrentLogSegment()>,warn,All journals failed to abort
<org.apache.hadoop.hdfs.server.namenode.FSEditLog: long rollEditLog()>,info,Rolling edit logs
"<org.apache.hadoop.hdfs.server.namenode.FSEditLog: java.lang.Class getJournalClass(org.apache.hadoop.conf.Configuration,java.lang.String)>",warn,"No class configured for  uriScheme ,  <*>  is empty "
"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: void verifyMaxDirItems(org.apache.hadoop.hdfs.server.namenode.INode[],int)>",error,FSDirectory.verifyMaxDirItems:  <*> 
"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: void verifyMaxComponentLength(byte[],java.lang.Object,int)>",error,ERROR in FSDirectory.verifyINodeName
"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: void validateRenameSource(java.lang.String,org.apache.hadoop.hdfs.server.namenode.INodesInPath)>",warn,DIR* FSDirectory.unprotectedRenameTo:  <*> 
"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: void validateRenameSource(java.lang.String,org.apache.hadoop.hdfs.server.namenode.INodesInPath)>",warn,DIR* FSDirectory.unprotectedRenameTo:  rename source cannot be the root 
"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: void validateRenameOverwrite(java.lang.String,java.lang.String,boolean,org.apache.hadoop.hdfs.server.namenode.INode,org.apache.hadoop.hdfs.server.namenode.INode)>",warn,DIR* FSDirectory.unprotectedRenameTo:  <*> 
"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: void validateRenameOverwrite(java.lang.String,java.lang.String,boolean,org.apache.hadoop.hdfs.server.namenode.INode,org.apache.hadoop.hdfs.server.namenode.INode)>",warn,DIR* FSDirectory.unprotectedRenameTo:  <*> 
"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: void validateRenameOverwrite(java.lang.String,java.lang.String,boolean,org.apache.hadoop.hdfs.server.namenode.INode,org.apache.hadoop.hdfs.server.namenode.INode)>",warn,DIR* FSDirectory.unprotectedRenameTo:  <*> 
"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: void validateRenameDestination(java.lang.String,java.lang.String,org.apache.hadoop.hdfs.server.namenode.INode)>",warn,DIR* FSDirectory.unprotectedRenameTo:  <*> 
"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: void updateCountNoQuotaCheck(org.apache.hadoop.hdfs.server.namenode.INodesInPath,int,long,long)>",error,BUG: unexpected exception 
"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: void unprotectedConcat(java.lang.String,java.lang.String[],long)>",debug,DIR* FSNamesystem.concat to  target 
"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: void renameTo(java.lang.String,java.lang.String,long,org.apache.hadoop.hdfs.server.namenode.INode$BlocksMapUpdateInfo,org.apache.hadoop.fs.Options$Rename[])>",debug,DIR* FSDirectory.renameTo:  src  to  dst 
"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: void addEncryptionZone(org.apache.hadoop.hdfs.server.namenode.INodeWithAdditionalFields,org.apache.hadoop.hdfs.server.namenode.XAttrFeature)>",warn,Error parsing protocol buffer of EZ XAttr  <*>  dir: <*> 
"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: void <init>(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.conf.Configuration)>",info,Caching file names occuring more than  <*>  times 
"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: org.apache.hadoop.hdfs.server.namenode.INodeFile unprotectedAddFile(long,java.lang.String,org.apache.hadoop.fs.permission.PermissionStatus,java.util.List,java.util.List,short,long,long,long,boolean,java.lang.String,java.lang.String,byte)>",debug,DIR* FSDirectory.unprotectedAddFile: exception when add  path  to the file system 
"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: org.apache.hadoop.hdfs.server.namenode.INodeFile addFile(java.lang.String,org.apache.hadoop.fs.permission.PermissionStatus,short,long,java.lang.String,java.lang.String)>",debug,DIR* addFile:  path  is added 
"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: org.apache.hadoop.hdfs.server.namenode.INodeFile addFile(java.lang.String,org.apache.hadoop.fs.permission.PermissionStatus,short,long,java.lang.String,java.lang.String)>",info,DIR* addFile: failed to add  path 
"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: org.apache.hadoop.hdfs.server.namenode.INode[] getRelativePathINodes(org.apache.hadoop.hdfs.server.namenode.INode,org.apache.hadoop.hdfs.server.namenode.INode)>",warn,Could not get full path. Corresponding file might have deleted already.
"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: org.apache.hadoop.fs.FileEncryptionInfo getFileEncryptionInfo(org.apache.hadoop.hdfs.server.namenode.INode,int,org.apache.hadoop.hdfs.server.namenode.INodesInPath)>",debug,Encryption zone  <*>  does not have a valid path. 
"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: org.apache.hadoop.fs.FileEncryptionInfo getFileEncryptionInfo(org.apache.hadoop.hdfs.server.namenode.INode,int,org.apache.hadoop.hdfs.server.namenode.INodesInPath)>",warn,Could not find encryption XAttr for file  <*>  in encryption zone  <*> 
"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: long unprotectedDelete(org.apache.hadoop.hdfs.server.namenode.INodesInPath,org.apache.hadoop.hdfs.server.namenode.INode$BlocksMapUpdateInfo,java.util.List,long)>",debug,DIR* FSDirectory.unprotectedDelete:  <*>  is removed 
"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: long delete(java.lang.String,org.apache.hadoop.hdfs.server.namenode.INode$BlocksMapUpdateInfo,java.util.List,long)>",debug,DIR* FSDirectory.delete:  src 
"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: java.lang.String constructRemainingPath(java.lang.String,byte[][],int)>",debug,Resolved path is  <*> 
"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: boolean unprotectedRenameTo(java.lang.String,java.lang.String,long,org.apache.hadoop.hdfs.server.namenode.INode$BlocksMapUpdateInfo,org.apache.hadoop.fs.Options$Rename[])>",debug,DIR* FSDirectory.unprotectedRenameTo:  src  is renamed to  dst 
"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: boolean unprotectedRenameTo(java.lang.String,java.lang.String,long,org.apache.hadoop.hdfs.server.namenode.INode$BlocksMapUpdateInfo,org.apache.hadoop.fs.Options$Rename[])>",warn,DIR* FSDirectory.unprotectedRenameTo:  rename destination cannot be the root 
"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: boolean unprotectedRenameTo(java.lang.String,java.lang.String,long,org.apache.hadoop.hdfs.server.namenode.INode$BlocksMapUpdateInfo,org.apache.hadoop.fs.Options$Rename[])>",warn,DIR* FSDirectory.unprotectedRenameTo:  <*> 
"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: boolean unprotectedRenameTo(java.lang.String,java.lang.String,long,org.apache.hadoop.hdfs.server.namenode.INode$BlocksMapUpdateInfo,org.apache.hadoop.fs.Options$Rename[])>",warn,DIR* FSDirectory.unprotectedRenameTo:  <*> 
"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: boolean unprotectedRenameTo(java.lang.String,java.lang.String,long,org.apache.hadoop.hdfs.server.namenode.INode$BlocksMapUpdateInfo,org.apache.hadoop.fs.Options$Rename[])>",warn,DIR* FSDirectory.unprotectedRenameTo:  <*> 
"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: boolean unprotectedRenameTo(java.lang.String,java.lang.String,long,org.apache.hadoop.hdfs.server.namenode.INode$BlocksMapUpdateInfo,org.apache.hadoop.fs.Options$Rename[])>",warn,DIR* FSDirectory.unprotectedRenameTo: failed to rename  src  to  dst 
"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: boolean unprotectedRenameTo(java.lang.String,java.lang.String,long)>",debug,DIR* FSDirectory.unprotectedRenameTo:  src  is renamed to  dst 
"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: boolean unprotectedRenameTo(java.lang.String,java.lang.String,long)>",warn,DIR* FSDirectory.unprotectedRenameTo: failed to rename  src  to  dst  because destination exists 
"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: boolean unprotectedRenameTo(java.lang.String,java.lang.String,long)>",warn,DIR* FSDirectory.unprotectedRenameTo: failed to rename  src  to  dst  because destination\'s parent does not exist 
"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: boolean unprotectedRenameTo(java.lang.String,java.lang.String,long)>",warn,DIR* FSDirectory.unprotectedRenameTo: failed to rename  src  to  dst  because the source can not be removed 
"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: boolean unprotectedRenameTo(java.lang.String,java.lang.String,long)>",warn,DIR* FSDirectory.unprotectedRenameTo: failed to rename  src  to  dst 
"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: boolean unprotectedRemoveBlock(java.lang.String,org.apache.hadoop.hdfs.server.namenode.INodeFile,org.apache.hadoop.hdfs.protocol.Block)>",debug,DIR* FSDirectory.removeBlock:  path  with  block  block is removed from the file system 
"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: boolean renameTo(java.lang.String,java.lang.String,long)>",debug,DIR* FSDirectory.renameTo:  src  to  dst 
"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: boolean deleteAllowed(org.apache.hadoop.hdfs.server.namenode.INodesInPath,java.lang.String)>",debug,DIR* FSDirectory.unprotectedDelete: failed to remove  src  because it does not exist 
"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: boolean deleteAllowed(org.apache.hadoop.hdfs.server.namenode.INodesInPath,java.lang.String)>",warn,DIR* FSDirectory.unprotectedDelete: failed to remove  src  because the root is not allowed to be deleted 
"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: boolean addLastINodeNoQuotaCheck(org.apache.hadoop.hdfs.server.namenode.INodesInPath,org.apache.hadoop.hdfs.server.namenode.INode)>",warn,FSDirectory.addChildNoQuotaCheck - unexpected
"<org.apache.hadoop.hdfs.server.namenode.FileJournalManager: void selectInputStreams(java.util.Collection,long,boolean)>",debug,this : selecting input streams starting at  fromTxId <*> from among  <*>  candidate file(s) 
<org.apache.hadoop.hdfs.server.namenode.FileJournalManager: void recoverUnfinalizedSegments()>,info,Recovering unfinalized segments in  <*> 
<org.apache.hadoop.hdfs.server.namenode.FileJournalManager: void recoverUnfinalizedSegments()>,info,Deleting zero-length edit log file  elf 
<org.apache.hadoop.hdfs.server.namenode.FileJournalManager: void recoverUnfinalizedSegments()>,info,Moving aside edit log file that seems to have zero transactions  elf 
<org.apache.hadoop.hdfs.server.namenode.FileJournalManager: void purgeLogsOlderThan(long)>,info,Purging logs older than  minTxIdToKeep 
"<org.apache.hadoop.hdfs.server.namenode.FileJournalManager: void finalizeLogSegment(long,long)>",info,Finalizing edits file  <*>  ->  <*> 
<org.apache.hadoop.hdfs.server.namenode.FileJournalManager: void doPreUpgrade()>,error,Failed to move aside pre-upgrade storage in image directory  <*> 
<org.apache.hadoop.hdfs.server.namenode.FileJournalManager: void doPreUpgrade()>,info,Starting upgrade of edits directory  <*> 
<org.apache.hadoop.hdfs.server.namenode.FileJournalManager: void discardEditLogSegments(long)>,info,"Discard the EditLog files, the given start txid is  startTxId "
<org.apache.hadoop.hdfs.server.namenode.FileJournalManager: void discardEditLogSegments(long)>,info,Trash the EditLog file  elf 
"<org.apache.hadoop.hdfs.server.namenode.FileJournalManager: void addStreamsToCollectionFromFiles(java.util.Collection,java.util.Collection,long,boolean)>",debug,passing over  elf  because it is in progress  and we are ignoring in-progress logs. 
"<org.apache.hadoop.hdfs.server.namenode.FileJournalManager: void addStreamsToCollectionFromFiles(java.util.Collection,java.util.Collection,long,boolean)>",debug,"passing over  elf  because it ends at  <*> , but we only care about transactions  as new as  fromTxId "
"<org.apache.hadoop.hdfs.server.namenode.FileJournalManager: void addStreamsToCollectionFromFiles(java.util.Collection,java.util.Collection,long,boolean)>",debug,selecting edit log stream  elf 
"<org.apache.hadoop.hdfs.server.namenode.FileJournalManager: void addStreamsToCollectionFromFiles(java.util.Collection,java.util.Collection,long,boolean)>",error,got IOException while trying to validate header of  elf .  Skipping. 
"<org.apache.hadoop.hdfs.server.namenode.FileJournalManager: org.apache.hadoop.hdfs.server.namenode.EditLogOutputStream startLogSegment(long,int)>",warn,Unable to start log segment  txid  at  <*> :  <*> 
"<org.apache.hadoop.hdfs.server.namenode.FileJournalManager: java.util.List matchEditLogs(java.io.File[],boolean)>",error,Edits file  f  has improperly formatted  transaction ID 
"<org.apache.hadoop.hdfs.server.namenode.FileJournalManager: java.util.List matchEditLogs(java.io.File[],boolean)>",error,In-progress edits file  f  has improperly  formatted transaction ID 
"<org.apache.hadoop.hdfs.server.namenode.FileJournalManager: java.util.List matchEditLogs(java.io.File[],boolean)>",error,In-progress stale edits file  f  has improperly  formatted transaction ID 
"<org.apache.hadoop.hdfs.server.namenode.FileJournalManager: java.util.List getRemoteEditLogs(long,boolean)>",error,got IOException while trying to validate header of  elf .  Skipping. 
<org.apache.hadoop.hdfs.server.namenode.EditLogFileOutputStream: void preallocate()>,debug,Preallocated  total  bytes at the end of  the edit log (offset  <*> ) 
<org.apache.hadoop.hdfs.server.namenode.EditLogFileOutputStream: void flushAndSync(boolean)>,info,Nothing to flush
<org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream: void init(boolean)>,<init>,No header found in log
<org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream: void init(boolean)>,<init>,EOF while reading layout flags from log
<org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream: org.apache.hadoop.hdfs.server.namenode.FSEditLogOp nextValidOp()>,error,nextValidOp: got exception while reading  this 
<org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream: org.apache.hadoop.hdfs.server.namenode.FSEditLogOp nextOpImpl(boolean)>,debug,skipping  skipAmt  bytes at the end  of edit log  \' <*> \': reached txid  <*>  out of  <*> 
<org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream: org.apache.hadoop.hdfs.server.namenode.FSEditLogOp nextOpImpl(boolean)>,error,caught exception initializing  this 
<org.apache.hadoop.hdfs.server.namenode.EditLogBackupOutputStream: void flushAndSync(boolean)>,info,Nothing to flush
"<org.apache.hadoop.hdfs.server.namenode.EditLogBackupOutputStream: void <init>(org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration,org.apache.hadoop.hdfs.server.protocol.JournalInfo)>",error,Error connecting to:  <*> 
"<org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature: void checkDiskspace(org.apache.hadoop.hdfs.server.namenode.INodeDirectory,long)>",error,BUG: Inconsistent diskspace for directory  <*> . Cached =  <*>  != Computed =  computed 
<org.apache.hadoop.hdfs.server.namenode.ClusterJspHelper: void getDecommissionNodeClusterState(java.util.Map)>,warn,Cluster console encounters a not handled situtation.
<org.apache.hadoop.hdfs.server.namenode.Checkpointer: void run()>,error,Exception in doCheckpoint: 
<org.apache.hadoop.hdfs.server.namenode.Checkpointer: void run()>,error,Throwable Exception in doCheckpoint: 
"<org.apache.hadoop.hdfs.server.namenode.Checkpointer: void rollForwardByApplyingLogs(org.apache.hadoop.hdfs.server.protocol.RemoteEditLogManifest,org.apache.hadoop.hdfs.server.namenode.FSImage,org.apache.hadoop.hdfs.server.namenode.FSNamesystem)>",<init>, <*> <*>
"<org.apache.hadoop.hdfs.server.namenode.Checkpointer: void rollForwardByApplyingLogs(org.apache.hadoop.hdfs.server.protocol.RemoteEditLogManifest,org.apache.hadoop.hdfs.server.namenode.FSImage,org.apache.hadoop.hdfs.server.namenode.FSNamesystem)>",info,Checkpointer about to load edits from  <*>  stream(s). 
<org.apache.hadoop.hdfs.server.namenode.Checkpointer: void initialize(org.apache.hadoop.conf.Configuration)>,info,Checkpoint Period :  <*>  secs  ( <*>  min) 
<org.apache.hadoop.hdfs.server.namenode.Checkpointer: void initialize(org.apache.hadoop.conf.Configuration)>,info,"Transactions count is  :  <*> , to trigger checkpoint "
<org.apache.hadoop.hdfs.server.namenode.Checkpointer: void doCheckpoint()>,debug,Doing checkpoint. Last applied:  <*> 
<org.apache.hadoop.hdfs.server.namenode.Checkpointer: void doCheckpoint()>,info,Unable to roll forward using only logs. Downloading image with txid  <*> 
<org.apache.hadoop.hdfs.server.namenode.Checkpointer: void doCheckpoint()>,info,Loading image with txid  <*> 
<org.apache.hadoop.hdfs.server.namenode.Checkpointer: void doCheckpoint()>,info,Checkpoint completed in  <*>  seconds.  New Image Size:  <*> 
"<org.apache.hadoop.hdfs.server.namenode.Checkpointer: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.namenode.BackupNode)>",warn,Checkpointer got exception
<org.apache.hadoop.hdfs.server.namenode.CheckpointConf: void warnForDeprecatedConfigs(org.apache.hadoop.conf.Configuration)>,warn,Configuration key  key  is deprecated! Ignoring...  Instead please specify a value for  dfs.namenode.checkpoint.txns 
"<org.apache.hadoop.hdfs.server.namenode.CancelDelegationTokenServlet: void doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)>",info,Request for token received with no authentication from  <*> 
"<org.apache.hadoop.hdfs.server.namenode.CancelDelegationTokenServlet: void doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)>",info,Exception while cancelling token. Re-throwing. 
"<org.apache.hadoop.hdfs.server.namenode.CacheManager: void removeDirective(long,org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker)>",info,removeDirective of  id  successful. 
"<org.apache.hadoop.hdfs.server.namenode.CacheManager: void removeDirective(long,org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker)>",warn,removeDirective of  id  failed:  
<org.apache.hadoop.hdfs.server.namenode.CacheManager: void removeCachePool(java.lang.String)>,info,removeCachePool of  poolName  failed:  
<org.apache.hadoop.hdfs.server.namenode.CacheManager: void removeCachePool(java.lang.String)>,info,removeCachePool of  poolName  successful. 
"<org.apache.hadoop.hdfs.server.namenode.CacheManager: void processCacheReportImpl(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,java.util.List)>",trace,Cache report from datanode {} has block {}
"<org.apache.hadoop.hdfs.server.namenode.CacheManager: void processCacheReportImpl(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,java.util.List)>",trace,Added block {}  to cachedBlocks
"<org.apache.hadoop.hdfs.server.namenode.CacheManager: void processCacheReportImpl(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,java.util.List)>",trace,Added block {} to CACHED list.
"<org.apache.hadoop.hdfs.server.namenode.CacheManager: void processCacheReportImpl(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,java.util.List)>",trace,Removed block {} from PENDING_CACHED list.
"<org.apache.hadoop.hdfs.server.namenode.CacheManager: void processCacheReport(org.apache.hadoop.hdfs.protocol.DatanodeID,java.util.List)>",debug,"Processed cache report from {}, blocks: {}, processing time: {} msecs"
"<org.apache.hadoop.hdfs.server.namenode.CacheManager: void modifyDirective(org.apache.hadoop.hdfs.protocol.CacheDirectiveInfo,org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker,java.util.EnumSet)>",info,modifyDirective of {} successfully applied {}.
"<org.apache.hadoop.hdfs.server.namenode.CacheManager: void modifyDirective(org.apache.hadoop.hdfs.protocol.CacheDirectiveInfo,org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker,java.util.EnumSet)>",warn,modifyDirective of  idString  failed:  
<org.apache.hadoop.hdfs.server.namenode.CacheManager: void modifyCachePool(org.apache.hadoop.hdfs.protocol.CachePoolInfo)>,info,modifyCachePool of  info  failed:  
<org.apache.hadoop.hdfs.server.namenode.CacheManager: void modifyCachePool(org.apache.hadoop.hdfs.protocol.CachePoolInfo)>,info,modifyCachePool of {} successful; {}
"<org.apache.hadoop.hdfs.server.namenode.CacheManager: void <init>(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.blockmanagement.BlockManager)>",info,Using minimum value {} for {}
<org.apache.hadoop.hdfs.server.namenode.CacheManager: org.apache.hadoop.hdfs.protocol.CachePoolInfo addCachePool(org.apache.hadoop.hdfs.protocol.CachePoolInfo)>,info,addCachePool of  info  failed:  
<org.apache.hadoop.hdfs.server.namenode.CacheManager: org.apache.hadoop.hdfs.protocol.CachePoolInfo addCachePool(org.apache.hadoop.hdfs.protocol.CachePoolInfo)>,info,addCachePool of {} successful.
"<org.apache.hadoop.hdfs.server.namenode.CacheManager: org.apache.hadoop.hdfs.protocol.CacheDirectiveInfo addDirective(org.apache.hadoop.hdfs.protocol.CacheDirectiveInfo,org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker,java.util.EnumSet)>",info,addDirective of {} successful.
"<org.apache.hadoop.hdfs.server.namenode.CacheManager: org.apache.hadoop.hdfs.protocol.CacheDirectiveInfo addDirective(org.apache.hadoop.hdfs.protocol.CacheDirectiveInfo,org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker,java.util.EnumSet)>",warn,addDirective of  info  failed:  
"<org.apache.hadoop.hdfs.server.namenode.CacheManager: long validateExpiryTime(org.apache.hadoop.hdfs.protocol.CacheDirectiveInfo,long)>",trace,Validating directive {} pool maxRelativeExpiryTime {}
<org.apache.hadoop.hdfs.server.namenode.BackupNode: void stop()>,error,Failed to report to name-node.
<org.apache.hadoop.hdfs.server.namenode.BackupNode: void registerWith(org.apache.hadoop.hdfs.server.protocol.NamespaceInfo)>,error,e . Shutting down. 
<org.apache.hadoop.hdfs.server.namenode.BackupNode: void registerWith(org.apache.hadoop.hdfs.server.protocol.NamespaceInfo)>,info,Problem connecting to name-node:  <*> 
<org.apache.hadoop.hdfs.server.namenode.BackupNode: void registerWith(org.apache.hadoop.hdfs.server.protocol.NamespaceInfo)>,warn,Encountered exception 
<org.apache.hadoop.hdfs.server.namenode.BackupNode: org.apache.hadoop.hdfs.server.protocol.NamespaceInfo handshake(org.apache.hadoop.hdfs.server.protocol.NamenodeProtocol)>,fatal,Incompatible build versions: active name-node BV =  <*> ; backup node BV =  <*> 
<org.apache.hadoop.hdfs.server.namenode.BackupNode: org.apache.hadoop.hdfs.server.protocol.NamespaceInfo handshake(org.apache.hadoop.conf.Configuration)>,info,Problem connecting to server:  <*> 
<org.apache.hadoop.hdfs.server.namenode.BackupNode: org.apache.hadoop.hdfs.server.protocol.NamespaceInfo handshake(org.apache.hadoop.conf.Configuration)>,warn,Encountered exception 
<org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer: void verifyJournalRequest(org.apache.hadoop.hdfs.server.protocol.JournalInfo)>,warn,Invalid namespaceID in journal request - expected  <*>  actual  <*> 
<org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer: void verifyJournalRequest(org.apache.hadoop.hdfs.server.protocol.JournalInfo)>,warn,Invalid clusterId in journal request - expected  <*>  actual  <*> 
"<org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer: org.apache.hadoop.hdfs.server.protocol.FenceResponse fence(org.apache.hadoop.hdfs.server.protocol.JournalInfo,long,java.lang.String)>",info,Fenced by  fencerInfo  with epoch  epoch 
<org.apache.hadoop.hdfs.server.namenode.BackupImage: void waitUntilNamespaceFrozen()>,info,Waiting until the NameNode rolls its edit logs in order to freeze the BackupNode namespace.
<org.apache.hadoop.hdfs.server.namenode.BackupImage: void waitUntilNamespaceFrozen()>,info,BackupNode namespace frozen.
<org.apache.hadoop.hdfs.server.namenode.BackupImage: void waitUntilNamespaceFrozen()>,warn,Interrupted waiting for namespace to freeze
<org.apache.hadoop.hdfs.server.namenode.BackupImage: void setState(org.apache.hadoop.hdfs.server.namenode.BackupImage$BNState)>,debug,State transition  <*>  ->  newState 
<org.apache.hadoop.hdfs.server.namenode.BackupImage: void recoverCreateRead()>,info,Storage directory  <*>  is not formatted. 
<org.apache.hadoop.hdfs.server.namenode.BackupImage: void recoverCreateRead()>,info,Formatting ...
<org.apache.hadoop.hdfs.server.namenode.BackupImage: void namenodeStartedLogSegment(long)>,info,NameNode started a new log segment at txid  txid 
<org.apache.hadoop.hdfs.server.namenode.BackupImage: void namenodeStartedLogSegment(long)>,info,Stopped applying edits to prepare for checkpoint.
<org.apache.hadoop.hdfs.server.namenode.BackupImage: void namenodeStartedLogSegment(long)>,warn,"NN started new log segment at txid  txid , but BN had only written up to txid  <*> in the log segment starting at  <*> . Aborting this  log segment. "
"<org.apache.hadoop.hdfs.server.namenode.BackupImage: void journal(long,int,byte[])>",trace,"Got journal, state =  <*> ; firstTxId =  firstTxId ; numTxns =  numTxns "
"<org.apache.hadoop.hdfs.server.namenode.BackupImage: void applyEdits(long,int,byte[])>",debug,data: <*> 
<org.apache.hadoop.hdfs.server.namenode.BackupImage: boolean tryConvergeJournalSpool()>,closeAllStreams, <*> <*>
<org.apache.hadoop.hdfs.server.namenode.BackupImage: boolean tryConvergeJournalSpool()>,closeAllStreams, <*> <*>
<org.apache.hadoop.hdfs.server.namenode.BackupImage: boolean tryConvergeJournalSpool()>,debug,Logs rolled while catching up to current segment
<org.apache.hadoop.hdfs.server.namenode.BackupImage: boolean tryConvergeJournalSpool()>,info,Loading edits into backupnode to try to catch up from txid  <*>  to  <*> 
<org.apache.hadoop.hdfs.server.namenode.BackupImage: boolean tryConvergeJournalSpool()>,info,Going to finish converging with remaining  editStreamsAll  txns from in-progress stream  stream 
<org.apache.hadoop.hdfs.server.namenode.BackupImage: boolean tryConvergeJournalSpool()>,info,Successfully synced BackupNode with NameNode at txnid  <*> 
<org.apache.hadoop.hdfs.server.namenode.BackupImage: boolean tryConvergeJournalSpool()>,warn,Unable to find stream starting with  <*> . This indicates that there is an error in synchronization in BackupImage 
<org.apache.hadoop.hdfs.server.mover.Mover: void main(java.lang.String[])>,error,Exiting  <*>  due to an exception 
"<org.apache.hadoop.hdfs.server.mover.Mover: int run(java.util.Map,org.apache.hadoop.conf.Configuration)>",info,namenodes =  namenodes 
<org.apache.hadoop.hdfs.server.mover.Mover$Processor: void getSnapshottableDirs()>,warn,Failed to get snapshottable directories. Ignore and continue.
"<org.apache.hadoop.hdfs.server.mover.Mover$Processor: boolean processRecursively(java.lang.String,org.apache.hadoop.hdfs.protocol.HdfsFileStatus)>",warn,Failed to check the status of  parent . Ignore it and continue. 
<org.apache.hadoop.hdfs.server.mover.Mover$Processor: boolean processPath(java.lang.String)>,warn,Failed to list directory  fullPath . Ignore the directory and continue. 
"<org.apache.hadoop.hdfs.server.mover.Mover$Processor: boolean processFile(java.lang.String,org.apache.hadoop.hdfs.protocol.HdfsLocatedFileStatus)>",warn,Failed to get the storage policy of file  fullPath 
"<org.apache.hadoop.hdfs.server.datanode.web.resources.DatanodeWebHdfsMethods: void init(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.web.resources.DelegationParam,java.lang.String,org.apache.hadoop.hdfs.web.resources.UriFsPathParam,org.apache.hadoop.hdfs.web.resources.HttpOpParam,org.apache.hadoop.hdfs.web.resources.Param[])>",trace,"HTTP  <*> :  op ,  path , ugi= ugi <*> "
<org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry: void unregisterSlot(org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$SlotId)>,trace,unregisterSlot: ShortCircuitRegistry is not enabled.
<org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry: void removeShm(org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm)>,debug,removing shm  shm 
"<org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry: void registerSlot(org.apache.hadoop.hdfs.ExtendedBlockId,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$SlotId,boolean)>",trace,this  can\'t register a slot because the  ShortCircuitRegistry is not enabled. 
"<org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry: void registerSlot(org.apache.hadoop.hdfs.ExtendedBlockId,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$SlotId,boolean)>",trace,this : registered  blockId  with slot  slotId  (isCached= isCached ) 
<org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry: void <init>(org.apache.hadoop.conf.Configuration)>,debug,"created new ShortCircuitRegistry with interruptCheck= <*> , shmPath= <*> "
<org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry: void <init>(org.apache.hadoop.conf.Configuration)>,debug,Disabling ShortCircuitRegistry
"<org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry: org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry$NewShmInfo createNewMemorySegment(java.lang.String,org.apache.hadoop.net.unix.DomainSocket)>",trace,createNewMemorySegment: ShortCircuitRegistry is not enabled.
"<org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry: org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry$NewShmInfo createNewMemorySegment(java.lang.String,org.apache.hadoop.net.unix.DomainSocket)>",trace,createNewMemorySegment: created  <*> 
<org.apache.hadoop.hdfs.server.datanode.ReplicaInPipeline: void stopWriter(long)>,warn,<*> \n <*> 
"<org.apache.hadoop.hdfs.server.datanode.ReplicaInPipeline: org.apache.hadoop.hdfs.server.datanode.fsdataset.ReplicaOutputStreams createStreams(boolean,org.apache.hadoop.util.DataChecksum)>",debug,writeTo blockfile is  <*>  of size  <*> 
"<org.apache.hadoop.hdfs.server.datanode.ReplicaInPipeline: org.apache.hadoop.hdfs.server.datanode.fsdataset.ReplicaOutputStreams createStreams(boolean,org.apache.hadoop.util.DataChecksum)>",debug,writeTo metafile is  <*>  of size  <*> 
"<org.apache.hadoop.hdfs.server.datanode.ReplicaInfo: void unlinkFile(java.io.File,org.apache.hadoop.hdfs.protocol.Block)>",info,detachFile failed to delete temporary file  <*> 
<org.apache.hadoop.hdfs.server.datanode.ReplicaInfo: boolean unlinkBlock(int)>,info,CopyOnWrite for block  this 
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RollingLogsImpl$Reader: java.lang.String next()>,warn,Failed to read next line.
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaTracker$RamDiskReplica: void deleteSavedFiles()>,warn,Failed to delete block file  <*> 
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaTracker$RamDiskReplica: void deleteSavedFiles()>,warn,Failed to delete meta file  <*> 
"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskAsyncLazyPersistService: void submitLazyPersistTask(java.lang.String,long,long,long,java.io.File,java.io.File,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl)>",debug,LazyWriter schedule async task to persist RamDisk block pool id:  bpId  block id:  blockId 
"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskAsyncLazyPersistService: void submitLazyPersistTask(java.lang.String,long,long,long,java.io.File,java.io.File,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl)>",warn,LazyWriter failed to create  <*> 
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskAsyncLazyPersistService: void shutdown()>,info,Shutting down all async lazy persist service threads
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskAsyncLazyPersistService: void shutdown()>,info,All async lazy persist service threads have been shut down
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskAsyncLazyPersistService: void shutdown()>,warn,AsyncLazyPersistService has already shut down.
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskAsyncLazyPersistService$ReplicaLazyPersistTask: void run()>,warn,LazyWriter failed to async persist RamDisk block pool id:  <*> block Id:  <*> 
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList: void removeVolume(org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl)>,debug,"The volume list has been changed concurrently, retry to remove volume:  target "
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList: void removeVolume(org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl)>,debug,Volume  target  does not exist or is removed by others. 
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList: void removeVolume(org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl)>,info,Removed volume:  target 
"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList: void getAllVolumesMap(java.lang.String,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaMap,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaTracker)>",info,Total time to add all replicas to map:  arr$# ms 
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList: void addVolume(org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl)>,debug,"The volume list has been changed concurrently, retry to remove volume:  newVolume "
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList: void addVolume(org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl)>,info,Added new volume:  <*> 
"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList: void addBlockPool(java.lang.String,org.apache.hadoop.conf.Configuration)>",info,Total time to scan all replicas for block pool  bpid :  arr$# ms 
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList$2: void run()>,info,Scanning block pool  <*>  on volume  <*> ... 
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList$2: void run()>,info,Time taken to scan block pool  <*>  on  <*> :  timeTaken ms 
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList$2: void run()>,info,Caught exception while scanning  <*> . Will throw later. 
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList$1: void run()>,info,Adding replicas to map for block pool  <*>  on volume  <*> ... 
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList$1: void run()>,info,Time to add replicas to map for block pool  <*>  on volume  <*> :  timeTaken ms 
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList$1: void run()>,info,Caught exception while adding replicas from  <*> . Will throw later. 
"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil: long getGenerationStampFromFile(java.io.File[],java.io.File)>",warn,Block  blockFile  does not have a metafile! 
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void unfinalizeBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock)>,warn,Block  b  unfinalized and removed.  
"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void truncateBlock(java.io.File,java.io.File,long,long)>",info,"truncateBlock: blockFile= blockFile , metaFile= metaFile , oldlen= oldlen , newlen= newlen "
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void shutdownBlockPool(java.lang.String)>,info,Removing block pool  bpid 
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void shutdown()>,warn,FsDatasetImpl.shutdown ignoring InterruptedException from LazyWriter.join
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void removeVolumes(java.util.Collection)>,info,Removing  <*>  from FsDataset. 
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void registerMBean(java.lang.String)>,info,Registered FSDatasetState MBean
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void registerMBean(java.lang.String)>,warn,Error registering FSDatasetState MBean
"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void onFailLazyPersist(java.lang.String,long)>",warn,Failed to save replica  <*> . re-enqueueing it. 
"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void onCompleteLazyPersist(java.lang.String,long,long,java.io.File[],org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl)>",debug,LazyWriter: Finish persisting RamDisk block:  block pool Id:  bpId  block id:  blockId  to block file  <*>  and meta file  <*>  on target volume  targetVolume 
"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void invalidate(java.lang.String,org.apache.hadoop.hdfs.protocol.Block[])>",info,Failed to delete replica  <*> : ReplicaInfo not found. 
"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void deleteBlockPool(java.lang.String,boolean)>",warn,"bpid  has some block files, cannot delete unless forced "
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void checkDataDir()>,warn,Removing replica  bpid : <*>  on failed volume  <*> 
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void checkDataDir()>,warn,Removed  removedBlocks  out of  totalBlocks (took  mlsec  millisecs) 
"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void checkAndUpdate(java.lang.String,long,java.io.File,java.io.File,org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi)>",warn,Deleted a metadata file without a block  <*> 
"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void checkAndUpdate(java.lang.String,long,java.io.File,java.io.File,org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi)>",warn,Removed block  blockId  from memory with missing block file on the disk 
"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void checkAndUpdate(java.lang.String,long,java.io.File,java.io.File,org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi)>",warn,Deleted a metadata file for the deleted block  <*> 
"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void checkAndUpdate(java.lang.String,long,java.io.File,java.io.File,org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi)>",warn,Added missing block to memory  <*> 
"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void checkAndUpdate(java.lang.String,long,java.io.File,java.io.File,org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi)>",warn,Failed to delete  diskFile . Will retry on next scan 
"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void checkAndUpdate(java.lang.String,long,java.io.File,java.io.File,org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi)>",warn,Block file in volumeMap  <*>  does not exist. Updating it to the file found during scan  <*> 
"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void checkAndUpdate(java.lang.String,long,java.io.File,java.io.File,org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi)>",warn,Updating generation stamp for block  blockId  from  <*>  to  <*> 
"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void checkAndUpdate(java.lang.String,long,java.io.File,java.io.File,org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi)>",warn,Metadata file in memory  <*>  does not match file found by scan  <*> 
"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void checkAndUpdate(java.lang.String,long,java.io.File,java.io.File,org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi)>",warn,Updating generation stamp for block  blockId  from  <*>  to  <*> 
"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void checkAndUpdate(java.lang.String,long,java.io.File,java.io.File,org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi)>",warn,Updating size of block  blockId  from  <*>  to  <*> 
"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void checkAndUpdate(java.lang.String,long,java.io.File,java.io.File,org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi)>",warn,Reporting the block  corruptBlock  as corrupt due to length mismatch 
"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void checkAndUpdate(java.lang.String,long,java.io.File,java.io.File,org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi)>",warn,Failed to repot bad block  corruptBlock 
"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void cacheBlock(java.lang.String,long)>",warn,"Failed to cache block with id  blockId , pool  bpid : ReplicaInfo not found. "
"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void cacheBlock(java.lang.String,long)>",warn,"Failed to cache block with id  blockId , pool  bpid : replica is not finalized; it is in state  <*> "
"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void cacheBlock(java.lang.String,long)>",warn,"Failed to cache block with id  blockId , pool  bpid : volume not found. "
"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void cacheBlock(java.lang.String,long)>",warn,Failed to cache block with id  blockId : volume was not an instance of FsVolumeImpl. 
"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void cacheBlock(java.lang.String,long)>",warn,Caching not supported on block with id  blockId  since the volume is backed by RAM. 
"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void bumpReplicaGS(org.apache.hadoop.hdfs.server.datanode.ReplicaInfo,long)>",debug,Renaming  <*>  to  <*> 
"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void adjustCrcChannelPosition(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.server.datanode.fsdataset.ReplicaOutputStreams,int)>",debug,Changing meta file offset of block  b  from  <*>  to  newPos 
"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void addVolume(org.apache.hadoop.hdfs.server.datanode.StorageLocation,java.util.List)>",info,"Added volume -  <*> , StorageType:  <*> "
"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void addVolume(org.apache.hadoop.hdfs.server.datanode.StorageLocation,java.util.List)>",warn,Caught exception when adding  <*> . Will throw later. 
"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void addVolume(java.util.Collection,org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory)>",info,"Added volume -  <*> , StorageType:  <*> "
"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void addBlockPool(java.lang.String,org.apache.hadoop.conf.Configuration)>",info,Adding block pool  bpid 
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void <clinit>()>,warn,Data node cannot fully support concurrent reading and writing without native code extensions on Windows.
"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: org.apache.hadoop.hdfs.server.protocol.ReplicaRecoveryInfo initReplicaRecovery(java.lang.String,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaMap,org.apache.hadoop.hdfs.protocol.Block,long,long)>",info,"initReplicaRecovery:  block , recoveryId= recoveryId , replica= <*> "
"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: org.apache.hadoop.hdfs.server.protocol.ReplicaRecoveryInfo initReplicaRecovery(java.lang.String,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaMap,org.apache.hadoop.hdfs.protocol.Block,long,long)>",info,initReplicaRecovery: update recovery id for  block  from  <*>  to  recoveryId 
"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: org.apache.hadoop.hdfs.server.protocol.ReplicaRecoveryInfo initReplicaRecovery(java.lang.String,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaMap,org.apache.hadoop.hdfs.protocol.Block,long,long)>",info,initReplicaRecovery: changing replica state for  block  from  <*>  to  <*> 
"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: org.apache.hadoop.hdfs.server.datanode.ReplicaInPipeline recoverRbw(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,long,long)>",info,Recover RBW replica  b 
"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: org.apache.hadoop.hdfs.server.datanode.ReplicaInPipeline recoverRbw(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,long,long)>",info,Recovering  rbw 
"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: org.apache.hadoop.hdfs.server.datanode.ReplicaInPipeline recoverAppend(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,long)>",info,Recover failed append to  b 
"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: org.apache.hadoop.hdfs.server.datanode.ReplicaInPipeline createTemporary(org.apache.hadoop.hdfs.StorageType,org.apache.hadoop.hdfs.protocol.ExtendedBlock)>",warn,Unable to stop existing writer for block  b  after  writerStopMs  miniseconds. 
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: org.apache.hadoop.hdfs.server.datanode.ReplicaInPipeline convertTemporaryToRbw(org.apache.hadoop.hdfs.protocol.ExtendedBlock)>,info,"Convert  b  from Temporary to RBW, visible length= <*> "
"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: org.apache.hadoop.hdfs.server.datanode.ReplicaInPipeline append(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,long)>",info,Appending to  <*> 
"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: org.apache.hadoop.hdfs.server.datanode.ReplicaBeingWritten append(java.lang.String,org.apache.hadoop.hdfs.server.datanode.FinalizedReplica,long,long)>",debug,Renaming  <*>  to  <*> 
"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: org.apache.hadoop.hdfs.server.datanode.ReplicaBeingWritten append(java.lang.String,org.apache.hadoop.hdfs.server.datanode.FinalizedReplica,long,long)>",debug,"Renaming  <*>  to  $u , file length= <*> "
"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: org.apache.hadoop.hdfs.server.datanode.ReplicaBeingWritten append(java.lang.String,org.apache.hadoop.hdfs.server.datanode.FinalizedReplica,long,long)>",warn,Cannot move meta file  <*> back to the finalized directory  <*> 
"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: java.lang.String updateReplicaUnderRecovery(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,long)>",info,"updateReplica:  oldBlock , recoveryId= recoveryId , length= newlength , replica= <*> "
"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: java.lang.String recoverClose(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,long)>",info,Recover failed close  b 
"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: java.io.File[] copyBlockFiles(long,long,java.io.File,java.io.File,java.io.File)>",debug,Copied  srcMeta  to  <*>  and calculated checksum 
"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: java.io.File[] copyBlockFiles(long,long,java.io.File,java.io.File,java.io.File)>",debug,Copied  srcFile  to  <*> 
"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: java.io.File validateBlockFile(java.lang.String,long)>",debug,"blockId= blockId , f= <*> "
"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: java.io.File moveBlockFiles(org.apache.hadoop.hdfs.protocol.Block,java.io.File,java.io.File)>",debug,addFinalizedBlock: Moved  <*>  to  <*>  and  srcfile  to  <*> 
"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: boolean delBlockFromDisk(java.io.File,java.io.File,org.apache.hadoop.hdfs.protocol.Block)>",warn,No file exists for block:  b 
"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: boolean delBlockFromDisk(java.io.File,java.io.File,org.apache.hadoop.hdfs.protocol.Block)>",warn,Not able to delete the block file:  blockFile 
"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: boolean delBlockFromDisk(java.io.File,java.io.File,org.apache.hadoop.hdfs.protocol.Block)>",warn,Not able to delete the meta block file:  metaFile 
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl$LazyWriter: void run()>,info,"LazyWriter was interrupted, exiting"
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl$LazyWriter: void run()>,warn,Ignoring exception in LazyWriter:
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl$LazyWriter: void evictBlocks()>,debug,Evicting block  <*> 
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl$LazyWriter: boolean saveNextReplica()>,debug,LazyWriter: Start persisting RamDisk block: block pool Id:  <*>  block id:  <*>  on target volume  <*> 
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl$LazyWriter: boolean saveNextReplica()>,warn,Exception saving replica  block 
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl$LazyWriter: boolean saveNextReplica()>,warn,Failed to save replica  block . re-enqueueing it. 
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl$LazyWriter: boolean saveNextReplica()>,warn,Failed to save replica  block . re-enqueueing it. 
"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache: void uncacheBlock(java.lang.String,long)>",debug,"Block with id {}, pool {} does not need to be uncached, because it is not currently in the mappableBlockMap."
"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache: void uncacheBlock(java.lang.String,long)>",debug,"Cancelling caching for block with id {}, pool {}."
"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache: void uncacheBlock(java.lang.String,long)>",debug,"{} is anchored, and can\'t be uncached now.  Scheduling it for uncaching in {} "
"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache: void uncacheBlock(java.lang.String,long)>",debug,{} has been scheduled for immediate uncaching.
"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache: void uncacheBlock(java.lang.String,long)>",debug,"Block with id {}, pool {} does not need to be uncached, because it is in state {}."
"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache: void cacheBlock(long,java.lang.String,java.lang.String,long,long,java.util.concurrent.Executor)>",debug,"Block with id {}, pool {} already exists in the FsDatasetCache with state {}"
"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache: void cacheBlock(long,java.lang.String,java.lang.String,long,long,java.util.concurrent.Executor)>",debug,"Initiating caching for Block with id {}, pool {}"
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$UncachingTask: void run()>,debug,Uncaching of {} completed. usedBytes = {}
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$UncachingTask: void run()>,debug,Deferred uncaching of {} completed. usedBytes = {}
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$UncachingTask: boolean shouldDefer()>,debug,Uncaching {} now that it is no longer in use by any clients.
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$UncachingTask: boolean shouldDefer()>,info,Replica {} still can\'t be uncached because some clients continue to use it.  Will wait for {}
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$UncachingTask: boolean shouldDefer()>,warn,Forcibly uncaching {} after {} because client(s) {} refused to stop using it.
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$CachingTask: void run()>,debug,Caching of {} was aborted.  We are now caching only {} bytes in total.
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$CachingTask: void run()>,debug,Caching of {} was aborted.  We are now caching only {} bytes in total.
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$CachingTask: void run()>,debug,Caching of {} was aborted.  We are now caching only {} bytes in total.
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$CachingTask: void run()>,debug,Caching of {} was aborted.  We are now caching only {} bytes in total.
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$CachingTask: void run()>,debug,Caching of {} was aborted.  We are now caching only {} bytes in total.
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$CachingTask: void run()>,debug,Caching of {} was aborted.  We are now caching only {} bytes in total.
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$CachingTask: void run()>,debug,Caching of {} was aborted.  We are now caching only {} bytes in total.
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$CachingTask: void run()>,debug,Successfully cached {}.  We are now caching {} bytes in total.
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$CachingTask: void run()>,debug,Caching of {} was aborted.  We are now caching only {} bytes in total.
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$CachingTask: void run()>,info,Failed to cache  <*> : failed to find backing  files. 
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$CachingTask: void run()>,warn,Failed to cache  <*> : could not reserve  <*>  more bytes in the cache:  dfs.datanode.max.locked.memory  of  <*>  exceeded. 
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$CachingTask: void run()>,warn,Failed to cache  <*> : Underlying blocks are not backed by files. 
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$CachingTask: void run()>,warn,Failed to cache  <*> : failed to open file 
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$CachingTask: void run()>,warn,Failed to cache  <*> : checksum verification failed. 
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$CachingTask: void run()>,warn,Failed to cache  <*> 
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$CachingTask: void run()>,warn,Caching of  <*>  was cancelled. 
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: void shutdown()>,info,Shutting down all async disk service threads
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: void shutdown()>,info,All async disk service threads have been shut down
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: void shutdown()>,warn,AsyncDiskService has already shut down.
"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: void deleteAsync(org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl,java.io.File,java.io.File,org.apache.hadoop.hdfs.protocol.ExtendedBlock,java.lang.String)>",info,Scheduling  <*>  file  blockFile  for deletion 
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService$ReplicaFileDeleteTask: void run()>,info,Deleted  <*>   <*>  file  <*> 
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService$ReplicaFileDeleteTask: void run()>,warn,Unexpected error trying to  <*>  block  <*>   <*>  at file  <*> . Ignored. 
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService$ReplicaFileDeleteTask: boolean moveFiles()>,debug,Moving files  <*>  and  <*>  to trash. 
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService$ReplicaFileDeleteTask: boolean moveFiles()>,error,Failed to create trash directory  <*> 
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService$2: void run()>,warn,sync_file_range error
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: void saveDfsUsed()>,warn,Failed to delete old dfsUsed file in  <*> 
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: void saveDfsUsed()>,warn,Failed to write dfsUsed to  $u 
"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: void getVolumeMap(org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaMap,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaTracker)>",info,Recovered  <*>  replicas from  <*> 
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: void deleteReplica(org.apache.hadoop.hdfs.server.datanode.ReplicaInfo)>,warn,Failed to delete block file  <*> 
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: void deleteReplica(org.apache.hadoop.hdfs.server.datanode.ReplicaInfo)>,warn,Failed to delete meta file  <*> 
"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: void addToReplicasMap(org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaMap,java.io.File,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaTracker,boolean)>",warn,Failed to delete restart meta file:  <*> 
"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: org.apache.hadoop.hdfs.server.datanode.ReplicaInfo selectReplicaToDelete(org.apache.hadoop.hdfs.server.datanode.ReplicaInfo,org.apache.hadoop.hdfs.server.datanode.ReplicaInfo)>",debug,resolveDuplicateReplicas decide to keep  replicaToKeep .  Will try to delete  <*> 
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: long loadDfsUsed()>,info,Cached dfsUsed found for  <*> :  <*> 
"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: java.io.File activateSavedReplica(org.apache.hadoop.hdfs.protocol.Block,java.io.File,java.io.File)>",info,Moved  blockFile  to  <*> 
"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: java.io.File activateSavedReplica(org.apache.hadoop.hdfs.protocol.Block,java.io.File,java.io.File)>",info,Moved  metaFile  to  <*> 
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: int moveLazyPersistReplicasToFinalized(java.io.File)>,warn,Failed to mkdirs  <*> 
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: int moveLazyPersistReplicasToFinalized(java.io.File)>,warn,Failed to move meta file from  file  to  <*> 
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: int moveLazyPersistReplicasToFinalized(java.io.File)>,warn,Failed to move block file from  <*>  to  <*> 
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: int moveLazyPersistReplicasToFinalized(java.io.File)>,warn,Failed to move  <*>  to  <*> 
<org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy: void setConf(org.apache.hadoop.conf.Configuration)>,info,"Available space volume choosing policy initialized: dfs.datanode.available-space-volume-choosing-policy.balanced-space-threshold =  <*> ,  dfs.datanode.available-space-volume-choosing-policy.balanced-space-preference-fraction  =  <*> "
<org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy: void setConf(org.apache.hadoop.conf.Configuration)>,warn,The value of dfs.datanode.available-space-volume-choosing-policy.balanced-space-preference-fraction is greater than . but should be in the range . - .
<org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy: void setConf(org.apache.hadoop.conf.Configuration)>,warn,The value of dfs.datanode.available-space-volume-choosing-policy.balanced-space-preference-fraction is less than . so volumes with less available disk space will receive more block allocations
"<org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy: org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi chooseVolume(java.util.List,long)>",debug,All volumes are within the configured free space balance threshold. Selecting  <*>  for write of block size  replicaSize 
"<org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy: org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi chooseVolume(java.util.List,long)>",debug,Volumes are imbalanced. Selecting  <*>  from high available space volumes for write of block size  replicaSize 
"<org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy: org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi chooseVolume(java.util.List,long)>",debug,Volumes are imbalanced. Selecting  <*>  from low available space volumes for write of block size  replicaSize 
<org.apache.hadoop.hdfs.server.datanode.DNConf: void <init>(org.apache.hadoop.conf.Configuration)>,info,dfs.blockreport.initialDelay is greater than dfs.blockreport.intervalMsec. Setting initial delay to  msec:
<org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: void start()>,info,Periodic Directory Tree Verification scan starting at  firstScanTime  with interval  <*> 
<org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: void shutdown()>,error,interrupted while waiting for masterThread to terminate
<org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: void shutdown()>,error,interrupted while waiting for reportCompileThreadPool to terminate
<org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: void shutdown()>,warn,"DirectoryScanner: shutdown has been called, but periodic scanner not started"
<org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: void shutdown()>,warn,DirectoryScanner: shutdown has been called
<org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: void run()>,error,Exception during DirectoryScanner execution - will continue next cycle
<org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: void run()>,error,System Error during DirectoryScanner execution - permanently terminating periodic scanner
<org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: void run()>,warn,this cycle terminating immediately because \'shouldRun\' has been deactivated
<org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: java.util.Map getDiskReport()>,error,Error compiling report
"<org.apache.hadoop.hdfs.server.datanode.DirectoryScanner$ReportCompiler: java.util.LinkedList compileReport(org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi,java.io.File,java.util.LinkedList)>",warn,Exception occured while compiling report: 
<org.apache.hadoop.hdfs.server.datanode.DataXceiverServer: void sendOOBToPeers()>,warn,Got error when sending OOB message.
<org.apache.hadoop.hdfs.server.datanode.DataXceiverServer: void sendOOBToPeers()>,warn,Interrupted when sending OOB message.
<org.apache.hadoop.hdfs.server.datanode.DataXceiverServer: void run()>,error,<*> :DataXceiverServer: Exiting due to:  
<org.apache.hadoop.hdfs.server.datanode.DataXceiverServer: void run()>,info,Shutting down DataXceiverServer before restart
<org.apache.hadoop.hdfs.server.datanode.DataXceiverServer: void run()>,warn,<*> :DataXceiverServer:  
<org.apache.hadoop.hdfs.server.datanode.DataXceiverServer: void run()>,warn,<*> :DataXceiverServer:  
<org.apache.hadoop.hdfs.server.datanode.DataXceiverServer: void run()>,warn,DataNode is out of memory. Will retry in  seconds.
<org.apache.hadoop.hdfs.server.datanode.DataXceiverServer: void run()>,warn,<*>  :DataXceiverServer: close exception 
<org.apache.hadoop.hdfs.server.datanode.DataXceiverServer: void kill()>,warn,<*> :DataXceiverServer.kill():  
<org.apache.hadoop.hdfs.server.datanode.DataXceiverServer: void closeAllPeers()>,info,Closing all peers.
"<org.apache.hadoop.hdfs.server.datanode.DataXceiverServer$BlockBalanceThrottler: void <init>(long,int)>",info,Balancing bandwith is  bandwidth  bytes/s 
"<org.apache.hadoop.hdfs.server.datanode.DataXceiverServer$BlockBalanceThrottler: void <init>(long,int)>",info,Number threads for balancing is  maxThreads 
"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void writeBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.StorageType,org.apache.hadoop.security.token.Token,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.hdfs.StorageType[],org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.protocol.datatransfer.BlockConstructionStage,int,long,long,long,org.apache.hadoop.util.DataChecksum,org.apache.hadoop.hdfs.server.datanode.CachingStrategy,boolean)>",debug,"opWriteBlock: stage= stage , clientname= clientname \n  block  = block , newGs= latestGenerationStamp , bytesRcvd=[ minBytesRcvd ,  maxBytesRcvd ] \n  targets= <*> ; pipelineSize= pipelineSize , srcDataNode= srcDataNode "
"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void writeBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.StorageType,org.apache.hadoop.security.token.Token,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.hdfs.StorageType[],org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.protocol.datatransfer.BlockConstructionStage,int,long,long,long,org.apache.hadoop.util.DataChecksum,org.apache.hadoop.hdfs.server.datanode.CachingStrategy,boolean)>",debug,"isDatanode= isDatanode , isClient= isClient , isTransfer= isTransfer "
"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void writeBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.StorageType,org.apache.hadoop.security.token.Token,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.hdfs.StorageType[],org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.protocol.datatransfer.BlockConstructionStage,int,long,long,long,org.apache.hadoop.util.DataChecksum,org.apache.hadoop.hdfs.server.datanode.CachingStrategy,boolean)>",debug,writeBlock receive buf size  <*>  tcp no delay  <*> 
"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void writeBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.StorageType,org.apache.hadoop.security.token.Token,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.hdfs.StorageType[],org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.protocol.datatransfer.BlockConstructionStage,int,long,long,long,org.apache.hadoop.util.DataChecksum,org.apache.hadoop.hdfs.server.datanode.CachingStrategy,boolean)>",debug,Connecting to datanode  <*> 
"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void writeBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.StorageType,org.apache.hadoop.security.token.Token,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.hdfs.StorageType[],org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.protocol.datatransfer.BlockConstructionStage,int,long,long,long,org.apache.hadoop.util.DataChecksum,org.apache.hadoop.hdfs.server.datanode.CachingStrategy,boolean)>",error,<*> :Exception transfering block  block  to mirror  <*> :  <*> 
"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void writeBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.StorageType,org.apache.hadoop.security.token.Token,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.hdfs.StorageType[],org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.protocol.datatransfer.BlockConstructionStage,int,long,long,long,org.apache.hadoop.util.DataChecksum,org.apache.hadoop.hdfs.server.datanode.CachingStrategy,boolean)>",info,Receiving  block  src:  <*>  dest:  <*> 
"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void writeBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.StorageType,org.apache.hadoop.security.token.Token,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.hdfs.StorageType[],org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.protocol.datatransfer.BlockConstructionStage,int,long,long,long,org.apache.hadoop.util.DataChecksum,org.apache.hadoop.hdfs.server.datanode.CachingStrategy,boolean)>",info,Datanode  <*>  got response for connect ack   from downstream datanode with firstbadlink as  <*> 
"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void writeBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.StorageType,org.apache.hadoop.security.token.Token,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.hdfs.StorageType[],org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.protocol.datatransfer.BlockConstructionStage,int,long,long,long,org.apache.hadoop.util.DataChecksum,org.apache.hadoop.hdfs.server.datanode.CachingStrategy,boolean)>",info,<*> :Exception transfering  block  to mirror  <*> - continuing without the mirror 
"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void writeBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.StorageType,org.apache.hadoop.security.token.Token,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.hdfs.StorageType[],org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.protocol.datatransfer.BlockConstructionStage,int,long,long,long,org.apache.hadoop.util.DataChecksum,org.apache.hadoop.hdfs.server.datanode.CachingStrategy,boolean)>",info,Datanode  <*>  forwarding connect ack to upstream firstbadlink is  firstBadLink 
"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void writeBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.StorageType,org.apache.hadoop.security.token.Token,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.hdfs.StorageType[],org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.protocol.datatransfer.BlockConstructionStage,int,long,long,long,org.apache.hadoop.util.DataChecksum,org.apache.hadoop.hdfs.server.datanode.CachingStrategy,boolean)>",info,Received  block  src:  <*>  dest:  <*>  of size  <*> 
"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void writeBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.StorageType,org.apache.hadoop.security.token.Token,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.hdfs.StorageType[],org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.protocol.datatransfer.BlockConstructionStage,int,long,long,long,org.apache.hadoop.util.DataChecksum,org.apache.hadoop.hdfs.server.datanode.CachingStrategy,boolean)>",info,opWriteBlock  block  received exception  <*> 
"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void writeBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.StorageType,org.apache.hadoop.security.token.Token,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.hdfs.StorageType[],org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.protocol.datatransfer.BlockConstructionStage,int,long,long,long,org.apache.hadoop.util.DataChecksum,org.apache.hadoop.hdfs.server.datanode.CachingStrategy,boolean)>",trace,TRANSFER: send close-ack
<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void sendOOB()>,info,Sending OOB to peer:  <*> 
<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void run()>,debug,<*> :Number of active connections is:  <*> 
<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void run()>,debug,Cached  <*>  closing after  opsProcessed  ops 
<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void run()>,debug,<*> :Number of active connections is:  <*> 
<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void run()>,debug,<*> :Number of active connections is:  <*> 
<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void run()>,debug,<*> :Number of active connections is:  <*> 
<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void run()>,error,<*> :DataXceiver error processing  <*>  operation   src:  <*>  dst:  <*> 
<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void run()>,info,Failed to read expected encryption handshake from client at  <*> . Perhaps the client  is running an older version of Hadoop which does not support  encryption 
<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void run()>,info,<*> ;  <*> 
<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void run()>,trace,<*> :DataXceiver error processing  <*>  operation   src:  <*>  dst:  <*> 
<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void requestShortCircuitShm(java.lang.String)>,warn,Failed to shut down socket in error handler
<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void requestShortCircuitShm(java.lang.String)>,warn,Failed to send success response back to the client.  Shutting down socket for  <*> . 
<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void requestShortCircuitShm(java.lang.String)>,warn,Failed to shut down socket in error handler
<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void requestShortCircuitShm(java.lang.String)>,warn,Failed to send success response back to the client.  Shutting down socket for  <*> . 
<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void requestShortCircuitShm(java.lang.String)>,warn,Failed to shut down socket in error handler
<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void requestShortCircuitShm(java.lang.String)>,warn,Failed to send success response back to the client.  Shutting down socket for  <*> . 
<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void requestShortCircuitShm(java.lang.String)>,warn,Failed to shut down socket in error handler
"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void requestShortCircuitFds(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$SlotId,int,boolean)>",info,Unregistering  registeredSlotId  because the  requestShortCircuitFdsForRead operation failed. 
"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void requestShortCircuitFds(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$SlotId,int,boolean)>",info,Unregistering  registeredSlotId  because the  requestShortCircuitFdsForRead operation failed. 
"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void requestShortCircuitFds(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$SlotId,int,boolean)>",trace,Reading receipt verification byte for  slotId 
"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void requestShortCircuitFds(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$SlotId,int,boolean)>",trace,Receipt verification is not enabled on the DataNode.  Not verifying  slotId 
"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void replaceBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.StorageType,org.apache.hadoop.security.token.Token,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo)>",debug,Connecting to datanode  <*> 
"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void replaceBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.StorageType,org.apache.hadoop.security.token.Token,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo)>",info,"Moved  block  from  <*> , delHint= delHint "
"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void replaceBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.StorageType,org.apache.hadoop.security.token.Token,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo)>",info,opReplaceBlock  block  received exception  <*> 
"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void replaceBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.StorageType,org.apache.hadoop.security.token.Token,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo)>",warn,Not able to receive block  <*>  from  <*>  because threads  quota is exceeded. 
"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void replaceBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.StorageType,org.apache.hadoop.security.token.Token,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo)>",warn,Error writing reply back to  <*> 
"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void replaceBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.StorageType,org.apache.hadoop.security.token.Token,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo)>",warn,Error writing reply back to  <*> 
"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void readBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token,java.lang.String,long,long,boolean,org.apache.hadoop.hdfs.server.datanode.CachingStrategy)>",debug,Error reading client status response. Will close connection.
"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void readBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token,java.lang.String,long,long,boolean,org.apache.hadoop.hdfs.server.datanode.CachingStrategy)>",info,opReadBlock  block  received exception  <*> 
"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void readBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token,java.lang.String,long,long,boolean,org.apache.hadoop.hdfs.server.datanode.CachingStrategy)>",trace,<*> :Ignoring exception while serving  block  to  <*> 
"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void readBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token,java.lang.String,long,long,boolean,org.apache.hadoop.hdfs.server.datanode.CachingStrategy)>",warn,Client  <*>  did not send a valid status code after reading.  Will close connection. 
"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void readBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token,java.lang.String,long,long,boolean,org.apache.hadoop.hdfs.server.datanode.CachingStrategy)>",warn,<*> :Got exception while serving  block  to  <*> 
"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void copyBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token)>",info,Not able to copy block  <*>   to  <*>  because threads  quota is exceeded. 
"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void copyBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token)>",info,Copied  block  to  <*> 
"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void copyBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token)>",info,opCopyBlock  block  received exception  <*> 
"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void checkAccess(java.io.OutputStream,boolean,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.protocol.datatransfer.Op,org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager$AccessMode)>",debug,Checking block access token for block \' <*> \' with mode \' mode \' 
"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void checkAccess(java.io.OutputStream,boolean,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.protocol.datatransfer.Op,org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager$AccessMode)>",warn,"Block token verification failed: op= op , remoteAddress= <*> , message= <*> "
"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void blockChecksum(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token)>",debug,"block= block , bytesPerCRC= <*> , crcPerBlock= crcPerBlock , md= <*> "
"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void <init>(org.apache.hadoop.hdfs.net.Peer,org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.server.datanode.DataXceiverServer)>",debug,Number of active connections is:  <*> 
<org.apache.hadoop.hdfs.server.datanode.DataStorage: void restoreTrash(java.lang.String)>,info,Restored trash for bpid  bpid 
"<org.apache.hadoop.hdfs.server.datanode.DataStorage: void removeDuplicateEntries(java.util.ArrayList,java.util.ArrayList)>",warn,Unexpectedly low genstamp on  <*> . 
"<org.apache.hadoop.hdfs.server.datanode.DataStorage: void removeDuplicateEntries(java.util.ArrayList,java.util.ArrayList)>",warn,Unexpectedly short length on  <*> . 
"<org.apache.hadoop.hdfs.server.datanode.DataStorage: void removeDuplicateEntries(java.util.ArrayList,java.util.ArrayList)>",warn,Unexpectedly short length on  <*> . 
"<org.apache.hadoop.hdfs.server.datanode.DataStorage: void removeDuplicateEntries(java.util.ArrayList,java.util.ArrayList)>",warn,Discarding  <*> . 
"<org.apache.hadoop.hdfs.server.datanode.DataStorage: void recoverTransitionRead(org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo,java.util.Collection,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption)>",info,DataNode version:  <*>  and NameNode layout version:  <*> 
"<org.apache.hadoop.hdfs.server.datanode.DataStorage: void makeBlockPoolDataDir(java.util.Collection,org.apache.hadoop.conf.Configuration)>",warn,Invalid directory in:  <*> :  <*> 
"<org.apache.hadoop.hdfs.server.datanode.DataStorage: void linkBlocks(org.apache.hadoop.hdfs.server.datanode.DataNode,java.io.File,java.io.File,int,org.apache.hadoop.fs.HardLink)>",error,There are  <*>  duplicate block  entries within the same volume. 
<org.apache.hadoop.hdfs.server.datanode.DataStorage: void enableTrash(java.lang.String)>,info,Enabled trash for bpid  bpid 
"<org.apache.hadoop.hdfs.server.datanode.DataStorage: void doUpgrade(org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo)>",info,Updating layout version from  <*>  to  <*>  for storage  <*> 
"<org.apache.hadoop.hdfs.server.datanode.DataStorage: void doUpgrade(org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo)>",info,Upgrading storage directory  <*> .\n   old LV =  <*> ; old CTime =  <*> .\n   new LV =  <*> ; new CTime =  <*> 
"<org.apache.hadoop.hdfs.server.datanode.DataStorage: void doUpgrade(org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo)>",info,Upgrade of  <*>  is complete 
"<org.apache.hadoop.hdfs.server.datanode.DataStorage: void doRollback(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo)>",info,Layout version rolled back to  <*>  for storage  <*> 
"<org.apache.hadoop.hdfs.server.datanode.DataStorage: void doRollback(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo)>",info,Rolling back storage directory  <*> .\n   target LV =  <*> ; target CTime =  <*> 
"<org.apache.hadoop.hdfs.server.datanode.DataStorage: void doRollback(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo)>",info,Rollback of  <*>  is complete 
<org.apache.hadoop.hdfs.server.datanode.DataStorage: void doFinalize(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory)>,info,Finalizing upgrade for storage directory  <*> .\n   cur LV =  <*> ; cur CTime =  <*> 
"<org.apache.hadoop.hdfs.server.datanode.DataStorage: org.apache.hadoop.hdfs.server.datanode.DataStorage$VolumeBuilder prepareVolume(org.apache.hadoop.hdfs.server.datanode.DataNode,java.io.File,java.util.List)>",warn,Storage directory is in use.
"<org.apache.hadoop.hdfs.server.datanode.DataStorage: org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory loadStorageDirectory(org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo,java.io.File,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption)>",info,Storage directory  dataDir  does not exist 
"<org.apache.hadoop.hdfs.server.datanode.DataStorage: org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory loadStorageDirectory(org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo,java.io.File,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption)>",info,Storage directory  dataDir  is not formatted for  <*> 
"<org.apache.hadoop.hdfs.server.datanode.DataStorage: org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory loadStorageDirectory(org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo,java.io.File,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption)>",info,Formatting ...
"<org.apache.hadoop.hdfs.server.datanode.DataStorage: java.util.List addStorageLocations(org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo,java.util.Collection,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption)>",info,Storage directory  dataDir  has already been used. 
"<org.apache.hadoop.hdfs.server.datanode.DataStorage: java.util.List addStorageLocations(org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo,java.util.Collection,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption)>",warn,Failed to add storage for block pool:  <*>  :  <*> 
"<org.apache.hadoop.hdfs.server.datanode.DataStorage: boolean createStorageID(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,boolean)>",info,Generated new storageID  <*>  for directory  <*> <*> 
<org.apache.hadoop.hdfs.server.datanode.DataStorage$1: void run()>,error,Finalize upgrade for  <*>  failed 
<org.apache.hadoop.hdfs.server.datanode.DataStorage$1: void run()>,info,Finalize upgrade for  <*>  is complete 
"<org.apache.hadoop.hdfs.server.datanode.DataNode: void transferBlocks(java.lang.String,org.apache.hadoop.hdfs.protocol.Block[],org.apache.hadoop.hdfs.protocol.DatanodeInfo[][],org.apache.hadoop.hdfs.StorageType[][])>",warn,Failed to transfer block  <*> 
"<org.apache.hadoop.hdfs.server.datanode.DataNode: void transferBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.hdfs.StorageType[])>",info,Can\'t send invalid block  block 
"<org.apache.hadoop.hdfs.server.datanode.DataNode: void transferBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.hdfs.StorageType[])>",info,<*>  Starting thread to transfer  block  to  <*> 
"<org.apache.hadoop.hdfs.server.datanode.DataNode: void syncBlock(org.apache.hadoop.hdfs.server.protocol.BlockRecoveryCommand$RecoveringBlock,java.util.List)>",debug,"block= <*> , (length= <*> ), syncList= syncList "
"<org.apache.hadoop.hdfs.server.datanode.DataNode: void syncBlock(org.apache.hadoop.hdfs.server.protocol.BlockRecoveryCommand$RecoveringBlock,java.util.List)>",warn,"Failed to updateBlock (newblock= $u , datanode= <*> ) "
<org.apache.hadoop.hdfs.server.datanode.DataNode: void startPlugins(org.apache.hadoop.conf.Configuration)>,info,Started plug-in  p 
<org.apache.hadoop.hdfs.server.datanode.DataNode: void startPlugins(org.apache.hadoop.conf.Configuration)>,warn,ServicePlugin  p  could not be started 
"<org.apache.hadoop.hdfs.server.datanode.DataNode: void startDataNode(org.apache.hadoop.conf.Configuration,java.util.List,org.apache.hadoop.hdfs.server.datanode.SecureDataNodeStarter$SecureResources)>",info,Starting DataNode with maxLockedMemory =  <*> 
"<org.apache.hadoop.hdfs.server.datanode.DataNode: void startDataNode(org.apache.hadoop.conf.Configuration,java.util.List,org.apache.hadoop.hdfs.server.datanode.SecureDataNodeStarter$SecureResources)>",info,dnUserName =  <*> 
"<org.apache.hadoop.hdfs.server.datanode.DataNode: void startDataNode(org.apache.hadoop.conf.Configuration,java.util.List,org.apache.hadoop.hdfs.server.datanode.SecureDataNodeStarter$SecureResources)>",info,supergroup =  <*> 
<org.apache.hadoop.hdfs.server.datanode.DataNode: void shutdownDatanode(boolean)>,info,shutdownDatanode command received (upgrade= forUpgrade ). Shutting down Datanode... 
<org.apache.hadoop.hdfs.server.datanode.DataNode: void shutdown()>,info,Stopped plug-in  p 
<org.apache.hadoop.hdfs.server.datanode.DataNode: void shutdown()>,info,"Waiting for threadgroup to exit, active threads is  <*> "
<org.apache.hadoop.hdfs.server.datanode.DataNode: void shutdown()>,info,Shutdown complete.
<org.apache.hadoop.hdfs.server.datanode.DataNode: void shutdown()>,warn,ServicePlugin  p  could not be stopped 
<org.apache.hadoop.hdfs.server.datanode.DataNode: void shutdown()>,warn,Exception shutting down DataNode
<org.apache.hadoop.hdfs.server.datanode.DataNode: void shutdown()>,warn,Received exception in BlockPoolManager#shutDownAll: 
<org.apache.hadoop.hdfs.server.datanode.DataNode: void shutdown()>,warn,Exception when unlocking storage:  <*> 
"<org.apache.hadoop.hdfs.server.datanode.DataNode: void secureMain(java.lang.String[],org.apache.hadoop.hdfs.server.datanode.SecureDataNodeStarter$SecureResources)>",fatal,Exception in secureMain
"<org.apache.hadoop.hdfs.server.datanode.DataNode: void secureMain(java.lang.String[],org.apache.hadoop.hdfs.server.datanode.SecureDataNodeStarter$SecureResources)>",warn,Exiting Datanode
"<org.apache.hadoop.hdfs.server.datanode.DataNode: void secureMain(java.lang.String[],org.apache.hadoop.hdfs.server.datanode.SecureDataNodeStarter$SecureResources)>",warn,Exiting Datanode
"<org.apache.hadoop.hdfs.server.datanode.DataNode: void secureMain(java.lang.String[],org.apache.hadoop.hdfs.server.datanode.SecureDataNodeStarter$SecureResources)>",warn,Exiting Datanode
"<org.apache.hadoop.hdfs.server.datanode.DataNode: void registerBlockPoolWithSecretManager(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration,java.lang.String)>",info,"Block token params received from NN: for block pool  blockPoolId  keyUpdateInterval= <*>  min(s), tokenLifetime= <*>  min(s) "
<org.apache.hadoop.hdfs.server.datanode.DataNode: void refreshVolumes(java.lang.String)>,info,Adding new volumes:  <*> 
<org.apache.hadoop.hdfs.server.datanode.DataNode: void refreshVolumes(java.lang.String)>,info,Storage directory is loaded:  <*> 
<org.apache.hadoop.hdfs.server.datanode.DataNode: void refreshVolumes(java.lang.String)>,info,Deactivating volumes:  <*> 
<org.apache.hadoop.hdfs.server.datanode.DataNode: void recoverBlock(org.apache.hadoop.hdfs.server.protocol.BlockRecoveryCommand$RecoveringBlock)>,warn,Recovery for replica  <*>  on data-node  id  is already in progress. Recovery id =  <*>  is aborted. 
<org.apache.hadoop.hdfs.server.datanode.DataNode: void recoverBlock(org.apache.hadoop.hdfs.server.protocol.BlockRecoveryCommand$RecoveringBlock)>,warn,Failed to obtain replica info for block (= <*> ) from datanode (= id ) 
"<org.apache.hadoop.hdfs.server.datanode.DataNode: void reconfigurePropertyImpl(java.lang.String,java.lang.String)>",info,Reconfiguring  property  to  newVal 
"<org.apache.hadoop.hdfs.server.datanode.DataNode: void notifyNamenodeReceivingBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,java.lang.String)>",error,Cannot find BPOfferService for reporting block receiving for bpid= <*> 
"<org.apache.hadoop.hdfs.server.datanode.DataNode: void notifyNamenodeReceivedBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,java.lang.String,java.lang.String)>",error,Cannot find BPOfferService for reporting block received for bpid= <*> 
"<org.apache.hadoop.hdfs.server.datanode.DataNode: void notifyNamenodeDeletedBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,java.lang.String)>",error,Cannot find BPOfferService for reporting block deleted for bpid= <*> 
"<org.apache.hadoop.hdfs.server.datanode.DataNode: void logRecoverBlock(java.lang.String,org.apache.hadoop.hdfs.server.protocol.BlockRecoveryCommand$RecoveringBlock)>",info,"who  calls recoverBlock( <*> , targets=[ <*> ] , newGenerationStamp= <*> ) "
<org.apache.hadoop.hdfs.server.datanode.DataNode: void join()>,warn,Received exception in Datanode#join:  <*> 
<org.apache.hadoop.hdfs.server.datanode.DataNode: void initStorage(org.apache.hadoop.hdfs.server.protocol.NamespaceInfo)>,info,Setting up storage: nsid= <*> ;bpid= <*> ;lv= <*> ;nsInfo= nsInfo ;dnuuid= <*> 
<org.apache.hadoop.hdfs.server.datanode.DataNode: void initIpcServer(org.apache.hadoop.conf.Configuration)>,info,Opened IPC server at  <*> 
<org.apache.hadoop.hdfs.server.datanode.DataNode: void initDirectoryScanner(org.apache.hadoop.conf.Configuration)>,info,Periodic Directory Tree Verification scan is disabled because  reason#_ 
<org.apache.hadoop.hdfs.server.datanode.DataNode: void initDataXceiver(org.apache.hadoop.conf.Configuration)>,info,Opened streaming server at  <*> 
<org.apache.hadoop.hdfs.server.datanode.DataNode: void initDataXceiver(org.apache.hadoop.conf.Configuration)>,info,Listening on UNIX domain socket:  <*> 
<org.apache.hadoop.hdfs.server.datanode.DataNode: void initDataBlockScanner(org.apache.hadoop.conf.Configuration)>,info,Periodic Block Verification scan disabled because  reason#_ 
<org.apache.hadoop.hdfs.server.datanode.DataNode: void handleDiskError(java.lang.String)>,warn,DataNode.handleDiskError: Keep Running:  <*> 
<org.apache.hadoop.hdfs.server.datanode.DataNode: void handleDiskError(java.lang.String)>,warn,DataNode is shutting down:  errMsgr 
"<org.apache.hadoop.hdfs.server.datanode.DataNode: void deleteBlockPool(java.lang.String,boolean)>",info,"deleteBlockPool command received for block pool  blockPoolId , force= force "
"<org.apache.hadoop.hdfs.server.datanode.DataNode: void deleteBlockPool(java.lang.String,boolean)>",warn,"The block pool  blockPoolId  is still running, cannot be deleted. "
"<org.apache.hadoop.hdfs.server.datanode.DataNode: void closeBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,java.lang.String,java.lang.String)>",warn,Cannot find BPOfferService for reporting block received for bpid= <*> 
<org.apache.hadoop.hdfs.server.datanode.DataNode: void checkReadAccess(org.apache.hadoop.hdfs.protocol.ExtendedBlock)>,debug,Got:  <*> 
<org.apache.hadoop.hdfs.server.datanode.DataNode: void checkDiskErrorAsync()>,info,Starting CheckDiskError Thread
<org.apache.hadoop.hdfs.server.datanode.DataNode: void checkDatanodeUuid()>,info,Generated and persisted new Datanode UUID  <*> 
"<org.apache.hadoop.hdfs.server.datanode.DataNode: void checkBlockToken(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager$AccessMode)>",debug,Got:  <*> 
"<org.apache.hadoop.hdfs.server.datanode.DataNode: void <init>(org.apache.hadoop.conf.Configuration,java.util.List,org.apache.hadoop.hdfs.server.datanode.SecureDataNodeStarter$SecureResources)>",info,File descriptor passing is enabled.
"<org.apache.hadoop.hdfs.server.datanode.DataNode: void <init>(org.apache.hadoop.conf.Configuration,java.util.List,org.apache.hadoop.hdfs.server.datanode.SecureDataNodeStarter$SecureResources)>",info,Configured hostname is  <*> 
"<org.apache.hadoop.hdfs.server.datanode.DataNode: void <init>(org.apache.hadoop.conf.Configuration,java.util.List,org.apache.hadoop.hdfs.server.datanode.SecureDataNodeStarter$SecureResources)>",warn,File descriptor passing is disabled because  <*> 
"<org.apache.hadoop.hdfs.server.datanode.DataNode: org.apache.hadoop.hdfs.server.protocol.InterDatanodeProtocol createInterDataNodeProtocolProxy(org.apache.hadoop.hdfs.protocol.DatanodeID,org.apache.hadoop.conf.Configuration,int,boolean)>",debug,Connecting to datanode  <*>  addr= <*> 
"<org.apache.hadoop.hdfs.server.datanode.DataNode: org.apache.hadoop.hdfs.protocol.BlockLocalPathInfo getBlockLocalPathInfo(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token)>",trace,getBlockLocalPathInfo successful block= block  blockfile  <*>  metafile  <*> 
"<org.apache.hadoop.hdfs.server.datanode.DataNode: org.apache.hadoop.hdfs.protocol.BlockLocalPathInfo getBlockLocalPathInfo(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token)>",trace,getBlockLocalPathInfo for block= block  returning null 
"<org.apache.hadoop.hdfs.server.datanode.DataNode: org.apache.hadoop.hdfs.net.DomainPeerServer getDomainPeerServer(org.apache.hadoop.conf.Configuration,int)>",warn,"Although short-circuit local reads are configured, they are disabled because you didn\'t configure dfs.domain.socket.path"
<org.apache.hadoop.hdfs.server.datanode.DataNode: java.util.List getStorageLocations(org.apache.hadoop.conf.Configuration)>,error,Failed to initialize storage directory  locationString . Exception details:  <*> 
<org.apache.hadoop.hdfs.server.datanode.DataNode: java.util.List getStorageLocations(org.apache.hadoop.conf.Configuration)>,error,Failed to initialize storage directory  locationString . Exception details:  <*> 
"<org.apache.hadoop.hdfs.server.datanode.DataNode: java.util.List checkStorageLocations(java.util.Collection,org.apache.hadoop.fs.LocalFileSystem,org.apache.hadoop.hdfs.server.datanode.DataNode$DataNodeDiskChecker)>",warn,Invalid dfs.datanode.data.dir  <*>  :  
"<org.apache.hadoop.hdfs.server.datanode.DataNode: java.io.FileInputStream[] requestShortCircuitFdsForRead(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token,int)>",debug,requestShortCircuitFdsForRead failed
"<org.apache.hadoop.hdfs.server.datanode.DataNode: boolean parseArguments(java.lang.String[],org.apache.hadoop.conf.Configuration)>",error,"-r, --rack arguments are not supported anymore. RackID resolution is handled by the NameNode."
<org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer: void run()>,debug,Connecting to datanode  <*> 
<org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer: void run()>,debug,<*> : close-ack= <*> 
<org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer: void run()>,info,<*> : Transmitted  <*>  (numBytes= <*> ) to  <*> 
<org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer: void run()>,warn,<*> :Failed to transfer  <*>  to  <*>  got  
"<org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer: void <init>(org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.hdfs.StorageType[],org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.datatransfer.BlockConstructionStage,java.lang.String)>",debug,"<*> :  b  (numBytes= <*> ) , stage= stage , clientname= clientname , targets= <*> , target storage types= <*> "
<org.apache.hadoop.hdfs.server.datanode.DataNode$6: void run()>,debug,InterruptedException in check disk error thread
<org.apache.hadoop.hdfs.server.datanode.DataNode$6: void run()>,warn,Unexpected exception occurred while checking disk error   <*> 
<org.apache.hadoop.hdfs.server.datanode.DataNode$4: void run()>,warn,recoverBlocks FAILED:  b 
<org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: void run()>,warn,Block Pool  <*>  is not alive 
<org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: void removeBlockPool(java.lang.String)>,info,Removed bpid= blockPoolId  from blockPoolScannerMap 
"<org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: void deleteBlocks(java.lang.String,org.apache.hadoop.hdfs.protocol.Block[])>",warn,No block pool scanner found for block pool id:  poolId 
"<org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: void deleteBlock(java.lang.String,org.apache.hadoop.hdfs.protocol.Block)>",warn,No block pool scanner found for block pool id:  poolId 
<org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: void addBlockPool(java.lang.String)>,info,"Added bpid= blockPoolId  to blockPoolScannerMap, new size= <*> "
<org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: void addBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock)>,warn,No block pool scanner found for block pool id:  <*> 
<org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner getNextBPScanner(java.lang.String)>,warn,"No block pool is up, going to wait"
<org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner getNextBPScanner(java.lang.String)>,warn,Received exception:  <*> 
"<org.apache.hadoop.hdfs.server.datanode.DataBlockScanner$Servlet: void doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)>",append,Periodic block scanner is not running. Please check the datanode log if this is unexpected.
"<org.apache.hadoop.hdfs.server.datanode.DataBlockScanner$Servlet: void doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)>",warn,Periodic block scanner is not running
"<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void trySendErrorReport(int,java.lang.String)>",warn,Error reporting an error to NameNode  <*> 
<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void triggerBlockReport(org.apache.hadoop.hdfs.client.BlockReportOptions)>,info,<*> : scheduling an incremental block report. 
<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void triggerBlockReport(org.apache.hadoop.hdfs.client.BlockReportOptions)>,info,<*> : scheduling a full block report. 
"<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void sleepAndLogInterrupts(int,java.lang.String)>",info,BPOfferService  this  interrupted while  stateString 
<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void run()>,error,Initialization failed for  this   <*> 
<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void run()>,error,Exception in BPOfferService for  this 
<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void run()>,fatal,Initialization failed for  this . Exiting.  
<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void run()>,info,this  starting to offer service 
<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void run()>,warn,Ending block pool service for:  this 
<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void run()>,warn,Ending block pool service for:  this 
<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void run()>,warn,Unexpected exception in block pool  this 
<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void run()>,warn,Ending block pool service for:  this 
<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void run()>,warn,Ending block pool service for:  this 
"<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void reportBadBlocks(org.apache.hadoop.hdfs.protocol.ExtendedBlock,java.lang.String,org.apache.hadoop.hdfs.StorageType)>",warn,Failed to report bad block  block  to namenode :   Exception 
<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void register()>,info,this  beginning handshake with NN 
<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void register()>,info,Problem connecting to server:  <*>  : <*> 
<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void register()>,info,Problem connecting to server:  <*> 
<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void register()>,info,Block pool  this  successfully registered with NN 
<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void offerService()>,info,For namenode  <*>  using  DELETEREPORT_INTERVAL of  <*>  msec   BLOCKREPORT_INTERVAL of  <*> msec  CACHEREPORT_INTERVAL of  <*> msec  Initial delay:  <*> msec ; heartBeatInterval= <*> 
<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void offerService()>,info,Took  <*> ms to process  <*>  commands from NN 
<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void offerService()>,warn,BPOfferService for  this  interrupted 
<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void offerService()>,warn,this  is shutting down 
<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void offerService()>,warn,RemoteException in offerService
<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void offerService()>,warn,IOException in offerService
<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void handleRollingUpgradeStatus(org.apache.hadoop.hdfs.server.protocol.HeartbeatResponse)>,error,Invalid BlockPoolId  <*>  in HeartbeatResponse. Expected  <*> 
<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void checkNNVersion(org.apache.hadoop.hdfs.server.protocol.NamespaceInfo)>,info,Reported NameNode version \' <*> \' does not match  DataNode version \' <*> \' but is within acceptable  limits. Note: This is normal during a rolling upgrade. 
<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: org.apache.hadoop.hdfs.server.protocol.NamespaceInfo retrieveNamespaceInfo()>,debug,this  received versionRequest response:  <*> 
<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: org.apache.hadoop.hdfs.server.protocol.NamespaceInfo retrieveNamespaceInfo()>,warn,Problem connecting to server:  <*> 
<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: org.apache.hadoop.hdfs.server.protocol.NamespaceInfo retrieveNamespaceInfo()>,warn,Problem connecting to server:  <*> 
<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: org.apache.hadoop.hdfs.server.protocol.HeartbeatResponse sendHeartBeat()>,debug,Sending heartbeat with  <*>  storage reports from service actor:  this 
<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: org.apache.hadoop.hdfs.server.protocol.DatanodeCommand cacheReport()>,debug,Sending cacheReport from service actor:  this 
<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: org.apache.hadoop.hdfs.server.protocol.DatanodeCommand cacheReport()>,debug,CacheReport of  <*>  block(s) took  <*>  msec to generate and  <*>  msecs for RPC and NN processing 
<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: java.util.List blockReport()>,info,"<*> uccessfully sent block report x <*> ,  containing  <*>  storage report(s), of which we sent  i$ .  The reports had  totalBlockCount  total blocks and used  kvPair#  RPC(s). This took  <*>  msec to generate and  <*>  msecs for RPC and NN processing.  Got back  <*> . "
<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: java.util.List blockReport()>,info,"<*> uccessfully sent block report x <*> ,  containing  <*>  storage report(s), of which we sent  i$ .  The reports had  totalBlockCount  total blocks and used  kvPair#  RPC(s). This took  brCreateCost  msec to generate and  brSendCost  msecs for RPC and NN processing.  Got back  <*> . "
<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: boolean processCommand(org.apache.hadoop.hdfs.server.protocol.DatanodeCommand[])>,warn,Error processing datanode Command
"<org.apache.hadoop.hdfs.server.datanode.BPOfferService: void updateActorStatesFromHeartbeat(org.apache.hadoop.hdfs.server.datanode.BPServiceActor,org.apache.hadoop.hdfs.server.protocol.NNHAStatusHeartbeat)>",info,Namenode  actor  trying to claim ACTIVE state with  txid= <*> 
"<org.apache.hadoop.hdfs.server.datanode.BPOfferService: void updateActorStatesFromHeartbeat(org.apache.hadoop.hdfs.server.datanode.BPServiceActor,org.apache.hadoop.hdfs.server.protocol.NNHAStatusHeartbeat)>",info,Acknowledging ACTIVE Namenode  actor 
"<org.apache.hadoop.hdfs.server.datanode.BPOfferService: void updateActorStatesFromHeartbeat(org.apache.hadoop.hdfs.server.datanode.BPServiceActor,org.apache.hadoop.hdfs.server.protocol.NNHAStatusHeartbeat)>",info,Namenode  actor  taking over ACTIVE state from  <*>  at higher txid= <*> 
"<org.apache.hadoop.hdfs.server.datanode.BPOfferService: void updateActorStatesFromHeartbeat(org.apache.hadoop.hdfs.server.datanode.BPServiceActor,org.apache.hadoop.hdfs.server.protocol.NNHAStatusHeartbeat)>",info,Namenode  actor  relinquishing ACTIVE state with  txid= <*> 
"<org.apache.hadoop.hdfs.server.datanode.BPOfferService: void updateActorStatesFromHeartbeat(org.apache.hadoop.hdfs.server.datanode.BPServiceActor,org.apache.hadoop.hdfs.server.protocol.NNHAStatusHeartbeat)>",warn,NN  actor  tried to claim ACTIVE state at txid= <*>  but there was already a more recent claim at txid= <*> 
"<org.apache.hadoop.hdfs.server.datanode.BPOfferService: void reportRemoteBadBlock(org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.protocol.ExtendedBlock)>",warn,Couldn\'t report bad block  block  to  actor 
"<org.apache.hadoop.hdfs.server.datanode.BPOfferService: boolean processCommandFromStandby(org.apache.hadoop.hdfs.server.protocol.DatanodeCommand,org.apache.hadoop.hdfs.server.datanode.BPServiceActor)>",info,DatanodeCommand action from standby: DNA_ACCESSKEYUPDATE
"<org.apache.hadoop.hdfs.server.datanode.BPOfferService: boolean processCommandFromStandby(org.apache.hadoop.hdfs.server.protocol.DatanodeCommand,org.apache.hadoop.hdfs.server.datanode.BPServiceActor)>",warn,Got a command from standby NN - ignoring command: <*> 
"<org.apache.hadoop.hdfs.server.datanode.BPOfferService: boolean processCommandFromStandby(org.apache.hadoop.hdfs.server.protocol.DatanodeCommand,org.apache.hadoop.hdfs.server.datanode.BPServiceActor)>",warn,Unknown DatanodeCommand action:  <*> 
"<org.apache.hadoop.hdfs.server.datanode.BPOfferService: boolean processCommandFromActor(org.apache.hadoop.hdfs.server.protocol.DatanodeCommand,org.apache.hadoop.hdfs.server.datanode.BPServiceActor)>",info,DatanodeCommand action : DNA_REGISTER from  <*>  with  <*>  state 
"<org.apache.hadoop.hdfs.server.datanode.BPOfferService: boolean processCommandFromActive(org.apache.hadoop.hdfs.server.protocol.DatanodeCommand,org.apache.hadoop.hdfs.server.datanode.BPServiceActor)>",info,DatanodeCommand action: DNA_CACHE for  <*>  of [ <*> ] 
"<org.apache.hadoop.hdfs.server.datanode.BPOfferService: boolean processCommandFromActive(org.apache.hadoop.hdfs.server.protocol.DatanodeCommand,org.apache.hadoop.hdfs.server.datanode.BPServiceActor)>",info,DatanodeCommand action: DNA_UNCACHE for  <*>  of [ <*> ] 
"<org.apache.hadoop.hdfs.server.datanode.BPOfferService: boolean processCommandFromActive(org.apache.hadoop.hdfs.server.protocol.DatanodeCommand,org.apache.hadoop.hdfs.server.datanode.BPServiceActor)>",info,Got finalize command for block pool  <*> 
"<org.apache.hadoop.hdfs.server.datanode.BPOfferService: boolean processCommandFromActive(org.apache.hadoop.hdfs.server.protocol.DatanodeCommand,org.apache.hadoop.hdfs.server.datanode.BPServiceActor)>",info,DatanodeCommand action: DNA_ACCESSKEYUPDATE
"<org.apache.hadoop.hdfs.server.datanode.BPOfferService: boolean processCommandFromActive(org.apache.hadoop.hdfs.server.protocol.DatanodeCommand,org.apache.hadoop.hdfs.server.datanode.BPServiceActor)>",info,DatanodeCommand action: DNA_BALANCERBANDWIDTHUPDATE
"<org.apache.hadoop.hdfs.server.datanode.BPOfferService: boolean processCommandFromActive(org.apache.hadoop.hdfs.server.protocol.DatanodeCommand,org.apache.hadoop.hdfs.server.datanode.BPServiceActor)>",info,Updating balance throttler bandwidth from  <*>  bytes/s  to:  <*>  bytes/s. 
"<org.apache.hadoop.hdfs.server.datanode.BPOfferService: boolean processCommandFromActive(org.apache.hadoop.hdfs.server.protocol.DatanodeCommand,org.apache.hadoop.hdfs.server.datanode.BPServiceActor)>",warn,Unknown DatanodeCommand action:  <*> 
"<org.apache.hadoop.hdfs.server.datanode.BlockSender: void readChecksum(byte[],int,int)>",warn, Could not read or failed to veirfy checksum for data at offset  <*>  for block  <*> 
<org.apache.hadoop.hdfs.server.datanode.BlockSender: void close()>,warn,Unable to drop cache on file close
"<org.apache.hadoop.hdfs.server.datanode.BlockSender: void <init>(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,long,boolean,boolean,boolean,org.apache.hadoop.hdfs.server.datanode.DataNode,java.lang.String,org.apache.hadoop.hdfs.server.datanode.CachingStrategy)>",debug,Bumping up the client provided block\'s genstamp to latest  <*>  for block  block 
"<org.apache.hadoop.hdfs.server.datanode.BlockSender: void <init>(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,long,boolean,boolean,boolean,org.apache.hadoop.hdfs.server.datanode.DataNode,java.lang.String,org.apache.hadoop.hdfs.server.datanode.CachingStrategy)>",debug,"block= block , replica= <*> "
"<org.apache.hadoop.hdfs.server.datanode.BlockSender: void <init>(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,long,boolean,boolean,boolean,org.apache.hadoop.hdfs.server.datanode.DataNode,java.lang.String,org.apache.hadoop.hdfs.server.datanode.CachingStrategy)>",debug,replica= <*> 
"<org.apache.hadoop.hdfs.server.datanode.BlockSender: void <init>(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,long,boolean,boolean,boolean,org.apache.hadoop.hdfs.server.datanode.DataNode,java.lang.String,org.apache.hadoop.hdfs.server.datanode.CachingStrategy)>",warn,Could not find metadata file for  block 
"<org.apache.hadoop.hdfs.server.datanode.BlockSender: void <init>(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,long,boolean,boolean,boolean,org.apache.hadoop.hdfs.server.datanode.DataNode,java.lang.String,org.apache.hadoop.hdfs.server.datanode.CachingStrategy)>",warn,<*> :sendBlock() :  <*> 
"<org.apache.hadoop.hdfs.server.datanode.BlockSender: int sendPacket(java.nio.ByteBuffer,int,java.io.OutputStream,boolean,org.apache.hadoop.hdfs.util.DataTransferThrottler)>",error,BlockSender.sendChunks() exception: 
"<org.apache.hadoop.hdfs.server.datanode.BlockSender: int sendPacket(java.nio.ByteBuffer,int,java.io.OutputStream,boolean,org.apache.hadoop.hdfs.util.DataTransferThrottler)>",info,Failed to send data:  <*> 
"<org.apache.hadoop.hdfs.server.datanode.BlockSender: int sendPacket(java.nio.ByteBuffer,int,java.io.OutputStream,boolean,org.apache.hadoop.hdfs.util.DataTransferThrottler)>",trace,Failed to send data:
"<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: void verifyChunks(java.nio.ByteBuffer,java.nio.ByteBuffer)>",info,report corrupt  <*>  from datanode  <*>  to namenode 
"<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: void verifyChunks(java.nio.ByteBuffer,java.nio.ByteBuffer)>",warn,Checksum error in block  <*>  from  <*> 
"<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: void verifyChunks(java.nio.ByteBuffer,java.nio.ByteBuffer)>",warn,Failed to report bad  <*>  from datanode  <*>  to namenode 
"<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: void receiveBlock(java.io.DataOutputStream,java.io.DataInputStream,java.io.DataOutputStream,java.lang.String,org.apache.hadoop.hdfs.util.DataTransferThrottler,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],boolean)>",info,Shutting down for restart ( <*> ). 
"<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: void receiveBlock(java.io.DataOutputStream,java.io.DataInputStream,java.io.DataOutputStream,java.lang.String,org.apache.hadoop.hdfs.util.DataTransferThrottler,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],boolean)>",info,Exception for  <*> 
"<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: void receiveBlock(java.io.DataOutputStream,java.io.DataInputStream,java.io.DataOutputStream,java.lang.String,org.apache.hadoop.hdfs.util.DataTransferThrottler,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],boolean)>",warn,Failed to delete restart meta file:  <*> 
"<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: void receiveBlock(java.io.DataOutputStream,java.io.DataInputStream,java.io.DataOutputStream,java.lang.String,org.apache.hadoop.hdfs.util.DataTransferThrottler,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],boolean)>",warn,<*> \n <*> 
"<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: void receiveBlock(java.io.DataOutputStream,java.io.DataInputStream,java.io.DataOutputStream,java.lang.String,org.apache.hadoop.hdfs.util.DataTransferThrottler,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],boolean)>",warn,Failed to delete restart meta file:  <*> 
"<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: void receiveBlock(java.io.DataOutputStream,java.io.DataInputStream,java.io.DataOutputStream,java.lang.String,org.apache.hadoop.hdfs.util.DataTransferThrottler,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],boolean)>",warn,<*> \n <*> 
"<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: void receiveBlock(java.io.DataOutputStream,java.io.DataInputStream,java.io.DataOutputStream,java.lang.String,org.apache.hadoop.hdfs.util.DataTransferThrottler,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],boolean)>",warn,Failed to delete restart meta file:  <*> 
"<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: void receiveBlock(java.io.DataOutputStream,java.io.DataInputStream,java.io.DataOutputStream,java.lang.String,org.apache.hadoop.hdfs.util.DataTransferThrottler,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],boolean)>",warn,<*> \n <*> 
<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: void manageWriterOsCache(long)>,warn,Slow manageWriterOsCache took  <*> ms (threshold= <*> ms) 
<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: void manageWriterOsCache(long)>,warn,Error managing cache for writer of block  <*> 
<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: void handleMirrorOutError(java.io.IOException)>,info,<*> :Exception writing  <*>  to mirror  <*> 
<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: void flushOrSync(boolean)>,warn,"Slow flushOrSync took  <*> ms (threshold= <*> ms), isSync: isSync , flushTotalNanos= flushTotalNanos ns "
"<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: void <init>(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.StorageType,java.io.DataInputStream,java.lang.String,java.lang.String,org.apache.hadoop.hdfs.protocol.datatransfer.BlockConstructionStage,long,long,long,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.util.DataChecksum,org.apache.hadoop.hdfs.server.datanode.CachingStrategy,boolean)>",debug,"<*> :  block \n  isClient  = <*> , clientname= clientname \n  isDatanode= <*> , srcDataNode= srcDataNode \n  inAddr= inAddr , myAddr= myAddr \n  cachingStrategy =  cachingStrategy "
"<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: void <init>(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.StorageType,java.io.DataInputStream,java.lang.String,java.lang.String,org.apache.hadoop.hdfs.protocol.datatransfer.BlockConstructionStage,long,long,long,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.util.DataChecksum,org.apache.hadoop.hdfs.server.datanode.CachingStrategy,boolean)>",warn,Could not get file descriptor for outputstream of class  <*> 
"<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: void <init>(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.StorageType,java.io.DataInputStream,java.lang.String,java.lang.String,org.apache.hadoop.hdfs.protocol.datatransfer.BlockConstructionStage,long,long,long,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.util.DataChecksum,org.apache.hadoop.hdfs.server.datanode.CachingStrategy,boolean)>",warn,IOException in BlockReceiver constructor. Cause is 
"<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: java.util.zip.Checksum computePartialChunkCrc(long,long)>",debug,"computePartialChunkCrc for  <*> : sizePartialChunk= sizePartialChunk , block offset= blkoff# , metafile offset= ckoff "
"<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: java.util.zip.Checksum computePartialChunkCrc(long,long)>",debug,Read in partial CRC chunk from disk for  <*> 
<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: int receivePacket()>,debug,Receiving one packet for block  <*> :  <*> 
<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: int receivePacket()>,debug,Receiving an empty packet or the end of the block  <*> 
<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: int receivePacket()>,debug,receivePacket for  <*> : previous write did not end at the chunk boundary.  onDiskLen= <*> 
<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: int receivePacket()>,debug,"Writing out partial crc for data len  <*> , skip=  "
<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: int receivePacket()>,warn,Slow BlockReceiver write packet to mirror took  duration_ ms (threshold= <*> ms) 
<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: int receivePacket()>,warn,Slow BlockReceiver write data to disk cost: <*> ms (threshold= <*> ms) 
<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: boolean packetSentInTime()>,info,A packet was last sent  diff  milliseconds ago. 
<org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder: void sendOOBResponse(org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$Status)>,info,Cannot send OOB response  ackStatus . Responder not running. 
<org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder: void sendOOBResponse(org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$Status)>,info,Sending an out of band ack of type  ackStatus 
"<org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder: void sendAckUpstreamUnprotected(org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck,long,long,long,org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$Status)>",debug,"<*> , replyAck= $u "
"<org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder: void sendAckUpstreamUnprotected(org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck,long,long,long,org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$Status)>",warn,"Slow PacketResponder send ack to upstream took  <*> ms (threshold= <*> ms),  <*> , replyAck= $u "
<org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder: void run()>,debug,<*>  got  $u 
<org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder: void run()>,debug,Calculated invalid ack time:  oobStatus ns. 
<org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder: void run()>,info,Relaying an out of band ack of type  <*> 
<org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder: void run()>,info,<*> : Thread is interrupted. 
<org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder: void run()>,info,<*>  terminating 
<org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder: void run()>,warn,The downstream error might be due to congestion in upstream including this node. Propagating the error: 
<org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder: void run()>,warn,IOException in BlockReceiver.run(): 
<org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder: void finalizeBlock(long)>,info,Received  <*>  size  <*>  from  <*> 
"<org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder: void enqueue(long,boolean,long,org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$Status)>",debug,<*> : enqueue  <*> 
<org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder: void close()>,debug,<*> : closing 
<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage: void setRollingUpgradeMarkers(java.util.List)>,info,Created  <*> 
<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage: void setRollingUpgradeMarkers(java.util.List)>,info,<*>  already exists. 
<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage: void restoreTrash()>,warn,Restoring trash failed for storage directory  sd 
"<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage: void recoverTransitionRead(org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo,java.util.Collection,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption)>",info,Analyzing storage directories for bpid  <*> 
"<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage: void format(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo)>",info,Formatting block pool  <*>  directory  <*> 
"<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage: void doUpgrade(org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo)>",info,Upgrading block pool storage directory  <*> .\n   old LV =  <*> ; old CTime =  <*> .\n   new LV =  <*> ; new CTime =  <*> 
"<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage: void doUpgrade(org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo)>",info,Upgrade of block pool  <*>  at  <*>  is complete 
"<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage: void doTransition(org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption)>",info,Restored  <*>  block files from trash. 
"<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage: void doRollback(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo)>",info,Rolling back storage directory  <*> .\n   target LV =  <*> ; target CTime =  <*> 
"<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage: void doRollback(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo)>",info,Rollback of  <*>  is complete 
<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage: void doFinalize(java.io.File)>,info,Finalizing upgrade for storage directory  <*> .\n   cur LV =  <*> ; cur CTime =  <*> 
<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage: void clearRollingUpgradeMarkers(java.util.List)>,info,Deleting  <*> 
<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage: void clearRollingUpgradeMarkers(java.util.List)>,warn,Failed to delete  <*> 
"<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage: org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory loadStorageDirectory(org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo,java.io.File,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption)>",info,Block pool storage directory  dataDir  does not exist 
"<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage: org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory loadStorageDirectory(org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo,java.io.File,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption)>",info,Block pool storage directory  dataDir  is not formatted for  <*> 
"<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage: org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory loadStorageDirectory(org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo,java.io.File,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption)>",info,Formatting ...
"<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage: java.util.List loadBpStorageDirectories(org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo,java.util.Collection,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption)>",warn,Failed to analyze storage directories for block pool  <*> 
<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage: java.lang.String getRestoreDirectory(java.io.File)>,info,Restoring  blockFile  to  <*> 
<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage: int restoreBlockFilesFromTrash(java.io.File)>,info,Not overwriting  $u  with smaller file from  trash directory. This message can be safely ignored. 
<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage$1: void run()>,error,Finalize upgrade for  <*>  failed. 
<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage$1: void run()>,info,Finalize upgrade for  <*>  is complete. 
<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: void verifyBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock)>,info,<*> Verification succeeded for  block 
<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: void verifyBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock)>,info,block  is no longer in the dataset 
<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: void verifyBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock)>,info,Verification failed for  block  - may be due to race with write 
<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: void verifyBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock)>,warn,<*> Verification failed for  block 
<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: void startNewPeriod()>,info,Starting a new period : work left in prev period :  <*> 
<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: void scan()>,debug,Starting to scan blockpool:  <*> 
<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: void scan()>,debug,"All remaining blocks were processed recently, so this run is complete"
<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: void scan()>,debug,Done scanning block pool:  <*> 
<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: void scan()>,debug,Done scanning block pool:  <*> 
<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: void scan()>,warn,RuntimeException during BlockPoolScanner.scan()
<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: void rollVerificationLogs()>,warn,Received exception: 
<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: void handleScanFailure(org.apache.hadoop.hdfs.protocol.ExtendedBlock)>,info,Reporting bad  block 
<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: void handleScanFailure(org.apache.hadoop.hdfs.protocol.ExtendedBlock)>,warn,Cannot report bad  <*> 
<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: void addBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock)>,warn,Adding an already existing block  block 
"<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: void <init>(java.lang.String,org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi,org.apache.hadoop.conf.Configuration)>",info,Periodic Block Verification Scanner initialized with interval  hours  hours for block pool  bpid 
"<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: void <init>(java.lang.String,org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi,org.apache.hadoop.conf.Configuration)>",warn,Could not open verfication log. Verification times are not stored.
<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: boolean workRemainingInCurrentPeriod()>,debug,"Skipping scan since bytesLeft= <*> , Start= <*> , period= <*> , now= <*>   <*> "
<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: boolean assignInitialVerificationTimes()>,warn,Failed to read previous verification times.
<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner$LogFileHandler: void close()>,warn,Failed to close the appender of  <*> 
"<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner$LogFileHandler: void append(long,long,long)>",warn,"Failed to append to  <*> , m= <*> "
<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner$LogEntry: org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner$LogEntry parseEntry(java.lang.String)>,warn,Cannot parse line:  line 
<org.apache.hadoop.hdfs.server.datanode.BlockPoolManager: void remove(org.apache.hadoop.hdfs.server.datanode.BPOfferService)>,info,Removed  bpos 
<org.apache.hadoop.hdfs.server.datanode.BlockPoolManager: void remove(org.apache.hadoop.hdfs.server.datanode.BPOfferService)>,warn,Couldn\'t remove BPOS  t  from bpByNameserviceId map 
<org.apache.hadoop.hdfs.server.datanode.BlockPoolManager: void refreshNamenodes(org.apache.hadoop.conf.Configuration)>,info,Refresh request received for nameservices:  <*> 
<org.apache.hadoop.hdfs.server.datanode.BlockPoolManager: void doRefreshNamenodes(java.util.Map)>,info,Starting BPOfferServices for nameservices:  <*> 
<org.apache.hadoop.hdfs.server.datanode.BlockPoolManager: void doRefreshNamenodes(java.util.Map)>,info,Stopping BPOfferServices for nameservices:  <*> 
<org.apache.hadoop.hdfs.server.datanode.BlockPoolManager: void doRefreshNamenodes(java.util.Map)>,info,Refreshing list of NNs for nameservices:  <*> 
"<org.apache.hadoop.hdfs.server.datanode.BlockMetadataHeader: org.apache.hadoop.util.DataChecksum readDataChecksum(java.io.DataInputStream,java.lang.Object)>",warn,Unexpected meta-file version for  name : version in file is  $i  but expected version is   
<org.apache.hadoop.hdfs.server.common.Util: java.util.List stringCollectionAsURIs(java.util.Collection)>,error,Error while processing URI:  name 
<org.apache.hadoop.hdfs.server.common.Util: java.net.URI stringAsURI(java.lang.String)>,error,Syntax error in URI  s . Please check hdfs configuration. 
<org.apache.hadoop.hdfs.server.common.Util: java.net.URI stringAsURI(java.lang.String)>,warn,Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
"<org.apache.hadoop.hdfs.server.common.Storage: void nativeCopyFileUnbuffered(java.io.File,java.io.File,boolean)>",debug,Failed to preserve last modified date from\' srcFile \' to \' destFile \' 
<org.apache.hadoop.hdfs.server.common.Storage: void checkVersionUpgradable(int)>,error,*********** Upgrade is not supported from this  older version  oldVersion  of storage to the current version.  Please upgrade to  Hadoop-.  or a later version and then upgrade to current  version. Old layout version is  <*>  and latest layout version this software version can  upgrade from is  $i . ************ 
<org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: void lock()>,info,Locking is disabled for  <*> 
<org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: void lock()>,info,Cannot lock storage  <*> . The directory is already locked 
<org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: void doRecover(org.apache.hadoop.hdfs.server.common.Storage$StorageState)>,info,Completing previous upgrade for storage directory  <*> 
<org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: void doRecover(org.apache.hadoop.hdfs.server.common.Storage$StorageState)>,info,Recovering storage directory  <*>  from previous upgrade 
<org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: void doRecover(org.apache.hadoop.hdfs.server.common.Storage$StorageState)>,info,Completing previous rollback for storage directory  <*> 
<org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: void doRecover(org.apache.hadoop.hdfs.server.common.Storage$StorageState)>,info,Recovering storage directory  <*>  from previous rollback 
<org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: void doRecover(org.apache.hadoop.hdfs.server.common.Storage$StorageState)>,info,Completing previous finalize for storage directory  <*> 
<org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: void doRecover(org.apache.hadoop.hdfs.server.common.Storage$StorageState)>,info,Completing previous checkpoint for storage directory  <*> 
<org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: void doRecover(org.apache.hadoop.hdfs.server.common.Storage$StorageState)>,info,Recovering storage directory  <*>  from failed checkpoint 
"<org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: org.apache.hadoop.hdfs.server.common.Storage$StorageState analyzeStorage(org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.common.Storage)>",info,<*>  does not exist. Creating ... 
"<org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: org.apache.hadoop.hdfs.server.common.Storage$StorageState analyzeStorage(org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.common.Storage)>",warn,Storage directory  <*>  does not exist 
"<org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: org.apache.hadoop.hdfs.server.common.Storage$StorageState analyzeStorage(org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.common.Storage)>",warn,<*> is not a directory 
"<org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: org.apache.hadoop.hdfs.server.common.Storage$StorageState analyzeStorage(org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.common.Storage)>",warn,Cannot access storage directory  <*> 
"<org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: org.apache.hadoop.hdfs.server.common.Storage$StorageState analyzeStorage(org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.common.Storage)>",warn,Cannot access storage directory  <*> 
<org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: java.nio.channels.FileLock tryLock()>,error,It appears that another namenode <*>  has already locked the storage directory 
<org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: java.nio.channels.FileLock tryLock()>,error,"Failed to acquire lock on  <*> . If this storage directory is mounted via NFS,  ensure that the appropriate nfs lock services are running. "
<org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: java.nio.channels.FileLock tryLock()>,info,Lock on  <*>  acquired by nodename  <*> 
"<org.apache.hadoop.hdfs.server.common.JspHelper: org.apache.hadoop.security.UserGroupInformation getUGI(javax.servlet.ServletContext,javax.servlet.http.HttpServletRequest,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod,boolean)>",debug,getUGI is returning:  <*> 
"<org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: void update(org.apache.hadoop.hdfs.protocol.Block,int,int,int,int,int)>",debug,UnderReplicationBlocks.update  block  curReplicas  curReplicas  curExpectedReplicas  curExpectedReplicas  oldReplicas  oldReplicas  oldExpectedReplicas   oldExpectedReplicas  curPri   <*>  oldPri   <*> 
"<org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: void update(org.apache.hadoop.hdfs.protocol.Block,int,int,int,int,int)>",debug,BLOCK* NameSystem.UnderReplicationBlock.update: block  has only  curReplicas  replicas and needs  curExpectedReplicas  replicas so is added to neededReplications  at priority level  <*> 
"<org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: boolean remove(org.apache.hadoop.hdfs.protocol.Block,int)>",debug,BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  priLevel 
"<org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: boolean remove(org.apache.hadoop.hdfs.protocol.Block,int)>",debug,BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  i 
"<org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: boolean add(org.apache.hadoop.hdfs.protocol.Block,int,int,int)>",debug,BLOCK* NameSystem.UnderReplicationBlock.add: block  has only  curReplicas  replicas and need  expectedReplicas  replicas so is added to neededReplications  at priority level  <*> 
"<org.apache.hadoop.hdfs.server.blockmanagement.PendingReplicationBlocks: void decrement(org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>",debug,Removing pending replication for  block 
<org.apache.hadoop.hdfs.server.blockmanagement.PendingReplicationBlocks$PendingReplicationMonitor: void run()>,debug,PendingReplicationMonitor thread is interrupted.
<org.apache.hadoop.hdfs.server.blockmanagement.PendingReplicationBlocks$PendingReplicationMonitor: void pendingReplicationCheck()>,debug,PendingReplicationMonitor checking Q
<org.apache.hadoop.hdfs.server.blockmanagement.PendingReplicationBlocks$PendingReplicationMonitor: void pendingReplicationCheck()>,warn,PendingReplicationMonitor timed out  block 
<org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: void printBlockDeletionTime(org.apache.commons.logging.Log)>,info,dfs.namenode.startup.delay.block.deletion.sec is set to  <*> 
<org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: void printBlockDeletionTime(org.apache.commons.logging.Log)>,info,The block deletion will start around  <*> 
"<org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: void add(org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.protocol.DatanodeInfo,boolean)>",info,BLOCK*  <*> : add  block  to  datanode 
<org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: java.util.List invalidateWork(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>,debug,Block deletion is delayed during NameNode startup. The deletion will start after  <*>  ms. 
"<org.apache.hadoop.hdfs.server.blockmanagement.HeartbeatManager: void <init>(org.apache.hadoop.hdfs.server.namenode.Namesystem,org.apache.hadoop.hdfs.server.blockmanagement.BlockManager,org.apache.hadoop.conf.Configuration)>",info,Setting heartbeat recheck interval to  <*>  since  dfs.namenode.stale.datanode.interval  is less than  dfs.namenode.heartbeat.recheck-interval 
<org.apache.hadoop.hdfs.server.blockmanagement.HeartbeatManager$Monitor: void run()>,error,Exception while checking heartbeat
<org.apache.hadoop.hdfs.server.blockmanagement.DecommissionManager$Monitor: void run()>,warn,<*>  interrupted:  <*> 
<org.apache.hadoop.hdfs.server.blockmanagement.DecommissionManager$Monitor: void check()>,warn,entry= entry 
<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: void wipeDatanode(org.apache.hadoop.hdfs.protocol.DatanodeID)>,debug,<*> .wipeDatanode( node ): storage  <*>  is removed from datanodeMap. 
<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: void stopDecommission(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>,info,Stop Decommissioning  node 
<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: void startDecommission(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>,info,Start Decommissioning  node   storage  with  <*>  blocks 
<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: void removeDeadDatanode(org.apache.hadoop.hdfs.protocol.DatanodeID)>,info,BLOCK* removeDeadDatanode: lost heartbeat from  d 
<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: void removeDatanode(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>,debug,remove datanode  nodeInfo 
<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: void removeDatanode(org.apache.hadoop.hdfs.protocol.DatanodeID)>,warn,BLOCK* removeDatanode:  node  does not exist 
<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: void registerDatanode(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration)>,add,<*>
<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: void registerDatanode(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration)>,debug,BLOCK* registerDatanode: node restarted.
<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: void registerDatanode(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration)>,info,BLOCK* registerDatanode: from  nodeReg  storage  <*> 
<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: void registerDatanode(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration)>,info,BLOCK* registerDatanode:  <*> 
<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: void registerDatanode(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration)>,info,BLOCK* registerDatanode:  <*>  is replaced by  nodeReg  with the same storageID  <*> 
<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: void registerDatanode(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration)>,remove,<*>
<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: void registerDatanode(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration)>,warn,Unresolved datanode registration:  <*> 
<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: void markAllDatanodesStale()>,info,Marking all datandoes as stale
<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: void checkIfClusterIsNowMultiRack(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>,debug,<*> Not checking for mis-replicated blocks because this NN is not yet processing repl queues. 
<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: void checkIfClusterIsNowMultiRack(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>,info,"<*> Re-checking all blocks for replication, since they should now be replicated cross-rack "
<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: void addDatanode(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>,debug,<*> .addDatanode:  node  node  is added to datanodeMap. 
"<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: void <init>(org.apache.hadoop.hdfs.server.blockmanagement.BlockManager,org.apache.hadoop.hdfs.server.namenode.Namesystem,org.apache.hadoop.conf.Configuration)>",error,error reading hosts files: 
"<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: void <init>(org.apache.hadoop.hdfs.server.blockmanagement.BlockManager,org.apache.hadoop.hdfs.server.namenode.Namesystem,org.apache.hadoop.conf.Configuration)>",info,dfs.block.invalidate.limit= <*> 
"<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: void <init>(org.apache.hadoop.hdfs.server.blockmanagement.BlockManager,org.apache.hadoop.hdfs.server.namenode.Namesystem,org.apache.hadoop.conf.Configuration)>",info,dfs.namenode.datanode.registration.ip-hostname-check= <*> 
"<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: org.apache.hadoop.hdfs.server.protocol.DatanodeCommand[] handleHeartbeat(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration,org.apache.hadoop.hdfs.server.protocol.StorageReport[],java.lang.String,long,long,int,int,int)>",info,Skipped stale nodes for recovery :  <*> 
<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor getDatanode(org.apache.hadoop.hdfs.protocol.DatanodeID)>,fatal,BLOCK* NameSystem.getDatanode:  <*> 
<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: org.apache.hadoop.hdfs.protocol.DatanodeID parseDNFromHostsEntry(java.lang.String)>,warn,Invalid hostname  hostStr  in hosts file 
"<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: long getStaleIntervalFromConf(org.apache.hadoop.conf.Configuration,long)>",warn,"The given interval for marking stale datanode =  <*> , which is less than    heartbeat intervals. This may cause too frequent changes of  stale states of DataNodes since a heartbeat msg may be missing  due to temporary short-term failures. Reset stale interval to  <*> . "
"<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: long getStaleIntervalFromConf(org.apache.hadoop.conf.Configuration,long)>",warn,"The given interval for marking stale datanode =  staleInterval , which is larger than heartbeat expire interval  heartbeatExpireInterval . "
<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: java.util.List getNetworkDependenciesWithDefault(org.apache.hadoop.hdfs.protocol.DatanodeInfo)>,error,Unresolved dependency mapping for host  <*> . Continuing with an empty dependency list 
<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: java.util.List getNetworkDependencies(org.apache.hadoop.hdfs.protocol.DatanodeInfo)>,<init>,The dependency call returned null for host  <*> 
<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: java.util.List getNetworkDependencies(org.apache.hadoop.hdfs.protocol.DatanodeInfo)>,error,The dependency call returned null for host  <*> 
<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: java.util.List getDatanodeListForReport(org.apache.hadoop.hdfs.protocol.HdfsConstants$DatanodeReportType)>,debug,"getDatanodeListForReport with includedNodes =  <*> , excludedNodes =  <*> , foundNodes =  <*> , nodes =  <*> "
<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: java.lang.String resolveNetworkLocationWithFallBackToDefaultLocation(org.apache.hadoop.hdfs.protocol.DatanodeID)>,error,Unresolved topology mapping. Using /default-rack for host  <*> 
<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: java.lang.String resolveNetworkLocation(org.apache.hadoop.hdfs.protocol.DatanodeID)>,<init>,Unresolved topology mapping for host  <*> 
<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: java.lang.String resolveNetworkLocation(org.apache.hadoop.hdfs.protocol.DatanodeID)>,error,The resolve call returned null!
<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: boolean checkDecommissionState(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>,info,Decommission complete for  node 
"<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: void updateHeartbeatState(org.apache.hadoop.hdfs.server.protocol.StorageReport[],long,long,int,int)>",info,Number of failed storage changes from  <*>  to  volFailures 
<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: void updateFailedStorage(java.util.Set)>,info,storageInfo  failed. 
<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: void pruneStorageMap(org.apache.hadoop.hdfs.server.protocol.StorageReport[])>,debug,Number of storages reported in heartbeat= <*> ; Number of storages in storageMap= <*> 
<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: void pruneStorageMap(org.apache.hadoop.hdfs.server.protocol.StorageReport[])>,debug,Deferring removal of stale storage  len$  with  <*>  blocks 
<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: void pruneStorageMap(org.apache.hadoop.hdfs.server.protocol.StorageReport[])>,info,Removed storage  len$  from DataNode this 
<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: void addBlockToBeRecovered(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoUnderConstruction)>,info,block  is already in the recovery queue 
<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: java.util.List removeZombieStorages()>,info,"<*>  had lastBlockReportId x <*> , but curBlockReportId = x <*> "
"<org.apache.hadoop.hdfs.server.blockmanagement.CorruptReplicasMap: void addToCorruptReplicasMap(org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,java.lang.String,org.apache.hadoop.hdfs.server.blockmanagement.CorruptReplicasMap$Reason)>",info,BLOCK NameSystem.addToCorruptReplicasMap:  <*>  added as corrupt on  dn  by  <*> reasonText 
"<org.apache.hadoop.hdfs.server.blockmanagement.CorruptReplicasMap: void addToCorruptReplicasMap(org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,java.lang.String,org.apache.hadoop.hdfs.server.blockmanagement.CorruptReplicasMap$Reason)>",info,BLOCK NameSystem.addToCorruptReplicasMap: duplicate requested for  <*>  to add as corrupt  on  dn  by  <*> reasonText 
<org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: void waitForRescanIfNeeded()>,warn,Interrupted while waiting for CacheReplicationMonitor rescan
"<org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: void rescanFile(org.apache.hadoop.hdfs.protocol.CacheDirective,org.apache.hadoop.hdfs.server.namenode.INodeFile)>",debug,"Directive {}: not scanning file {} because bytesNeeded for pool {} is {}, but the pool\'s limit is {}"
"<org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: void rescanFile(org.apache.hadoop.hdfs.protocol.CacheDirective,org.apache.hadoop.hdfs.server.namenode.INodeFile)>",debug,Directive {}: caching {}: {}/{} bytes
"<org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: void rescanFile(org.apache.hadoop.hdfs.protocol.CacheDirective,org.apache.hadoop.hdfs.server.namenode.INodeFile)>",trace,"Directive {}: can\'t cache block {} because it is in state {}, not COMPLETE."
"<org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: void rescanFile(org.apache.hadoop.hdfs.protocol.CacheDirective,org.apache.hadoop.hdfs.server.namenode.INodeFile)>",trace,Directive {}: setting replication for block {} to {}
<org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: void rescanCacheDirectives()>,debug,Directive {}: the directive expired at {} (now = {})
<org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: void rescanCacheDirectives()>,debug,Directive {}: got UnresolvedLinkException while resolving path {}
<org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: void rescanCacheDirectives()>,debug,Directive {}: No inode found at {}
<org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: void rescanCacheDirectives()>,debug,"Directive {}: ignoring non-directive, non-file inode {} "
<org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: void rescanCachedBlockMap()>,trace,Block {}: removing from PENDING_UNCACHED for node {} because the DataNode uncached it.
<org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: void rescanCachedBlockMap()>,trace,Block {}: can\'t cache block because it is {}
<org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: void rescanCachedBlockMap()>,trace,Block {}: removing from PENDING_CACHED for node {}because we already have {} cached replicas and we only need {}
<org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: void rescanCachedBlockMap()>,trace,Block {}: removing from PENDING_UNCACHED for node {} because we only have {} cached replicas and we need {}
<org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: void rescanCachedBlockMap()>,trace,"Block {}: removing from cachedBlocks, since neededCached == , and pendingUncached and pendingCached are empty."
"<org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: void addNewPendingUncached(int,org.apache.hadoop.hdfs.server.namenode.CachedBlock,java.util.List,java.util.List)>",warn,Logic error: we\'re trying to uncache more replicas than actually exist for  cachedBlock 
"<org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: void addNewPendingCached(int,org.apache.hadoop.hdfs.server.namenode.CachedBlock,java.util.List,java.util.List)>",debug,"Block {}: can\'t add new cached replicas, because there is no record of this block on the NameNode."
"<org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: void addNewPendingCached(int,org.apache.hadoop.hdfs.server.namenode.CachedBlock,java.util.List,java.util.List)>",debug,"Block {}: can\'t cache this block, because it is not yet complete."
"<org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: void addNewPendingCached(int,org.apache.hadoop.hdfs.server.namenode.CachedBlock,java.util.List,java.util.List)>",debug,Block {}: we only have {} of {} cached replicas. {} DataNodes have insufficient cache capacity.
"<org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: void addNewPendingCached(int,org.apache.hadoop.hdfs.server.namenode.CachedBlock,java.util.List,java.util.List)>",trace,"Block {}: DataNode {} is not a valid possibility because the block has size {}, but the DataNode only has {}bytes of cache remaining ({} pending bytes, {} already cached."
"<org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: void addNewPendingCached(int,org.apache.hadoop.hdfs.server.namenode.CachedBlock,java.util.List,java.util.List)>",trace,Block {}: added to PENDING_CACHED on DataNode {}
"<org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithNodeGroup: int addDependentNodesToExcludedNodes(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,java.util.Set)>",warn,Not able to find datanode  hostname  which has dependency with datanode  <*> 
"<org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault: void chooseRemoteRack(int,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,java.util.Set,long,int,java.util.List,boolean,java.util.EnumMap)>",debug,"Failed to choose remote rack (location = ~ <*> ), fallback to local rack "
"<org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault: org.apache.hadoop.net.Node chooseTarget(int,org.apache.hadoop.net.Node,java.util.Set,long,int,java.util.List,boolean,org.apache.hadoop.hdfs.protocol.BlockStoragePolicy,java.util.EnumSet,boolean)>",trace,storageTypes= <*> 
"<org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault: org.apache.hadoop.net.Node chooseTarget(int,org.apache.hadoop.net.Node,java.util.Set,long,int,java.util.List,boolean,org.apache.hadoop.hdfs.protocol.BlockStoragePolicy,java.util.EnumSet,boolean)>",trace,"Failed to place enough replicas, still in need of  <*>  to reach  totalReplicasExpected  (unavailableStorages= unavailableStorages , storagePolicy= storagePolicy , newBlock= newBlock ) "
"<org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault: org.apache.hadoop.net.Node chooseTarget(int,org.apache.hadoop.net.Node,java.util.Set,long,int,java.util.List,boolean,org.apache.hadoop.hdfs.protocol.BlockStoragePolicy,java.util.EnumSet,boolean)>",warn,<*>   <*> 
"<org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault: org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo[] chooseTarget(java.lang.String,int,org.apache.hadoop.net.Node,java.util.Set,long,java.util.List,org.apache.hadoop.hdfs.protocol.BlockStoragePolicy)>",debug,"Failed to choose with favored nodes (= favoredNodes ), disregard favored nodes hint and retry. "
"<org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault: org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo[] chooseTarget(java.lang.String,int,org.apache.hadoop.net.Node,java.util.Set,long,java.util.List,org.apache.hadoop.hdfs.protocol.BlockStoragePolicy)>",warn,Could not find a target for file  src  with favored node  favoredNode 
"<org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault: org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo chooseLocalRack(org.apache.hadoop.net.Node,java.util.Set,long,int,java.util.List,boolean,java.util.EnumMap)>",debug,"Failed to choose from local rack (location =  <*> ), retry with the rack of the next replica (location =  <*> ) "
"<org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault: org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo chooseLocalRack(org.apache.hadoop.net.Node,java.util.Set,long,int,java.util.List,boolean,java.util.EnumMap)>",debug,"Failed to choose from local rack (location =  <*> ); the second replica is not found, retry choosing ramdomly "
"<org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault: org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo chooseFromNextRack(org.apache.hadoop.net.Node,java.util.Set,long,int,java.util.List,boolean,java.util.EnumMap)>",debug,"Failed to choose from the next rack (location =  <*> ), retry choosing ramdomly "
"<org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault: java.util.List chooseReplicasToDelete(java.util.Collection,int,java.util.List,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>",warn,No excess replica can be found. excessTypes:  excessTypes . moreThanOne:  <*> . exactlyOne:  <*> . 
<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void stopReplicationInitializer()>,warn,Interrupted while waiting for replicationQueueInitializer. Returning..
"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void setReplication(short,short,java.lang.String,org.apache.hadoop.hdfs.protocol.Block[])>",info,Decreasing replication from  $i  to  $i  for  src 
"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void setReplication(short,short,java.lang.String,org.apache.hadoop.hdfs.protocol.Block[])>",info,Increasing replication from  $i  to  $i  for  src 
<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void rescanPostponedMisreplicatedBlocks()>,debug,BLOCK* rescanPostponedMisreplicatedBlocks: Postponed mis-replicated block  b  no longer found  in block map. 
<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void rescanPostponedMisreplicatedBlocks()>,debug,"BLOCK* rescanPostponedMisreplicatedBlocks: Re-scanned block  b , result is  <*> "
"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void removeZombieReplicas(org.apache.hadoop.hdfs.server.protocol.BlockReportContext,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo)>",warn,"processReport x <*> : removing zombie storage  <*> , which no longer exists on the DataNode. "
"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void removeZombieReplicas(org.apache.hadoop.hdfs.server.protocol.BlockReportContext,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo)>",warn,"processReport x <*> : removed  <*>  replicas from storage  <*> , which no longer exists on the DataNode. "
"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void removeStoredBlock(org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>",debug,BLOCK* removeStoredBlock:  block  from  node 
"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void removeStoredBlock(org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>",debug,BLOCK* removeStoredBlock:  block  has already been removed from node  node 
"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void removeStoredBlock(org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>",debug,BLOCK* removeStoredBlock:  block  is removed from excessBlocks 
"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void queueReportedBlock(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$ReplicaState,java.lang.String)>",debug,Queueing reported block  block  in state  reportedState  from datanode  <*>  for later processing because  reason . 
<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void processQueuedMessages(java.lang.Iterable)>,debug,Processing previouly queued message  rbi 
<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void processOverReplicatedBlocksOnReCommission(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>,info,Invalidated  numOverReplicated  over-replicated blocks on  srcNode  during recommissioning 
"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void processOverReplicatedBlock(org.apache.hadoop.hdfs.protocol.Block,short,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>",trace,BLOCK* processOverReplicatedBlock: Postponing  block  since storage  storage  does not yet have up-to-date information. 
<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void processMisReplicatesAsync()>,info,Total number of blocks            =  <*> 
<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void processMisReplicatesAsync()>,info,Number of invalid blocks          =  nrInvalid 
<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void processMisReplicatesAsync()>,info,Number of under-replicated blocks =  nrUnderReplicated 
<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void processMisReplicatesAsync()>,info,Number of  over-replicated blocks =  nrOverReplicated <*> 
<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void processMisReplicatesAsync()>,info,Number of blocks being written    =  nrUnderConstruction 
<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void processMisReplicatesAsync()>,info,"STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in  <*>  msec "
<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void processMisReplicatesAsync()>,info,Interrupted while processing replication queues.
<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void processMisReplicatesAsync()>,trace,block  block :  <*> 
"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void processIncrementalBlockReport(org.apache.hadoop.hdfs.protocol.DatanodeID,org.apache.hadoop.hdfs.server.protocol.StorageReceivedDeletedBlocks)>",debug,BLOCK* block  <*> :  <*>  is received from  nodeID 
"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void processIncrementalBlockReport(org.apache.hadoop.hdfs.protocol.DatanodeID,org.apache.hadoop.hdfs.server.protocol.StorageReceivedDeletedBlocks)>",debug,"*BLOCK* NameNode.processIncrementalBlockReport: from  nodeID  receiving:  receiving ,   received:  received ,   deleted:  deleted "
"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void processIncrementalBlockReport(org.apache.hadoop.hdfs.protocol.DatanodeID,org.apache.hadoop.hdfs.server.protocol.StorageReceivedDeletedBlocks)>",warn,BLOCK* processIncrementalBlockReport is received from dead or unregistered node  nodeID 
"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void processIncrementalBlockReport(org.apache.hadoop.hdfs.protocol.DatanodeID,org.apache.hadoop.hdfs.server.protocol.StorageReceivedDeletedBlocks)>",warn,Unknown block status code reported by  nodeID :  rdbi 
"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void processChosenExcessReplica(java.util.Collection,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.protocol.Block)>",debug,"BLOCK* chooseExcessReplicates: ( chosen ,  b ) is added to invalidated blocks set "
"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void processAndHandleReportedBlock(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$ReplicaState,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>",info,BLOCK* addBlock: logged info for  <*>  of  i$_  reported. 
"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void processAndHandleReportedBlock(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$ReplicaState,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>",info,BLOCK* addBlock: block  b_#  on  <*>  size  <*>  does not belong to any file 
<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void processAllPendingDNMessages()>,info,Processing  <*>  messages from DataNodes  that were previously queued during standby state 
"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void markBlockReplicasAsCorrupt(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,long,long,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo[])>",info,BLOCK* markBlockReplicasAsCorrupt: mark block replica b  on  <*>  as corrupt because the dn is not in the new committed  storage list. 
"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void markBlockAsCorrupt(org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$BlockToMarkCorrupt,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>",info,BLOCK markBlockAsCorrupt:  b  cannot be marked as corrupt as it does not belong to any file 
"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void logBlockReplicationInfo(org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,org.apache.hadoop.hdfs.server.blockmanagement.NumberReplicas)>",info,"Block:  block , Expected Replicas:  <*> , live replicas:  <*> , corrupt replicas:  <*> , decommissioned replicas:  <*> , excess replicas:  <*> , Is Open File:  <*> , Datanodes having this block:  <*> , Current Datanode:  srcNode , Is current datanode decommissioning:  <*> "
<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void invalidateCorruptReplicas(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo)>,info,invalidateCorruptReplicas error in deleting bad block  blk  on  node 
"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void findAndMarkBlockAsCorrupt(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.DatanodeInfo,java.lang.String,java.lang.String)>",info,BLOCK* findAndMarkBlockAsCorrupt:  blk  not found 
<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void addToInvalidates(org.apache.hadoop.hdfs.protocol.Block)>,info,BLOCK* addToInvalidates:  b   <*> 
"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void addToExcessReplicate(org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.protocol.Block)>",debug,"BLOCK* addToExcessReplicate: ( dn ,  block ) is added to excessReplicateMap "
"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void addBlock(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.protocol.Block,java.lang.String)>",warn,BLOCK* blockReceived:  block  is expected to be removed from an unrecorded node  delHint 
"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void <init>(org.apache.hadoop.hdfs.server.namenode.Namesystem,org.apache.hadoop.hdfs.server.namenode.FSClusterStats,org.apache.hadoop.conf.Configuration)>",info,defaultReplication         =  <*> 
"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void <init>(org.apache.hadoop.hdfs.server.namenode.Namesystem,org.apache.hadoop.hdfs.server.namenode.FSClusterStats,org.apache.hadoop.conf.Configuration)>",info,maxReplication             =  $i 
"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void <init>(org.apache.hadoop.hdfs.server.namenode.Namesystem,org.apache.hadoop.hdfs.server.namenode.FSClusterStats,org.apache.hadoop.conf.Configuration)>",info,minReplication             =  $i 
"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void <init>(org.apache.hadoop.hdfs.server.namenode.Namesystem,org.apache.hadoop.hdfs.server.namenode.FSClusterStats,org.apache.hadoop.conf.Configuration)>",info,maxReplicationStreams      =  <*> 
"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void <init>(org.apache.hadoop.hdfs.server.namenode.Namesystem,org.apache.hadoop.hdfs.server.namenode.FSClusterStats,org.apache.hadoop.conf.Configuration)>",info,replicationRecheckInterval =  <*> 
"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void <init>(org.apache.hadoop.hdfs.server.namenode.Namesystem,org.apache.hadoop.hdfs.server.namenode.FSClusterStats,org.apache.hadoop.conf.Configuration)>",info,encryptDataTransfer        =  <*> 
"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void <init>(org.apache.hadoop.hdfs.server.namenode.Namesystem,org.apache.hadoop.hdfs.server.namenode.FSClusterStats,org.apache.hadoop.conf.Configuration)>",info,maxNumBlocksToLog          =  <*> 
"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations getBlocksWithLocations(org.apache.hadoop.hdfs.protocol.DatanodeID,long)>",warn,BLOCK* getBlocks: Asking for blocks from an unrecorded node  datanode 
"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$BlockToMarkCorrupt checkReplicaCorrupt(org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$ReplicaState,org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$BlockUCState,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>",info,"Received an RBW replica for  storedBlock  on  dn : ignoring it, since it is  complete with the same genstamp "
"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$BlockToMarkCorrupt checkReplicaCorrupt(org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$ReplicaState,org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$BlockUCState,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>",warn,Unexpected replica state  reportedState  for block:  storedBlock  on  dn  size  <*> 
"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo processReportedBlock(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$ReplicaState,java.util.Collection,java.util.Collection,java.util.Collection,java.util.Collection)>",debug,Reported block  block  on  <*>  size  <*>  replicaState =  reportedState 
"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo processReportedBlock(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$ReplicaState,java.util.Collection,java.util.Collection,java.util.Collection,java.util.Collection)>",debug,In memory blockUCState =  <*> 
<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager createBlockTokenSecretManager(org.apache.hadoop.conf.Configuration)>,error,Security is enabled but block access tokens (via dfs.block.access.token.enable) aren\'t enabled. This may cause issues when clients attempt to talk to a DataNode.
<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager createBlockTokenSecretManager(org.apache.hadoop.conf.Configuration)>,info,dfs.block.access.token.enable= <*> 
<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager createBlockTokenSecretManager(org.apache.hadoop.conf.Configuration)>,info,"dfs.block.access.key.update.interval= <*>  min(s),  dfs.block.access.token.lifetime = <*>  min(s),  dfs.encrypt.data.transfer.algorithm = <*> "
"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: org.apache.hadoop.hdfs.protocol.LocatedBlocks createLocatedBlocks(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo[],long,boolean,long,long,boolean,boolean,org.apache.hadoop.fs.FileEncryptionInfo)>",debug,blocks =  <*> 
"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: org.apache.hadoop.hdfs.protocol.LocatedBlock createLocatedBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,long)>",warn,Inconsistent number of corrupt replicas for  blk  blockMap has  <*>  but corrupt replicas map has  <*> 
"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: org.apache.hadoop.hdfs.protocol.Block addStoredBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,boolean)>",info,BLOCK* addStoredBlock:  block  on  <*>  size  <*>  but it does not belong to any file 
"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: org.apache.hadoop.hdfs.protocol.Block addStoredBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,boolean)>",logAddStoredBlock,block
"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: org.apache.hadoop.hdfs.protocol.Block addStoredBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,boolean)>",warn,BLOCK* addStoredBlock: Redundant addStoredBlock request received for  storedBlock  on  <*>  size  <*> 
"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: org.apache.hadoop.hdfs.protocol.Block addStoredBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,boolean)>",warn,Inconsistent number of corrupt replicas for  storedBlock blockMap has  <*>  but corrupt replicas map has  <*> 
"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: java.util.Collection processReport(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.protocol.BlockListAsLongs)>",info,BLOCK* processReport: logged info for  <*>  of  i$  reported. 
<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: int invalidateWorkForOneNode(org.apache.hadoop.hdfs.protocol.DatanodeInfo)>,debug,"In safemode, not computing replication work"
<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: int invalidateWorkForOneNode(org.apache.hadoop.hdfs.protocol.DatanodeInfo)>,info,BLOCK*  <*> : ask  dn  to delete  <*> 
<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: int invalidateWorkForOneNode(org.apache.hadoop.hdfs.protocol.DatanodeInfo)>,warn,"DataNode  dn  cannot be found with UUID  <*> , removing block invalidation work. "
<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: int computeReplicationWorkForBlocks(java.util.List)>,debug,Block  block  cannot be repl from any node 
<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: int computeReplicationWorkForBlocks(java.util.List)>,debug,BLOCK* block  <*>  is moved from neededReplications to pendingReplications 
<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: int computeReplicationWorkForBlocks(java.util.List)>,debug,BLOCK* neededReplications =  <*>  pendingReplications =  <*> 
<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: int computeReplicationWorkForBlocks(java.util.List)>,info,BLOCK* Removing  block  from neededReplications as it has enough replicas 
<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: int computeReplicationWorkForBlocks(java.util.List)>,info,BLOCK* Removing  <*>  from neededReplications as it has enough replicas 
<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: int computeReplicationWorkForBlocks(java.util.List)>,info,BLOCK* ask  <*>  to replicate  <*>  to  $u 
"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: boolean processReport(org.apache.hadoop.hdfs.protocol.DatanodeID,org.apache.hadoop.hdfs.server.protocol.DatanodeStorage,org.apache.hadoop.hdfs.protocol.BlockListAsLongs,org.apache.hadoop.hdfs.server.protocol.BlockReportContext,boolean)>",debug,processReport x <*> : no zombie storages found. 
"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: boolean processReport(org.apache.hadoop.hdfs.protocol.DatanodeID,org.apache.hadoop.hdfs.server.protocol.DatanodeStorage,org.apache.hadoop.hdfs.protocol.BlockListAsLongs,org.apache.hadoop.hdfs.server.protocol.BlockReportContext,boolean)>",debug,processReport x <*> :  <*>  more RPCs remaining in this report. 
"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: boolean processReport(org.apache.hadoop.hdfs.protocol.DatanodeID,org.apache.hadoop.hdfs.server.protocol.DatanodeStorage,org.apache.hadoop.hdfs.protocol.BlockListAsLongs,org.apache.hadoop.hdfs.server.protocol.BlockReportContext,boolean)>",info,BLOCK* processReport: discarded non-initial block report from  nodeID  because namenode still in startup phase 
"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: boolean processReport(org.apache.hadoop.hdfs.protocol.DatanodeID,org.apache.hadoop.hdfs.server.protocol.DatanodeStorage,org.apache.hadoop.hdfs.protocol.BlockListAsLongs,org.apache.hadoop.hdfs.server.protocol.BlockReportContext,boolean)>",info,BLOCK* processReport: Received first block report from  storage  after starting up or becoming active. Its block  contents are no longer considered stale 
"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: boolean processReport(org.apache.hadoop.hdfs.protocol.DatanodeID,org.apache.hadoop.hdfs.server.protocol.DatanodeStorage,org.apache.hadoop.hdfs.protocol.BlockListAsLongs,org.apache.hadoop.hdfs.server.protocol.BlockReportContext,boolean)>",info,BLOCK* processReport:  staleBefore  on  <*>  size  <*>  does not belong to any file 
"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: boolean processReport(org.apache.hadoop.hdfs.protocol.DatanodeID,org.apache.hadoop.hdfs.server.protocol.DatanodeStorage,org.apache.hadoop.hdfs.protocol.BlockListAsLongs,org.apache.hadoop.hdfs.server.protocol.BlockReportContext,boolean)>",info,"BLOCK* processReport: from storage  <*>  node  nodeID , blocks:  <*> , hasStaleStorages:  <*> , processing time:  <*>  msecs "
<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: boolean isReplicationInProgress(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>,warn,"srcNode  srcNode  is dead  when decommission is in progress. Continue to mark  it as decommission in progress. In that way, when it rejoins the  cluster it can continue the decommission process. "
"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: boolean invalidateBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$BlockToMarkCorrupt,org.apache.hadoop.hdfs.protocol.DatanodeInfo)>",debug,BLOCK* invalidateBlocks:  b  on  dn  listed for deletion. 
"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: boolean invalidateBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$BlockToMarkCorrupt,org.apache.hadoop.hdfs.protocol.DatanodeInfo)>",info,BLOCK* invalidateBlock:  b  on  dn 
"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: boolean invalidateBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$BlockToMarkCorrupt,org.apache.hadoop.hdfs.protocol.DatanodeInfo)>",info,BLOCK* invalidateBlocks: postponing invalidation of  b  on  dn  because  <*>  replica(s) are located on nodes  with potentially out-of-date block reports 
"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: boolean invalidateBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$BlockToMarkCorrupt,org.apache.hadoop.hdfs.protocol.DatanodeInfo)>",info,BLOCK* invalidateBlocks:  b  on  dn  is the only copy and was not deleted 
<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor: void run()>,fatal,ReplicationMonitor thread received Runtime exception. 
<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor: void run()>,info,Stopping ReplicationMonitor.
<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor: void run()>,info,ReplicationMonitor received an exception while shutting down.
<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor: void run()>,info,Stopping ReplicationMonitor for testing.
<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$1: void run()>,error,Error while processing replication queues async
<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$1: void run()>,info,Interrupted while processing replication queues.
<org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoUnderConstruction: void setGenerationStampAndVerifyReplicas(long)>,info,BLOCK* Removing stale replica from location:  <*> 
<org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoUnderConstruction: void initializeBlockRecovery(long)>,info,"BLOCK*  this  recovery started, primary= primary "
<org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoUnderConstruction: void initializeBlockRecovery(long)>,warn,"BLOCK* BlockInfoUnderConstruction.initLeaseRecovery: No blocks found, lease removed."
<org.apache.hadoop.hdfs.server.balancer.NameNodeConnector: void close()>,warn,Failed to delete  <*> 
<org.apache.hadoop.hdfs.server.balancer.KeyManager: void close()>,warn,Exception shutting down access key updater thread
"<org.apache.hadoop.hdfs.server.balancer.KeyManager: void <init>(java.lang.String,org.apache.hadoop.hdfs.server.protocol.NamenodeProtocol,boolean,org.apache.hadoop.conf.Configuration)>",info,"Block token params received from NN: update interval= <*> , token lifetime= <*> "
<org.apache.hadoop.hdfs.server.balancer.KeyManager$BlockKeyUpdater: void run()>,debug,InterruptedException in block key updater thread
<org.apache.hadoop.hdfs.server.balancer.KeyManager$BlockKeyUpdater: void run()>,error,Failed to set keys
<org.apache.hadoop.hdfs.server.balancer.KeyManager$BlockKeyUpdater: void run()>,error,Exception in block key updater thread
<org.apache.hadoop.hdfs.server.balancer.KeyManager$BlockKeyUpdater: void close()>,warn,Exception shutting down key updater thread
"<org.apache.hadoop.hdfs.server.balancer.KeyManager$BlockKeyUpdater: void <init>(org.apache.hadoop.hdfs.server.balancer.KeyManager,long)>",info,Update block keys every  <*> 
<org.apache.hadoop.hdfs.server.balancer.Dispatcher: long dispatchBlockMoves()>,warn,Dispatcher thread failed
<org.apache.hadoop.hdfs.server.balancer.Dispatcher: boolean shouldIgnore(org.apache.hadoop.hdfs.protocol.DatanodeInfo)>,trace,"Excluding datanode  dn :  <*> ,  <*> ,  <*> ,  notIncluded "
<org.apache.hadoop.hdfs.server.balancer.Dispatcher$Source: void dispatchBlocks()>,warn,Exception while getting block list
<org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove: void dispatch()>,debug,Start moving  this 
<org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove: void dispatch()>,info,Successfully moved  this 
<org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove: void dispatch()>,warn,Failed to move  this :  <*> 
"<org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove: boolean markMovedIfGoodBlock(org.apache.hadoop.hdfs.server.balancer.Dispatcher$DBlock,org.apache.hadoop.hdfs.StorageType)>",debug,Decided to move  this 
"<org.apache.hadoop.hdfs.server.balancer.Balancer: void matchSourceWithTargetToMove(org.apache.hadoop.hdfs.server.balancer.Dispatcher$Source,org.apache.hadoop.hdfs.server.balancer.Dispatcher$DDatanode$StorageGroup)>",info,Decided to move  <*>  bytes from  <*>  to  <*> 
<org.apache.hadoop.hdfs.server.balancer.Balancer: void main(java.lang.String[])>,error,Exiting balancer due an exception
<org.apache.hadoop.hdfs.server.balancer.Balancer: void logUtilizationCollections()>,logUtilizationCollection,over-utilized
<org.apache.hadoop.hdfs.server.balancer.Balancer: void logUtilizationCollections()>,logUtilizationCollection,above-average
<org.apache.hadoop.hdfs.server.balancer.Balancer: void logUtilizationCollections()>,logUtilizationCollection,below-average
<org.apache.hadoop.hdfs.server.balancer.Balancer: void logUtilizationCollections()>,logUtilizationCollection,underutilized
"<org.apache.hadoop.hdfs.server.balancer.Balancer: void logUtilizationCollection(java.lang.String,java.util.Collection)>",info,<*>   name :  items 
"<org.apache.hadoop.hdfs.server.balancer.Balancer: int run(java.util.Collection,org.apache.hadoop.hdfs.server.balancer.Balancer$Parameters,org.apache.hadoop.conf.Configuration)>",info,namenodes  =  namenodes 
"<org.apache.hadoop.hdfs.server.balancer.Balancer: int run(java.util.Collection,org.apache.hadoop.hdfs.server.balancer.Balancer$Parameters,org.apache.hadoop.conf.Configuration)>",info,parameters =  p 
<org.apache.hadoop.hdfs.server.balancer.Balancer$Cli: org.apache.hadoop.hdfs.server.balancer.Balancer$Parameters parse(java.lang.String[])>,info,Using a threshold of  <*> 
"<org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSecretManager: void addPersistedDelegationToken(org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier,long)>",<init>,Same delegation token being added twice; invalid entry in fsimage or editlogs
"<org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSecretManager: void addPersistedDelegationToken(org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier,long)>",warn,No KEY found for persisted identifier  <*> 
"<org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager: void checkAccess(org.apache.hadoop.hdfs.security.token.block.BlockTokenIdentifier,java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager$AccessMode)>",debug,"Checking access for user= userId , block= block , access mode= mode  using  <*> "
<org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager: void addKeys(org.apache.hadoop.hdfs.security.token.block.ExportedBlockKeys)>,info,Setting block keys
<org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager: org.apache.hadoop.hdfs.security.token.block.ExportedBlockKeys exportKeys()>,debug,Exporting access keys
<org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager: byte[] createPassword(org.apache.hadoop.hdfs.security.token.block.BlockTokenIdentifier)>,debug,Generating block token for  <*> 
<org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager: boolean updateKeys()>,info,Updating block keys
<org.apache.hadoop.hdfs.RemoteBlockReader2: void sendReadResult(org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$Status)>,info,Could not send read status ( statusCode ) to datanode  <*> :  <*> 
<org.apache.hadoop.hdfs.RemoteBlockReader2: void readTrailingEmptyPacket()>,trace,Reading empty packet at end of read
<org.apache.hadoop.hdfs.RemoteBlockReader2: void readNextPacket()>,trace,DFSClient readNextPacket got header  <*> 
"<org.apache.hadoop.hdfs.RemoteBlockReader: void sendReadResult(org.apache.hadoop.hdfs.net.Peer,org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$Status)>",info,Could not send read status ( statusCode ) to datanode  <*> :  <*> 
"<org.apache.hadoop.hdfs.RemoteBlockReader: int readChunk(long,byte[],int,int,byte[])>",debug,DFSClient readChunk got header  $u 
<org.apache.hadoop.hdfs.qjournal.server.JournalNode: void stop(int)>,warn,Unable to stop HTTP server for  this 
"<org.apache.hadoop.hdfs.qjournal.server.JournalNode: org.apache.hadoop.hdfs.qjournal.server.Journal getOrCreateJournal(java.lang.String,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption)>",info,Initializing journal in directory  <*> 
<org.apache.hadoop.hdfs.qjournal.server.JournalNode$ErrorReporter: void reportErrorOnFile(java.io.File)>,fatal,Error reported on file  f ... exiting 
<org.apache.hadoop.hdfs.qjournal.server.Journal: void updateLastPromisedEpoch(long)>,info,Updating lastPromisedEpoch from  <*>  to  newEpoch  for client  <*> 
"<org.apache.hadoop.hdfs.qjournal.server.Journal: void startLogSegment(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo,long,int)>",info,Updating lastWriterEpoch from  <*>  to  <*>  for client  <*> 
"<org.apache.hadoop.hdfs.qjournal.server.Journal: void startLogSegment(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo,long,int)>",warn,Client is requesting a new log segment  txid  though we are already writing  <*> .  Aborting the current segment in order to begin the new one. 
"<org.apache.hadoop.hdfs.qjournal.server.Journal: void journal(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo,long,long,int,byte[])>",trace,Writing txid  firstTxnId - <*> 
"<org.apache.hadoop.hdfs.qjournal.server.Journal: void journal(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo,long,long,int,byte[])>",warn,Sync of transaction range  firstTxnId - <*>  took  <*> ms 
<org.apache.hadoop.hdfs.qjournal.server.Journal: void format(org.apache.hadoop.hdfs.server.protocol.NamespaceInfo)>,info,Formatting  this  with namespace info:  nsInfo 
"<org.apache.hadoop.hdfs.qjournal.server.Journal: void finalizeLogSegment(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo,long,long)>",info,Validating log segment  <*>  about to be  finalized 
<org.apache.hadoop.hdfs.qjournal.server.Journal: void doUpgrade(org.apache.hadoop.hdfs.server.common.StorageInfo)>,info,Starting upgrade of edits directory: .\n   old LV =  <*> ; old CTime =  <*> .\n   new LV =  <*> ; new CTime =  <*> 
<org.apache.hadoop.hdfs.qjournal.server.Journal: void doFinalize()>,info,Finalizing upgrade for journal  <*> . <*> 
<org.apache.hadoop.hdfs.qjournal.server.Journal: void completeHalfDoneAcceptRecovery(org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PersistedRecoveryPaxosData)>,info,Rolling forward previously half-completed synchronization:  <*>  ->  <*> 
"<org.apache.hadoop.hdfs.qjournal.server.Journal: void acceptRecovery(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo,org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$SegmentStateProto,java.net.URL)>",alwaysAssert,<*> 
"<org.apache.hadoop.hdfs.qjournal.server.Journal: void acceptRecovery(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo,org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$SegmentStateProto,java.net.URL)>",info,Synchronizing log  <*> : no current segment in place 
"<org.apache.hadoop.hdfs.qjournal.server.Journal: void acceptRecovery(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo,org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$SegmentStateProto,java.net.URL)>",info,Synchronizing log  <*> : old segment  <*>  is not the right length 
"<org.apache.hadoop.hdfs.qjournal.server.Journal: void acceptRecovery(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo,org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$SegmentStateProto,java.net.URL)>",info,Skipping download of log  <*> : already have up-to-date logs 
"<org.apache.hadoop.hdfs.qjournal.server.Journal: void acceptRecovery(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo,org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$SegmentStateProto,java.net.URL)>",info,Accepted recovery for segment  <*> :  <*> 
<org.apache.hadoop.hdfs.qjournal.server.Journal: org.apache.hadoop.hdfs.server.namenode.FileJournalManager$EditLogFile scanStorageForLatestEdits()>,info,Scanning storage  <*> 
<org.apache.hadoop.hdfs.qjournal.server.Journal: org.apache.hadoop.hdfs.server.namenode.FileJournalManager$EditLogFile scanStorageForLatestEdits()>,info,Latest log is  latestLog 
<org.apache.hadoop.hdfs.qjournal.server.Journal: org.apache.hadoop.hdfs.server.namenode.FileJournalManager$EditLogFile scanStorageForLatestEdits()>,info,No files in  <*> 
<org.apache.hadoop.hdfs.qjournal.server.Journal: org.apache.hadoop.hdfs.server.namenode.FileJournalManager$EditLogFile scanStorageForLatestEdits()>,warn,Latest log  latestLog  has no transactions.  moving it aside and looking for previous log 
<org.apache.hadoop.hdfs.qjournal.server.Journal: org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$SegmentStateProto getSegmentInfo(long)>,info,Edit log file  <*>  appears to be empty.  Moving it aside... 
<org.apache.hadoop.hdfs.qjournal.server.Journal: org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$SegmentStateProto getSegmentInfo(long)>,info,getSegmentInfo( segmentTxId ):  <*>  ->  <*> 
"<org.apache.hadoop.hdfs.qjournal.server.Journal: org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryResponseProto prepareRecovery(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo,long)>",info,Prepared recovery for segment  segmentTxId :  <*> 
"<org.apache.hadoop.hdfs.qjournal.server.Journal: java.io.File syncLog(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo,org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$SegmentStateProto,java.net.URL)>",info,Synchronizing log  <*>  from  url 
<org.apache.hadoop.hdfs.qjournal.server.Journal$1: java.lang.Void run()>,warn,Failed to delete temporary file  <*> 
"<org.apache.hadoop.hdfs.qjournal.server.JNStorage: void purgeMatching(java.io.File,java.util.List,long)>",info,Purging no-longer needed file  <*> 
"<org.apache.hadoop.hdfs.qjournal.server.JNStorage: void purgeMatching(java.io.File,java.util.List,long)>",warn,Unable to delete no-longer-needed data  f 
<org.apache.hadoop.hdfs.qjournal.server.JNStorage: void format(org.apache.hadoop.hdfs.server.protocol.NamespaceInfo)>,info,Formatting journal  <*>  with nsid:  <*> 
<org.apache.hadoop.hdfs.qjournal.server.JNStorage: void close()>,info,Closing journal storage for  <*> 
"<org.apache.hadoop.hdfs.qjournal.server.GetJournalEditServlet: boolean isValidRequestor(javax.servlet.http.HttpServletRequest,org.apache.hadoop.conf.Configuration)>",debug,Validating request made by  <*>  /  <*> . This user is:  <*> 
"<org.apache.hadoop.hdfs.qjournal.server.GetJournalEditServlet: boolean isValidRequestor(javax.servlet.http.HttpServletRequest,org.apache.hadoop.conf.Configuration)>",debug,SecondaryNameNode principal could not be added
"<org.apache.hadoop.hdfs.qjournal.server.GetJournalEditServlet: boolean isValidRequestor(javax.servlet.http.HttpServletRequest,org.apache.hadoop.conf.Configuration)>",debug,isValidRequestor is comparing to valid requestor:  msg 
"<org.apache.hadoop.hdfs.qjournal.server.GetJournalEditServlet: boolean isValidRequestor(javax.servlet.http.HttpServletRequest,org.apache.hadoop.conf.Configuration)>",debug,isValidRequestor is allowing:  <*> 
"<org.apache.hadoop.hdfs.qjournal.server.GetJournalEditServlet: boolean isValidRequestor(javax.servlet.http.HttpServletRequest,org.apache.hadoop.conf.Configuration)>",debug,isValidRequestor is allowing other JN principal:  <*> 
"<org.apache.hadoop.hdfs.qjournal.server.GetJournalEditServlet: boolean isValidRequestor(javax.servlet.http.HttpServletRequest,org.apache.hadoop.conf.Configuration)>",debug,isValidRequestor is rejecting:  <*> 
"<org.apache.hadoop.hdfs.qjournal.server.GetJournalEditServlet: boolean isValidRequestor(javax.servlet.http.HttpServletRequest,org.apache.hadoop.conf.Configuration)>",warn,Received null remoteUser while authorizing access to GetJournalEditServlet
"<org.apache.hadoop.hdfs.qjournal.server.GetJournalEditServlet: boolean checkStorageInfoOrSendError(org.apache.hadoop.hdfs.qjournal.server.JNStorage,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)>",warn,Received an invalid request file transfer request from  <*> :  <*> 
"<org.apache.hadoop.hdfs.qjournal.server.GetJournalEditServlet: boolean checkRequestorOrSendError(org.apache.hadoop.conf.Configuration,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)>",warn,Received non-NN/JN request for edits from  <*> 
<org.apache.hadoop.hdfs.qjournal.client.QuorumOutputStream: void flushAndSync(boolean)>,waitForWriteQuorum,<*> <*> <*> <*>
<org.apache.hadoop.hdfs.qjournal.client.QuorumOutputStream: void abort()>,warn,Aborting  this 
"<org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: void selectInputStreams(java.util.Collection,long,boolean)>",debug,selectInputStream manifests:\n <*> 
<org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: void recoverUnfinalizedSegments()>,debug,newEpoch( <*> ) responses:\n <*> 
<org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: void recoverUnfinalizedSegments()>,info,Starting recovery process for unclosed journal segments...
<org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: void recoverUnfinalizedSegments()>,info,Successfully started new epoch  <*> 
<org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: void recoverUnclosedSegment(long)>,<init>,"One of the loggers had a response, but no best logger was found."
<org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: void recoverUnclosedSegment(long)>,info,Beginning recovery of unclosed segment starting at txid  segmentTxId 
<org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: void recoverUnclosedSegment(long)>,info,Recovery prepare phase complete. Responses:\n <*> 
<org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: void recoverUnclosedSegment(long)>,info,Using already-accepted recovery for segment starting at txid  segmentTxId :  <*> 
<org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: void recoverUnclosedSegment(long)>,info,Using longest log:  <*> 
<org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: void recoverUnclosedSegment(long)>,info,None of the responders had a log to recover:  <*> 
<org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: void recoverUnclosedSegment(long)>,waitForWriteQuorum,<*> <*>
<org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: void recoverUnclosedSegment(long)>,waitForWriteQuorum,<*> <*>
<org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: void purgeLogsOlderThan(long)>,info,Purging remote journals older than txid  minTxIdToKeep 
"<org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: void finalizeLogSegment(long,long)>",waitForWriteQuorum,firstTxId lastTxId
"<org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: void <init>(org.apache.hadoop.conf.Configuration,java.net.URI,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo,org.apache.hadoop.hdfs.qjournal.client.AsyncLogger$Factory)>",<init>,loggerFactory
"<org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: org.apache.hadoop.hdfs.server.namenode.EditLogOutputStream startLogSegment(long,int)>",waitForWriteQuorum,txId layoutVersion
<org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: java.util.List getLoggerAddresses(java.net.URI)>,warn,Quorum journal URI \' uri \' has an even number  of Journal Nodes specified. This is not recommended! 
<org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: boolean hasSomeData()>,<init>,Timed out waiting for response from loggers
"<org.apache.hadoop.hdfs.qjournal.client.QuorumCall: void waitFor(int,int,int,int,java.lang.String)>",info,msg . No responses yet. 
"<org.apache.hadoop.hdfs.qjournal.client.QuorumCall: void waitFor(int,int,int,int,java.lang.String)>",warn,msg . No responses yet. 
<org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$7: java.lang.Void call()>,addWriteEndToEndLatency,<*> <*>
<org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$7: java.lang.Void call()>,addWriteEndToEndLatency,<*> <*>
<org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$7: java.lang.Void call()>,addWriteRpcLatency,<*> <*>
<org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$7: java.lang.Void call()>,addWriteRpcLatency,<*> <*>
<org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$7: java.lang.Void call()>,warn,Took  <*> ms to send a batch of  <*>  edits ( <*>  bytes) to  remote journal  <*> 
<org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$7: java.lang.Void call()>,warn,Remote journal  <*>  failed to  write txns  <*> - <*> . Will try to write to this JN again after the next  log roll. 
<org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$7: java.lang.Void call()>,warn,Took  <*> ms to send a batch of  <*>  edits ( <*>  bytes) to  remote journal  <*> 
<org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$10: java.lang.Void call()>,info,Restarting previously-stopped writes to  <*>  in segment starting at txid  <*> 
"<org.apache.hadoop.hdfs.protocolPB.ClientDatanodeProtocolTranslatorPB: void <init>(org.apache.hadoop.hdfs.protocol.DatanodeID,org.apache.hadoop.conf.Configuration,int,boolean)>",debug,Connecting to datanode  <*>  addr= <*> 
"<org.apache.hadoop.hdfs.protocolPB.ClientDatanodeProtocolTranslatorPB: org.apache.hadoop.hdfs.protocolPB.ClientDatanodeProtocolPB createClientDatanodeProtocolProxy(org.apache.hadoop.hdfs.protocol.DatanodeID,org.apache.hadoop.conf.Configuration,int,boolean,org.apache.hadoop.hdfs.protocol.LocatedBlock)>",debug,Connecting to datanode  <*>  addr= <*> 
"<org.apache.hadoop.hdfs.protocol.datatransfer.Sender: void send(java.io.DataOutputStream,org.apache.hadoop.hdfs.protocol.datatransfer.Op,com.google.protobuf.Message)>",trace,Sending DataTransferOp  <*> :  proto 
"<org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer: org.apache.hadoop.hdfs.protocol.datatransfer.IOStreamPair receive(org.apache.hadoop.hdfs.net.Peer,java.io.OutputStream,java.io.InputStream,int,org.apache.hadoop.hdfs.protocol.DatanodeID)>",debug,"SASL server doing encrypted handshake for peer = {}, datanodeId = {}"
"<org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer: org.apache.hadoop.hdfs.protocol.datatransfer.IOStreamPair receive(org.apache.hadoop.hdfs.net.Peer,java.io.OutputStream,java.io.InputStream,int,org.apache.hadoop.hdfs.protocol.DatanodeID)>",debug,"SASL server skipping handshake in unsecured configuration for peer = {}, datanodeId = {}"
"<org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer: org.apache.hadoop.hdfs.protocol.datatransfer.IOStreamPair receive(org.apache.hadoop.hdfs.net.Peer,java.io.OutputStream,java.io.InputStream,int,org.apache.hadoop.hdfs.protocol.DatanodeID)>",debug,"SASL server skipping handshake in secured configuration for peer = {}, datanodeId = {}"
"<org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer: org.apache.hadoop.hdfs.protocol.datatransfer.IOStreamPair receive(org.apache.hadoop.hdfs.net.Peer,java.io.OutputStream,java.io.InputStream,int,org.apache.hadoop.hdfs.protocol.DatanodeID)>",debug,"SASL server doing general handshake for peer = {}, datanodeId = {}"
"<org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer: org.apache.hadoop.hdfs.protocol.datatransfer.IOStreamPair receive(org.apache.hadoop.hdfs.net.Peer,java.io.OutputStream,java.io.InputStream,int,org.apache.hadoop.hdfs.protocol.DatanodeID)>",debug,"SASL server skipping handshake in secured configuration with no SASL protection configured for peer = {}, datanodeId = {}"
"<org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer: org.apache.hadoop.hdfs.protocol.datatransfer.IOStreamPair getEncryptedStreams(org.apache.hadoop.hdfs.net.Peer,java.io.OutputStream,java.io.InputStream)>",debug,Server using encryption algorithm  <*> 
"<org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer: org.apache.hadoop.hdfs.protocol.datatransfer.IOStreamPair doSaslHandshake(java.io.OutputStream,java.io.InputStream,java.util.Map,javax.security.auth.callback.CallbackHandler)>",debug,Server using cipher suite  <*> 
"<org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient: org.apache.hadoop.hdfs.protocol.datatransfer.IOStreamPair send(java.net.InetAddress,java.io.OutputStream,java.io.InputStream,org.apache.hadoop.hdfs.security.token.block.DataEncryptionKey,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.protocol.DatanodeID)>",debug,"SASL client doing encrypted handshake for addr = {}, datanodeId = {}"
"<org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient: org.apache.hadoop.hdfs.protocol.datatransfer.IOStreamPair send(java.net.InetAddress,java.io.OutputStream,java.io.InputStream,org.apache.hadoop.hdfs.security.token.block.DataEncryptionKey,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.protocol.DatanodeID)>",debug,"SASL client skipping handshake in unsecured configuration for addr = {}, datanodeId = {}"
"<org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient: org.apache.hadoop.hdfs.protocol.datatransfer.IOStreamPair send(java.net.InetAddress,java.io.OutputStream,java.io.InputStream,org.apache.hadoop.hdfs.security.token.block.DataEncryptionKey,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.protocol.DatanodeID)>",debug,"SASL client skipping handshake in secured configuration with privileged port for addr = {}, datanodeId = {}"
"<org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient: org.apache.hadoop.hdfs.protocol.datatransfer.IOStreamPair send(java.net.InetAddress,java.io.OutputStream,java.io.InputStream,org.apache.hadoop.hdfs.security.token.block.DataEncryptionKey,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.protocol.DatanodeID)>",debug,"SASL client skipping handshake in secured configuration with unsecured cluster for addr = {}, datanodeId = {}"
"<org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient: org.apache.hadoop.hdfs.protocol.datatransfer.IOStreamPair send(java.net.InetAddress,java.io.OutputStream,java.io.InputStream,org.apache.hadoop.hdfs.security.token.block.DataEncryptionKey,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.protocol.DatanodeID)>",debug,"SASL client doing general handshake for addr = {}, datanodeId = {}"
"<org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient: org.apache.hadoop.hdfs.protocol.datatransfer.IOStreamPair send(java.net.InetAddress,java.io.OutputStream,java.io.InputStream,org.apache.hadoop.hdfs.security.token.block.DataEncryptionKey,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.protocol.DatanodeID)>",debug,"SASL client skipping handshake in secured configuration with no SASL protection configured for addr = {}, datanodeId = {}"
"<org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient: org.apache.hadoop.hdfs.protocol.datatransfer.IOStreamPair getEncryptedStreams(java.io.OutputStream,java.io.InputStream,org.apache.hadoop.hdfs.security.token.block.DataEncryptionKey)>",debug,Client using encryption algorithm {}
"<org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient: org.apache.hadoop.hdfs.protocol.datatransfer.IOStreamPair checkTrustAndSend(java.net.InetAddress,java.io.OutputStream,java.io.InputStream,org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataEncryptionKeyFactory,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.protocol.DatanodeID)>",debug,"SASL client skipping handshake on trusted connection for addr = {}, datanodeId = {}"
"<org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil: void checkSaslComplete(org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslParticipant,java.util.Map)>",debug,"Verifying QOP, requested QOP = {}, negotiated QOP = {}"
<org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil: org.apache.hadoop.security.SaslPropertiesResolver getSaslPropertiesResolver(org.apache.hadoop.conf.Configuration)>,debug,"DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for {}"
<org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil: org.apache.hadoop.security.SaslPropertiesResolver getSaslPropertiesResolver(org.apache.hadoop.conf.Configuration)>,debug,"DataTransferProtocol using SaslPropertiesResolver, configured QOP {} = {}, configured class {} = {}"
"<org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil: org.apache.hadoop.hdfs.protocol.datatransfer.IOStreamPair createStreamPair(org.apache.hadoop.conf.Configuration,org.apache.hadoop.crypto.CipherOption,java.io.OutputStream,java.io.InputStream,boolean)>",debug,Creating IOStreamPair of CryptoInputStream and CryptoOutputStream.
"<org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver: void doRead(java.nio.channels.ReadableByteChannel,java.io.InputStream)>",trace,readNextPacket: dataPlusChecksumLen =  dataPlusChecksumLen  headerLen =  $i 
"<org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: java.util.List chooseStorageTypes(short,java.lang.Iterable,java.util.EnumSet,boolean)>",warn,"Failed to place enough replicas: expected size is  expectedSize  but only  <*>  storage types can be selected  (replication= $i , selected= <*> , unavailable= unavailables , removed= $u , policy= this ) "
"<org.apache.hadoop.hdfs.PeerCache: void <init>(int,long)>",info,SocketCache disabled.
"<org.apache.hadoop.hdfs.PeerCache: org.apache.hadoop.hdfs.net.Peer get(org.apache.hadoop.hdfs.protocol.DatanodeID,boolean)>",warn,"got IOException closing stale peer  <*> , which is  ageMs  ms old "
<org.apache.hadoop.hdfs.net.TcpPeerServer: void close()>,error,error closing TcpPeerServer: 
<org.apache.hadoop.hdfs.net.DomainPeerServer: void close()>,error,error closing DomainPeerServer: 
"<org.apache.hadoop.hdfs.NameNodeProxies: org.apache.hadoop.hdfs.server.namenode.ha.AbstractNNFailoverProxyProvider createFailoverProxyProvider(org.apache.hadoop.conf.Configuration,java.net.URI,java.lang.Class,boolean,java.util.concurrent.atomic.AtomicBoolean)>",debug,Couldn\'t create proxy provider  failoverProxyProviderClass 
"<org.apache.hadoop.hdfs.NameNodeProxies: org.apache.hadoop.hdfs.NameNodeProxies$ProxyAndInfo createProxyWithLossyRetryHandler(org.apache.hadoop.conf.Configuration,java.net.URI,java.lang.Class,int,java.util.concurrent.atomic.AtomicBoolean)>",warn,Currently creating proxy using LossyRetryInvocationHandler requires NN HA setup
"<org.apache.hadoop.hdfs.NameNodeProxies: org.apache.hadoop.hdfs.NameNodeProxies$ProxyAndInfo createNonHAProxy(org.apache.hadoop.conf.Configuration,java.net.InetSocketAddress,java.lang.Class,org.apache.hadoop.security.UserGroupInformation,boolean,java.util.concurrent.atomic.AtomicBoolean)>",error,Unsupported protocol found when creating the proxy connection to NameNode:  <*> 
<org.apache.hadoop.hdfs.LeaseRenewer: void run(int)>,debug,Lease renewer daemon for  <*>  with renew id  id  executed 
<org.apache.hadoop.hdfs.LeaseRenewer: void run(int)>,debug,Lease renewer daemon for  <*>  with renew id  id  is not current 
<org.apache.hadoop.hdfs.LeaseRenewer: void run(int)>,debug,Lease renewer daemon for  <*>  with renew id  id  expired 
<org.apache.hadoop.hdfs.LeaseRenewer: void run(int)>,warn,Failed to renew lease for  <*>  for  <*>  seconds.  Aborting ... 
<org.apache.hadoop.hdfs.LeaseRenewer: void run(int)>,warn,Failed to renew lease for  <*>  for  <*>  seconds.  Will retry shortly ... 
<org.apache.hadoop.hdfs.LeaseRenewer: void renew()>,debug,Did not renew lease for client  c 
<org.apache.hadoop.hdfs.LeaseRenewer: void renew()>,debug,Lease renewed for client  <*> 
<org.apache.hadoop.hdfs.LeaseRenewer: void interruptAndJoin()>,debug,Wait for lease checker to terminate
<org.apache.hadoop.hdfs.LeaseRenewer$1: void run()>,debug,Lease renewer daemon for  <*>  with renew id  <*>  started 
<org.apache.hadoop.hdfs.LeaseRenewer$1: void run()>,debug,Lease renewer daemon for  <*>  with renew id  <*>  exited 
<org.apache.hadoop.hdfs.LeaseRenewer$1: void run()>,debug,<*>  is interrupted. 
<org.apache.hadoop.hdfs.LeaseRenewer$1: void run()>,debug,Lease renewer daemon for  <*>  with renew id  <*>  exited 
<org.apache.hadoop.hdfs.LeaseRenewer$1: void run()>,debug,Lease renewer daemon for  <*>  with renew id  <*>  exited 
"<org.apache.hadoop.hdfs.HAUtil: void cloneDelegationTokenForLogicalUri(org.apache.hadoop.security.UserGroupInformation,java.net.URI,java.util.Collection)>",debug,Mapped HA service delegation token for logical URI  haUri  to namenode  singleNNAddr 
"<org.apache.hadoop.hdfs.HAUtil: void cloneDelegationTokenForLogicalUri(org.apache.hadoop.security.UserGroupInformation,java.net.URI,java.util.Collection)>",debug,No HA service delegation token found for logical URI  haUri 
"<org.apache.hadoop.hdfs.DFSUtil: org.apache.hadoop.http.HttpServer2$Builder httpServerTemplateForNNAndJN(org.apache.hadoop.conf.Configuration,java.net.InetSocketAddress,java.net.InetSocketAddress,java.lang.String,java.lang.String,java.lang.String)>",info,Starting web server as:  <*> 
"<org.apache.hadoop.hdfs.DFSUtil: org.apache.hadoop.http.HttpServer2$Builder httpServerTemplateForNNAndJN(org.apache.hadoop.conf.Configuration,java.net.InetSocketAddress,java.net.InetSocketAddress,java.lang.String,java.lang.String,java.lang.String)>",info,Starting Web-server for  name  at:  <*> 
"<org.apache.hadoop.hdfs.DFSUtil: org.apache.hadoop.http.HttpServer2$Builder httpServerTemplateForNNAndJN(org.apache.hadoop.conf.Configuration,java.net.InetSocketAddress,java.net.InetSocketAddress,java.lang.String,java.lang.String,java.lang.String)>",info,Starting Web-server for  name  at:  <*> 
<org.apache.hadoop.hdfs.DFSUtil: org.apache.hadoop.http.HttpConfig$Policy getHttpPolicy(org.apache.hadoop.conf.Configuration)>,warn,hadoop.ssl.enabled is deprecated. Please use dfs.http.policy.
<org.apache.hadoop.hdfs.DFSUtil: org.apache.hadoop.http.HttpConfig$Policy getHttpPolicy(org.apache.hadoop.conf.Configuration)>,warn,dfs.https.enable is deprecated. Please use dfs.http.policy.
"<org.apache.hadoop.hdfs.DFSUtil: java.util.Map getAddressesForNameserviceId(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String[])>",warn,Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
"<org.apache.hadoop.hdfs.DFSUtil: java.lang.String[] getSuffixIDs(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.hdfs.DFSUtil$AddressMatcher)>",warn,Exception in creating socket address  <*> 
"<org.apache.hadoop.hdfs.DFSOutputStream: void writeChunk(byte[],int,int,byte[],int,int)>",debug,"DFSClient writeChunk allocating new packet seqno= <*> , src= <*> , packetSize= <*> , chunksPerPacket= <*> , bytesCurBlock= <*> "
"<org.apache.hadoop.hdfs.DFSOutputStream: void writeChunk(byte[],int,int,byte[],int,int)>",debug,"DFSClient writeChunk packet full seqno= <*> , src= <*> , bytesCurBlock= <*> , blockSize= <*> , appendChunk= <*> "
<org.apache.hadoop.hdfs.DFSOutputStream: void waitForAckedSeqno(long)>,debug,Waiting for ack for:  seqno 
<org.apache.hadoop.hdfs.DFSOutputStream: void waitForAckedSeqno(long)>,warn,Slow waitForAckedSeqno took  <*> ms (threshold= <*> ms) 
<org.apache.hadoop.hdfs.DFSOutputStream: void queueCurrentPacket()>,debug,Queued packet  <*> 
"<org.apache.hadoop.hdfs.DFSOutputStream: void flushOrSync(boolean,java.util.EnumSet)>",debug,DFSClient flush() : bytesCurBlock  <*>  lastFlushOffset  <*> 
"<org.apache.hadoop.hdfs.DFSOutputStream: void flushOrSync(boolean,java.util.EnumSet)>",warn,Unable to persist blocks in hflush for  <*> 
"<org.apache.hadoop.hdfs.DFSOutputStream: void flushOrSync(boolean,java.util.EnumSet)>",warn,Error while syncing
"<org.apache.hadoop.hdfs.DFSOutputStream: void computePacketChunkSize(int,int)>",debug,"computePacketChunkSize: src= <*> , chunkSize= chunkSize , chunksPerPacket= <*> , packetSize= <*> "
<org.apache.hadoop.hdfs.DFSOutputStream: void completeFile(org.apache.hadoop.hdfs.protocol.ExtendedBlock)>,info,Unable to close file because dfsclient  was unable to contact the HDFS servers. clientRunning  <*>  hdfsTimeout  <*> 
<org.apache.hadoop.hdfs.DFSOutputStream: void completeFile(org.apache.hadoop.hdfs.protocol.ExtendedBlock)>,info,Could not complete  <*>  retrying... 
<org.apache.hadoop.hdfs.DFSOutputStream: void completeFile(org.apache.hadoop.hdfs.protocol.ExtendedBlock)>,warn,Caught exception 
"<org.apache.hadoop.hdfs.DFSOutputStream: void <init>(org.apache.hadoop.hdfs.DFSClient,java.lang.String,org.apache.hadoop.util.Progressable,org.apache.hadoop.hdfs.protocol.HdfsFileStatus,org.apache.hadoop.util.DataChecksum)>",debug,Set non-null progress callback on DFSOutputStream  src 
"<org.apache.hadoop.hdfs.DFSOutputStream: java.net.Socket createSocketForPipeline(org.apache.hadoop.hdfs.protocol.DatanodeInfo,int,org.apache.hadoop.hdfs.DFSClient)>",debug,Connecting to datanode  <*> 
"<org.apache.hadoop.hdfs.DFSOutputStream: java.net.Socket createSocketForPipeline(org.apache.hadoop.hdfs.protocol.DatanodeInfo,int,org.apache.hadoop.hdfs.DFSClient)>",debug,Send buf size  <*> 
<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: void run()>,debug,Allocating new block
<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: void run()>,debug,Append to block  <*> 
<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: void run()>,debug,DataStreamer block  <*>  sending packet  e_ 
<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: void run()>,warn,Caught exception 
<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: void run()>,warn,Caught exception 
<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: void run()>,warn,Caught exception 
<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: void run()>,warn,DataStreamer Exception
<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: void endBlock()>,debug,Closing old block  <*> 
<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: void closeResponder()>,warn,Caught exception 
<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: void addDatanode2ExistingPipeline()>,debug,lastAckedSeqno =  <*> 
<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: org.apache.hadoop.hdfs.protocol.LocatedBlock nextBlockOutputStream()>,info,Abandoning  <*> 
<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: org.apache.hadoop.hdfs.protocol.LocatedBlock nextBlockOutputStream()>,info,Excluding datanode  <*> 
"<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: org.apache.hadoop.hdfs.protocol.LocatedBlock locateFollowingBlock(long,org.apache.hadoop.hdfs.protocol.DatanodeInfo[])>",info,Exception while adding a block
"<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: org.apache.hadoop.hdfs.protocol.LocatedBlock locateFollowingBlock(long,org.apache.hadoop.hdfs.protocol.DatanodeInfo[])>",info,Waiting for replication for  <*>  seconds 
"<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: org.apache.hadoop.hdfs.protocol.LocatedBlock locateFollowingBlock(long,org.apache.hadoop.hdfs.protocol.DatanodeInfo[])>",warn,NotReplicatedYetException sleeping  <*>  retries left  retries 
"<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: org.apache.hadoop.hdfs.protocol.LocatedBlock locateFollowingBlock(long,org.apache.hadoop.hdfs.protocol.DatanodeInfo[])>",warn,Caught exception 
<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: boolean setupPipelineForAppendOrRecovery()>,warn,Could not get block locations. Source file \ <*> \ - Aborting... 
<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: boolean setupPipelineForAppendOrRecovery()>,warn,Error Recovery for block  <*>  in pipeline  $u : bad datanode  <*> 
<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: boolean setupPipelineForAppendOrRecovery()>,warn,Failed to replace datanode. Continue with the remaining datanodes since dfs.client.block.write.replace-datanode-on-failure.best-effort is set to true.
<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: boolean setupPipelineForAppendOrRecovery()>,warn,Datanode did not restart in time:  <*> 
<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: boolean processDatanodeError()>,info,Error Recovery for  <*>  waiting for responder to exit.  
<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: boolean processDatanodeError()>,warn,Error recovering pipeline for writing  <*> . Already retried  times for the same packet. 
"<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: boolean createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.hdfs.StorageType[],long,boolean)>",debug,pipeline =  <*> 
"<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: boolean createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.hdfs.StorageType[],long,boolean)>",info,nodes are empty for write pipeline of block  <*> 
"<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: boolean createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.hdfs.StorageType[],long,boolean)>",info,Exception in createBlockOutputStream
"<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: boolean createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.hdfs.StorageType[],long,boolean)>",info,"Will fetch a new encryption key and retry, encryption key was invalid when connecting to  <*>  :  <*> "
"<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: boolean createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.hdfs.StorageType[],long,boolean)>",info,Waiting for the datanode to be restarted:  <*> 
<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor: void run()>,debug,DFSClient  $u 
<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor: void run()>,info,A datanode is restarting:  <*> 
<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor: void run()>,warn,"Slow ReadProcessor read fields took  duration ms (threshold= <*> ms); ack:  $u , targets:  <*> "
<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor: void run()>,warn,DFSOutputStream ResponseProcessor exception  for block  <*> 
<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$2: void onRemoval(com.google.common.cache.RemovalNotification)>,info,Removing node  <*>  from the excluded nodes list 
<org.apache.hadoop.hdfs.DFSInputStream: void seek(long)>,debug,Exception while seek to  targetPos  from  <*>  of  <*>  from  <*> 
<org.apache.hadoop.hdfs.DFSInputStream: void seek(long)>,warn,"BlockReader failed to seek to  targetPos . Instead, it seeked to  <*> . "
<org.apache.hadoop.hdfs.DFSInputStream: void openInfo()>,warn,Last block locations not available. Datanodes might not have reported blocks completely. Will retry for  retriesForLastBlockLength  times 
<org.apache.hadoop.hdfs.DFSInputStream: void closeCurrentBlockReader()>,error,error closing blockReader
<org.apache.hadoop.hdfs.DFSInputStream: void close()>,warn,"closing file  <*> , but there are still  unreleased ByteBuffers allocated by read().   Please release  <*> . "
"<org.apache.hadoop.hdfs.DFSInputStream: void actualGetFromOneDataNode(org.apache.hadoop.hdfs.DFSInputStream$DNAddrPair,org.apache.hadoop.hdfs.protocol.LocatedBlock,long,long,byte[],int,java.util.Map)>",info,"Will fetch a new encryption key and retry, encryption key was invalid when connecting to  targetAddr  :  <*> "
"<org.apache.hadoop.hdfs.DFSInputStream: void actualGetFromOneDataNode(org.apache.hadoop.hdfs.DFSInputStream$DNAddrPair,org.apache.hadoop.hdfs.protocol.LocatedBlock,long,long,byte[],int,java.util.Map)>",warn,fetchBlockByteRange(). Got a checksum exception for  <*>  at  <*> : <*>  from  chosenNode 
"<org.apache.hadoop.hdfs.DFSInputStream: void actualGetFromOneDataNode(org.apache.hadoop.hdfs.DFSInputStream$DNAddrPair,org.apache.hadoop.hdfs.protocol.LocatedBlock,long,long,byte[],int,java.util.Map)>",warn,Connection failure:  <*> 
<org.apache.hadoop.hdfs.DFSInputStream: org.apache.hadoop.hdfs.protocol.DatanodeInfo blockSeekTo(long)>,info,Successfully connected to  <*>  for  <*> 
<org.apache.hadoop.hdfs.DFSInputStream: org.apache.hadoop.hdfs.protocol.DatanodeInfo blockSeekTo(long)>,info,"Will fetch a new encryption key and retry, encryption key was invalid when connecting to  <*>  :  <*> "
<org.apache.hadoop.hdfs.DFSInputStream: org.apache.hadoop.hdfs.protocol.DatanodeInfo blockSeekTo(long)>,warn,"Failed to connect to  <*>  for block , add to deadNodes and continue.  <*> "
"<org.apache.hadoop.hdfs.DFSInputStream: org.apache.hadoop.hdfs.DFSInputStream$DNAddrPair getBestNodeDNAddrPair(org.apache.hadoop.hdfs.protocol.LocatedBlock,java.util.Collection)>",debug,Connecting to datanode  <*> 
"<org.apache.hadoop.hdfs.DFSInputStream: org.apache.hadoop.hdfs.DFSInputStream$DNAddrPair chooseDataNode(org.apache.hadoop.hdfs.protocol.LocatedBlock,java.util.Collection)>",info,No node available for  <*> 
"<org.apache.hadoop.hdfs.DFSInputStream: org.apache.hadoop.hdfs.DFSInputStream$DNAddrPair chooseDataNode(org.apache.hadoop.hdfs.protocol.LocatedBlock,java.util.Collection)>",info,Could not obtain  <*>  from any node:  <*> <*> . Will get new block locations from namenode and retry... 
"<org.apache.hadoop.hdfs.DFSInputStream: org.apache.hadoop.hdfs.DFSInputStream$DNAddrPair chooseDataNode(org.apache.hadoop.hdfs.protocol.LocatedBlock,java.util.Collection)>",warn,<*> <*> . Throwing a BlockMissingException 
"<org.apache.hadoop.hdfs.DFSInputStream: org.apache.hadoop.hdfs.DFSInputStream$DNAddrPair chooseDataNode(org.apache.hadoop.hdfs.protocol.LocatedBlock,java.util.Collection)>",warn,"DFS chooseDataNode: got #  <*>  IOException, will wait for  <*>  msec. "
<org.apache.hadoop.hdfs.DFSInputStream: long readBlockLength(org.apache.hadoop.hdfs.protocol.LocatedBlock)>,debug,Failed to getReplicaVisibleLength from datanode  datanode  for block  <*> 
<org.apache.hadoop.hdfs.DFSInputStream: long fetchLocatedBlocksAndGetLastBlockLength()>,debug,newInfo =  <*> 
"<org.apache.hadoop.hdfs.DFSInputStream: java.nio.ByteBuffer tryReadZeroCopy(int,java.util.EnumSet)>",debug,Unable to perform a zero-copy read from offset  curPos  of  <*> ;  length  bytes left in block.   blockPos= blockPos ; curPos= curPos ; curEnd= curEnd 
"<org.apache.hadoop.hdfs.DFSInputStream: java.nio.ByteBuffer tryReadZeroCopy(int,java.util.EnumSet)>",debug,Reducing read length from  maxLength  to  length  to avoid going more than one byte  past the end of the block.  blockPos= blockPos ; curPos= curPos ; curEnd= curEnd 
"<org.apache.hadoop.hdfs.DFSInputStream: java.nio.ByteBuffer tryReadZeroCopy(int,java.util.EnumSet)>",debug,"Unable to perform a zero-copy read from offset  curPos  of  <*> ; -bit MappedByteBuffer limit  exceeded.  blockPos= blockPos , curEnd= curEnd "
"<org.apache.hadoop.hdfs.DFSInputStream: java.nio.ByteBuffer tryReadZeroCopy(int,java.util.EnumSet)>",debug,Reducing read length from  maxLength  to  length  to avoid -bit limit.   blockPos= blockPos ; curPos= curPos ; curEnd= curEnd 
"<org.apache.hadoop.hdfs.DFSInputStream: java.nio.ByteBuffer tryReadZeroCopy(int,java.util.EnumSet)>",debug,unable to perform a zero-copy read from offset  curPos  of  <*> ; BlockReader#getClientMmap returned  null. 
"<org.apache.hadoop.hdfs.DFSInputStream: java.nio.ByteBuffer tryReadZeroCopy(int,java.util.EnumSet)>",debug,readZeroCopy read  length  bytes from offset  curPos  via the zero-copy read  path.  blockEnd =  <*> 
"<org.apache.hadoop.hdfs.DFSInputStream: int readWithStrategy(org.apache.hadoop.hdfs.DFSInputStream$ReaderStrategy,int,int)>",warn,DFS Read
"<org.apache.hadoop.hdfs.DFSInputStream: int readBuffer(org.apache.hadoop.hdfs.DFSInputStream$ReaderStrategy,int,int,java.util.Map)>",warn,Found Checksum error for  <*>  from  <*>  at  <*> 
"<org.apache.hadoop.hdfs.DFSInputStream: int readBuffer(org.apache.hadoop.hdfs.DFSInputStream$ReaderStrategy,int,int,java.util.Map)>",warn,Exception while reading from  <*>  of  <*>  from  <*> 
"<org.apache.hadoop.hdfs.DFSInputStream: boolean tokenRefetchNeeded(java.io.IOException,java.net.InetSocketAddress)>",info,Access token was invalid when connecting to  targetAddr  :  ex 
<org.apache.hadoop.hdfs.DFSInotifyEventInputStream: org.apache.hadoop.hdfs.inotify.EventBatch take()>,debug,"take(): poll() returned null, sleeping for {} ms"
"<org.apache.hadoop.hdfs.DFSInotifyEventInputStream: org.apache.hadoop.hdfs.inotify.EventBatch poll(long,java.util.concurrent.TimeUnit)>",debug,timed poll(): timed out
"<org.apache.hadoop.hdfs.DFSInotifyEventInputStream: org.apache.hadoop.hdfs.inotify.EventBatch poll(long,java.util.concurrent.TimeUnit)>",debug,"timed poll(): poll() returned null, sleeping for {} ms"
<org.apache.hadoop.hdfs.DFSInotifyEventInputStream: org.apache.hadoop.hdfs.inotify.EventBatch poll()>,debug,"poll(): lastReadTxid is -, reading current txid from NN"
<org.apache.hadoop.hdfs.DFSInotifyEventInputStream: org.apache.hadoop.hdfs.inotify.EventBatch poll()>,debug,poll(): read no edits from the NN when requesting edits after txid {}
"<org.apache.hadoop.hdfs.DFSClient: void reportChecksumFailure(java.lang.String,org.apache.hadoop.hdfs.protocol.LocatedBlock[])>",info,Found corruption while reading  file . Error repairing corrupt blocks. Bad blocks remain. 
<org.apache.hadoop.hdfs.DFSClient: void initThreadsNumForHedgedReads(int)>,debug,Using hedged reads; pool threads= num 
<org.apache.hadoop.hdfs.DFSClient: void clearDataEncryptionKey()>,debug,Clearing encryption key
<org.apache.hadoop.hdfs.DFSClient: void cancelDelegationToken(org.apache.hadoop.security.token.Token)>,info,Cancelling  <*> 
"<org.apache.hadoop.hdfs.DFSClient: void <init>(java.net.URI,org.apache.hadoop.hdfs.protocol.ClientProtocol,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem$Statistics)>",debug,Using legacy short-circuit local reads.
"<org.apache.hadoop.hdfs.DFSClient: void <init>(java.net.URI,org.apache.hadoop.hdfs.protocol.ClientProtocol,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem$Statistics)>",debug,No KeyProvider found.
"<org.apache.hadoop.hdfs.DFSClient: void <init>(java.net.URI,org.apache.hadoop.hdfs.protocol.ClientProtocol,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem$Statistics)>",debug,Found KeyProvider:  <*> 
"<org.apache.hadoop.hdfs.DFSClient: void <init>(java.net.URI,org.apache.hadoop.hdfs.protocol.ClientProtocol,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem$Statistics)>",debug,Using local interfaces [ <*> ] with addresses [ <*> ] 
"<org.apache.hadoop.hdfs.DFSClient: void <init>(java.net.URI,org.apache.hadoop.hdfs.protocol.ClientProtocol,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem$Statistics)>",warn,"dfs.client.test.drop.namenode.response.number is set to  <*> , this hacked client will proactively drop responses "
<org.apache.hadoop.hdfs.DFSClient: org.apache.hadoop.security.token.Token getDelegationToken(org.apache.hadoop.io.Text)>,info,Created  <*> 
<org.apache.hadoop.hdfs.DFSClient: org.apache.hadoop.security.token.Token getDelegationToken(org.apache.hadoop.io.Text)>,info,Cannot get delegation token from  renewer 
<org.apache.hadoop.hdfs.DFSClient: org.apache.hadoop.hdfs.security.token.block.DataEncryptionKey newDataEncryptionKey()>,debug,Getting new encryption token from NN
"<org.apache.hadoop.hdfs.DFSClient: org.apache.hadoop.hdfs.protocol.datatransfer.IOStreamPair connectToDN(org.apache.hadoop.hdfs.protocol.DatanodeInfo,int,org.apache.hadoop.hdfs.protocol.LocatedBlock)>",debug,Connecting to datanode  <*> 
"<org.apache.hadoop.hdfs.DFSClient: org.apache.hadoop.hdfs.DFSOutputStream create(java.lang.String,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,boolean,short,long,org.apache.hadoop.util.Progressable,int,org.apache.hadoop.fs.Options$ChecksumOpt,java.net.InetSocketAddress[])>",debug,src : masked= <*> 
"<org.apache.hadoop.hdfs.DFSClient: org.apache.hadoop.fs.MD5MD5CRC32FileChecksum getFileChecksum(java.lang.String,long)>",debug,"write to  <*> :  <*> , block= <*> "
"<org.apache.hadoop.hdfs.DFSClient: org.apache.hadoop.fs.MD5MD5CRC32FileChecksum getFileChecksum(java.lang.String,long)>",debug,Retrieving checksum from an earlier-version DataNode: inferring checksum by reading first byte
"<org.apache.hadoop.hdfs.DFSClient: org.apache.hadoop.fs.MD5MD5CRC32FileChecksum getFileChecksum(java.lang.String,long)>",debug,"set bytesPerCRC= bytesPerCRC , crcPerBlock= crcPerBlock "
"<org.apache.hadoop.hdfs.DFSClient: org.apache.hadoop.fs.MD5MD5CRC32FileChecksum getFileChecksum(java.lang.String,long)>",debug,got reply from  <*> : md= $u 
"<org.apache.hadoop.hdfs.DFSClient: org.apache.hadoop.fs.MD5MD5CRC32FileChecksum getFileChecksum(java.lang.String,long)>",debug,Got access token error in response to OP_BLOCK_CHECKSUM for file  src  for block  <*>  from datanode  <*> . Will retry the block once. 
"<org.apache.hadoop.hdfs.DFSClient: org.apache.hadoop.fs.MD5MD5CRC32FileChecksum getFileChecksum(java.lang.String,long)>",warn,"src= src , datanodes[ j ]= <*> "
<org.apache.hadoop.hdfs.DFSClient: org.apache.hadoop.fs.BlockStorageLocation[] getBlockStorageLocations(java.util.List)>,trace,metadata returned:  <*> 
<org.apache.hadoop.hdfs.DFSClient: long renewDelegationToken(org.apache.hadoop.security.token.Token)>,info,Renewing  <*> 
<org.apache.hadoop.hdfs.DFSClient: long getBlockSize(java.lang.String)>,warn,Problem getting block size
<org.apache.hadoop.hdfs.DFSClient: java.net.SocketAddress getRandomLocalInterfaceAddr()>,debug,Using local interface  addr 
<org.apache.hadoop.hdfs.DFSClient: boolean renewLease()>,warn,Failed to renew lease for  <*>  for  <*>  seconds (>= hard-limit = L  seconds.)  Closing all files being written ... 
"<org.apache.hadoop.hdfs.DFSClient: boolean primitiveMkdir(java.lang.String,org.apache.hadoop.fs.permission.FsPermission,boolean)>",debug,src : masked= absPermission 
<org.apache.hadoop.hdfs.DFSClient: boolean isLocalAddress(java.net.InetSocketAddress)>,trace,Address  targetAddr <*> 
<org.apache.hadoop.hdfs.DFSClient: boolean isLocalAddress(java.net.InetSocketAddress)>,trace,Address  targetAddr <*> 
"<org.apache.hadoop.hdfs.DFSClient$Renewer: void cancel(org.apache.hadoop.security.token.Token,org.apache.hadoop.conf.Configuration)>",info,Cancelling  <*> 
<org.apache.hadoop.hdfs.DFSClient$Conf: void <init>(org.apache.hadoop.conf.Configuration)>,debug,dfs.client.use.legacy.blockreader.local =  <*> 
<org.apache.hadoop.hdfs.DFSClient$Conf: void <init>(org.apache.hadoop.conf.Configuration)>,debug,dfs.client.read.shortcircuit =  <*> 
<org.apache.hadoop.hdfs.DFSClient$Conf: void <init>(org.apache.hadoop.conf.Configuration)>,debug,dfs.client.domain.socket.data.traffic =  <*> 
<org.apache.hadoop.hdfs.DFSClient$Conf: void <init>(org.apache.hadoop.conf.Configuration)>,debug,dfs.domain.socket.path =  <*> 
<org.apache.hadoop.hdfs.DFSClient$Conf: org.apache.hadoop.util.DataChecksum$Type getChecksumType(org.apache.hadoop.conf.Configuration)>,warn,Bad checksum type:  <*> . Using default  CRCC 
"<org.apache.hadoop.hdfs.DFSClient$2: void rejectedExecution(java.lang.Runnable,java.util.concurrent.ThreadPoolExecutor)>",info,"Execution rejected, Executing in current thread"
<org.apache.hadoop.hdfs.ClientContext: void printConfWarningIfNeeded(org.apache.hadoop.hdfs.DFSClient$Conf)>,warn,"Existing client context \' <*> \' does not match  requested configuration.  Existing:  <*> , Requested:  <*> "
<org.apache.hadoop.hdfs.client.HdfsUtils: boolean isHealthy(java.net.URI)>,debug,Is namenode in safemode?  <*> ; uri= uri 
<org.apache.hadoop.hdfs.client.HdfsUtils: boolean isHealthy(java.net.URI)>,debug,Got an exception for uri= uri 
"<org.apache.hadoop.hdfs.BlockStorageLocationUtil: java.util.Map queryDatanodesForHdfsBlocksMetadata(org.apache.hadoop.conf.Configuration,java.util.Map,int,int,boolean)>",debug,Could not fetch information from datanode
"<org.apache.hadoop.hdfs.BlockStorageLocationUtil: java.util.Map queryDatanodesForHdfsBlocksMetadata(org.apache.hadoop.conf.Configuration,java.util.Map,int,int,boolean)>",info,Cancelled while waiting for datanode  <*> :  <*> 
"<org.apache.hadoop.hdfs.BlockStorageLocationUtil: java.util.Map queryDatanodesForHdfsBlocksMetadata(org.apache.hadoop.conf.Configuration,java.util.Map,int,int,boolean)>",info,Datanode  <*>  does not support  required #getHdfsBlocksMetadata() API 
"<org.apache.hadoop.hdfs.BlockStorageLocationUtil: java.util.Map queryDatanodesForHdfsBlocksMetadata(org.apache.hadoop.conf.Configuration,java.util.Map,int,int,boolean)>",info,Failed to query block locations on datanode  <*> :  <*> 
"<org.apache.hadoop.hdfs.BlockStorageLocationUtil: java.util.Map queryDatanodesForHdfsBlocksMetadata(org.apache.hadoop.conf.Configuration,java.util.Map,int,int,boolean)>",info,Interrupted while fetching HdfsBlocksMetadata
"<org.apache.hadoop.hdfs.BlockStorageLocationUtil: java.util.Map queryDatanodesForHdfsBlocksMetadata(org.apache.hadoop.conf.Configuration,java.util.Map,int,int,boolean)>",warn,Invalid access token when trying to retrieve information from datanode  <*> 
"<org.apache.hadoop.hdfs.BlockStorageLocationUtil: java.util.Map associateVolumeIdsWithBlocks(java.util.List,java.util.Map)>",debug,No data for block  blockId 
"<org.apache.hadoop.hdfs.BlockStorageLocationUtil: java.util.Map associateVolumeIdsWithBlocks(java.util.List,java.util.Map)>",debug,"Datanode responded with a block volume id we did not request, omitting."
"<org.apache.hadoop.hdfs.BlockReaderLocalLegacy: org.apache.hadoop.hdfs.protocol.BlockLocalPathInfo getBlockPathInfo(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.conf.Configuration,int,org.apache.hadoop.security.token.Token,boolean,org.apache.hadoop.hdfs.StorageType)>",debug,Cached location of block  blk  as  <*> 
"<org.apache.hadoop.hdfs.BlockReaderLocalLegacy: org.apache.hadoop.hdfs.BlockReaderLocalLegacy newBlockReader(org.apache.hadoop.hdfs.DFSClient$Conf,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.protocol.DatanodeInfo,long,long,org.apache.hadoop.hdfs.StorageType)>",debug,New BlockReaderLocalLegacy for file  <*>  of size  <*>  startOffset  startOffset  length  length  short circuit checksum  <*> 
"<org.apache.hadoop.hdfs.BlockReaderLocalLegacy: org.apache.hadoop.hdfs.BlockReaderLocalLegacy newBlockReader(org.apache.hadoop.hdfs.DFSClient$Conf,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.protocol.DatanodeInfo,long,long,org.apache.hadoop.hdfs.StorageType)>",warn,BlockReaderLocalLegacy: Removing  blk  from cache because local file  <*>  could not be opened. 
<org.apache.hadoop.hdfs.BlockReaderLocalLegacy: long skip(long)>,debug,skip  n 
"<org.apache.hadoop.hdfs.BlockReaderLocalLegacy: int read(byte[],int,int)>",trace,read off  off  len  len 
"<org.apache.hadoop.hdfs.BlockReaderLocalLegacy$LocalDatanodeInfo: org.apache.hadoop.hdfs.protocol.ClientDatanodeProtocol getDatanodeProxy(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.conf.Configuration,int,boolean)>",warn,encountered exception 
<org.apache.hadoop.hdfs.BlockReaderLocal: void close()>,trace,"close(filename= <*> , block= <*> ) "
<org.apache.hadoop.hdfs.BlockReaderLocal: org.apache.hadoop.hdfs.shortcircuit.ClientMmap getClientMmap(java.util.EnumSet)>,trace,"can\'t get an mmap for  <*>  of  <*>  since SKIP_CHECKSUMS was not given,  we aren\'t skipping checksums, and the block is not mlocked. "
<org.apache.hadoop.hdfs.BlockReaderLocal: long skip(long)>,trace,"skip(n= n , block= <*> , filename= <*> ): discarded  discardedFromBuf  bytes from  dataBuf and advanced dataPos by  remaining "
<org.apache.hadoop.hdfs.BlockReaderLocal: int read(java.nio.ByteBuffer)>,info,<*> : starting 
<org.apache.hadoop.hdfs.BlockReaderLocal: int read(java.nio.ByteBuffer)>,info,traceString : I/O error 
<org.apache.hadoop.hdfs.BlockReaderLocal: int read(java.nio.ByteBuffer)>,info,traceString : returning  nRead 
"<org.apache.hadoop.hdfs.BlockReaderLocal: int read(byte[],int,int)>",trace,<*> : starting 
"<org.apache.hadoop.hdfs.BlockReaderLocal: int read(byte[],int,int)>",trace,traceString : I/O error 
"<org.apache.hadoop.hdfs.BlockReaderLocal: int read(byte[],int,int)>",trace,traceString : returning  nRead 
<org.apache.hadoop.hdfs.BlockReaderLocal: boolean fillDataBuf(boolean)>,trace,loaded  <*>  bytes into bounce  buffer from offset  oldDataPos  of  <*> 
"<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplicaInfo requestFileDescriptors(org.apache.hadoop.hdfs.net.DomainPeer,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$Slot)>",debug,this : <*> 
"<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplicaInfo requestFileDescriptors(org.apache.hadoop.hdfs.net.DomainPeer,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$Slot)>",trace,Sending receipt verification byte for slot  slot 
"<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplicaInfo requestFileDescriptors(org.apache.hadoop.hdfs.net.DomainPeer,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$Slot)>",warn,this : error creating ShortCircuitReplica. 
"<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplicaInfo requestFileDescriptors(org.apache.hadoop.hdfs.net.DomainPeer,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$Slot)>",warn,short-circuit read access is disabled for DataNode  <*> .  reason:  <*> 
"<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplicaInfo requestFileDescriptors(org.apache.hadoop.hdfs.net.DomainPeer,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$Slot)>",warn,short-circuit read access for the file  <*>  is disabled for DataNode  <*> .  reason:  <*> 
"<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplicaInfo requestFileDescriptors(org.apache.hadoop.hdfs.net.DomainPeer,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$Slot)>",warn,this : unknown response code  <*>  while attempting to set up short-circuit access.  <*> 
<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplicaInfo createShortCircuitReplicaInfo()>,debug,this : closing stale domain peer  peer 
<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplicaInfo createShortCircuitReplicaInfo()>,trace,this : trying to create ShortCircuitReplicaInfo. 
<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplicaInfo createShortCircuitReplicaInfo()>,trace,this : allocShmSlot used up our previous socket  <*> .  Allocating a new one... 
<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplicaInfo createShortCircuitReplicaInfo()>,warn,this : I/O error requesting file descriptors.   Disabling domain socket  <*> 
<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReaderFactory$BlockReaderPeer nextTcpPeer()>,trace,nextTcpPeer: reusing existing peer  <*> 
<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReaderFactory$BlockReaderPeer nextTcpPeer()>,trace,nextTcpPeer: created newConnectedPeer  <*> 
<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReaderFactory$BlockReaderPeer nextTcpPeer()>,trace,nextTcpPeer: failed to create newConnectedPeer connected to  <*> 
<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReaderFactory$BlockReaderPeer nextDomainPeer()>,trace,nextDomainPeer: reusing existing peer  <*> 
<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReader getRemoteBlockReaderFromTcp()>,debug,Closed potentially stale remote peer  peer 
<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReader getRemoteBlockReaderFromTcp()>,trace,this : trying to create a remote block reader from a  TCP socket 
<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReader getRemoteBlockReaderFromTcp()>,trace,this : got security exception while constructing  a remote block reader from  peer 
<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReader getRemoteBlockReaderFromTcp()>,warn,I/O error constructing remote block reader.
<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReader getRemoteBlockReaderFromDomain()>,debug,this : not trying to create a  remote block reader because the UNIX domain socket at  <*>  is not usable. 
<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReader getRemoteBlockReaderFromDomain()>,debug,Closed potentially stale domain peer  <*> 
<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReader getRemoteBlockReaderFromDomain()>,trace,this : trying to create a remote block reader from the  UNIX domain socket at  <*> 
<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReader getRemoteBlockReaderFromDomain()>,trace,this : got security exception while constructing  a remote block reader from the unix domain socket at  <*> 
<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReader getRemoteBlockReaderFromDomain()>,warn,I/O error constructing remote block reader.  Disabling domain socket  <*> 
<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReader getLegacyBlockReaderLocal()>,debug,this : can\'t construct  BlockReaderLocalLegacy because  disableLegacyBlockReaderLocal is set. 
<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReader getLegacyBlockReaderLocal()>,trace,this : trying to construct BlockReaderLocalLegacy 
<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReader getLegacyBlockReaderLocal()>,trace,this : can\'t construct BlockReaderLocalLegacy because  the address  <*>  is not local 
<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReader getLegacyBlockReaderLocal()>,warn,this : error creating legacy BlockReaderLocal.   Disabling legacy local reads. 
<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReader getBlockReaderLocal()>,debug,this :  <*>  is not  usable for short circuit; giving up on BlockReaderLocal. 
<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReader getBlockReaderLocal()>,debug,this : failed to get  ShortCircuitReplica. Cannot construct  BlockReaderLocal via  <*> 
<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReader getBlockReaderLocal()>,trace,this : trying to construct a BlockReaderLocal  for short-circuit reads. 
<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReader getBlockReaderLocal()>,trace,this : got InvalidToken exception while trying to  construct BlockReaderLocal via  <*> 
<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReader build()>,trace,this : returning new legacy block reader local. 
<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReader build()>,trace,this : returning new block reader local. 
<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReader build()>,trace,this : returning new remote block reader using  UNIX domain socket on  <*> 
"<jdk.internal.event.EventHelper: void logX509ValidationEvent(int,int[])>",fine,"ValidationChain: {}, {}"
"<jdk.internal.event.EventHelper: void logX509CertificateEvent(java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,int,long,long,long)>",fine,"XCertificate: Alg:{}, Serial:{}, Subject:{}, Issuer:{}, Key type:{}, Length:{}, Cert Id:{}, Valid from:{}, Valid until:{}"
"<jdk.internal.event.EventHelper: void logTLSHandshakeEvent(java.time.Instant,java.lang.String,int,java.lang.String,java.lang.String,long)>",fine,"<*>  TLSHandshake: {}:{}, {}, {}, {} "
"<jdk.internal.event.EventHelper: void logSecurityPropertyEvent(java.lang.String,java.lang.String)>",fine,"SecurityPropertyModification: key:{}, value:{}"
